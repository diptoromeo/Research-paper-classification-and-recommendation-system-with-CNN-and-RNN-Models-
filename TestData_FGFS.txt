<paper no=1>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The architecture of fifth generation inference computers</paper_heading>
	<authors>L.O. Hertzberger</authors>
	<abstract>The architecture of the FGCs sequential and parallel inference machines are described.</abstract>
	<keywords>Architecture; FGCs; parallel; Machines </keywords>
	<publication_month_year>1984-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 1</volumes_issues>
</paper>
<paper no=2>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Supercomputers — past, present, prospects</paper_heading>
	<authors>Sidney Fernbach</authors>
	<abstract>In the fourth decade of the computer age, supercomputers are attracting increasingly more attention.</abstract>
	<keywords>Computer age; Supercomputer attracting; Increasingly</keywords>
	<publication_month_year>1984-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 1</volumes_issues>
</paper>
<paper no=3>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VLSI research in the U.S.A.</paper_heading>
	<authors>R.M. Burger, L.W. Sumney</authors>
	<abstract>Co-operative R&D in Advanced Information Technologies is not a Japanese privilege.</abstract>
	<keywords>Co-operative; R&D; Information Technologies; Japanese privilege.</keywords>
	<publication_month_year>1984-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 1</volumes_issues>
</paper>
<paper no=4>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Software engineering and artificial intelligence in new generation computing</paper_heading>
	<authors>Robert Kowalski</authors>
	<abstract>SPL Insight, a company of the British SPL consortium devoted to advanced information technology studies, has given its 1984 Award to Professor Bob Kowalski for his achievements in Fifth Generation computing. Professor Kowalski delivered an Award Lecture on May 15, 1984 in London. He and SPL Insight have kindly given permission to reproduce his lecture here.</abstract>
	<keywords>SPL Insight; SPL consortium</keywords>
	<publication_month_year>1984-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 1</volumes_issues>
</paper>
<paper no=5>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Progress in the fifth generation inference architectures</paper_heading>
	<authors>L.O. Hertzberger, R.P. Van de Riet</authors>
	<abstract>In an earlier article, the architecture of the FGCS sequential inference machines PSI and SIM and the parallel inference machine PIM were described. This article gives a progress report concerning these developments. Moreover, the relational database machine (DELTA) and the connection of DELTA with PSI to form a first prototype of a Knowledge Base Machine (KBM) are discussed. Most of the information is based on the 1984 research report on the Fifth Generation Computer Systems.</abstract>
	<keywords>FGCS; inference enginelogic programming; parallel architecture</keywords>
	<publication_month_year>1984-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 2</volumes_issues>
</paper>
<paper no=6>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Cooperative access systems</paper_heading>
	<authors>W. Wahlster</authors>
	<abstract>In the next five years we can expect to find a variety of cooperative natural language (NL) interfaces to database systems, expert systems, operating systems, CAD systems and even text formatting systems. Although the general problem of getting computers to understand NL is far from being solved, the technology for limited NL access systems is available now and there is a large growing market for such cooperative interfaces to a wide variety of software. In the first part of the paper, we illustrate the capabilities for cooperative response generation implemented in AI systems. Then we briefly review the state of the art in NL interfaces to databases and expert systems. Topics covered include user modeling, the generation of explanations, mixed initiative dialogs and knowledge acquisition via NL communication.</abstract>
	<keywords>Natural language (NL); CAD systems; AI systems; Database Systems</keywords>
	<publication_month_year>1984-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 2</volumes_issues>
</paper>
<paper no=7>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Associative learning using similarity knowledge bases for relational database search</paper_heading>
	<authors>Kiyohiko Nakamura, Andrew P. Sage, Sosuke Iwai</authors>
	<abstract>A knowledge-based interface system for relational databases is presented. The system consists of two basic components; a similarity knowledge base and a generalization reasoning engine. The knowledge base is a particular type of semantic network that represents similarity relationships between the data. The similarity relationships are derived from the relational database by a psychological model of similarity. The reasoning engine propagates information obtained from the user's responses over the similarity network and also determines the generic kind of question it should next pose to the user. Through this dialog process, the interface system aids the user in specifying requests for relevant data, as well as in retrieving data from the database. Following a presentation of the constructs that support database design, we note that agenda dependency of the dialog results will typically occur. A smoothing mechanism that can reduce this dependency is presented. Incorporation of smoothing in the database design improves the efficiency and the effectiveness of the resulting system.</abstract>
	<keywords>Interface system; Databases design</keywords>
	<publication_month_year>1984-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 2</volumes_issues>
</paper>
<paper no=8>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>USERNET: A supercomputer network architecture</paper_heading>
	<authors>Franklin F. Kuo, Jose J. Garcia-Luna-Aceves</authors>
	<abstract>Network access to supercomputers is the only viable approach to support the widespread sharing of such resources by geographically scattered research communities. This article presents an approach for the provision of nationwide access to supercomputers in the USA.</abstract>
	<keywords>Network access; Supercomputer; Widespread sharing; Geographically scattered research communities</keywords>
	<publication_month_year>1985-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 3</volumes_issues>
</paper>
<paper no=9>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Advanced object oriented architectures</paper_heading>
	<authors>Wolfgang K. Giloi</authors>
	<abstract>The principle of having a processor manipulate the states of single memory words in a word-at-a-time fashion causes an ‘intellectual bottleneck’ for the programmer as well as a physical, performance limiting bottleneck for the executing machine. Functional programming mitigates the intellectual but not the physical bottleneck. The intellectual bottleneck problem can also be resolved within the framework of procedural programming by introducing into an appropriate programming language objects of data structure types as the entities to be manipulated. In order to also resolve the physical bottleneck problem, appropriate data structure objects should exist already at the hardware level as objects of machine data structure types that are recognized and manipulated by the machine. This approach allows the computer to deal efficiently with the parallelism inherent in data structure objects. In the paper, appropriate, generic machine data structure types are introduced, to provide the basis for the efficient representation and processing of arbitrary, application-oriented data types. The hardware representation of the machine data structure type is based upon the use of descriptor information. The resulting computer architecture is free of the “von Neumann bottleneck”, conceptually as well as physically.</abstract>
	<keywords>Intellectual bottleneck; Physical performance;  Physical bottleneck</keywords>
	<publication_month_year>1985-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 3</volumes_issues>
</paper>
<paper no=10>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards a connection machine for logical inference</paper_heading>
	<authors>W. Bibel, B. Buchberger</authors>
	<abstract>This paper gives an outline of the architecture of a multiprocessor machine for deduction in first-order logic and its functional behavior, which fully exploits the parallelism inherent in the deductive process. With respect to the inferential side, the approach is based on the connection method which lends itself to a parallel treatment as illustrated with several examples. With respect to the architectural side, it is argued that in comparison with other well-known parallel architectures the L-networks appear to be most suitable for this kind of problem.</abstract>
	<keywords>Parallelism inherent; Parallel treatment; illustrate</keywords>
	<publication_month_year>1985-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 3</volumes_issues>
</paper>
<paper no=11>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Manchester dataflow machine</paper_heading>
	<authors>J.R. Gurd</authors>
	<abstract>A prototype dataflow computer system has been constructed by a research team at the University of Manchester. The hardware has been operational since October 1981, but has been steadily enhanced since that time. Store capacities and I/O bandwidth are approaching the state where realistically large applications programs can be used to evaluate system performance. During the period of hardware enhancement, there have been parallel advances in the development of dataflow system software in the form of assemblers, compilers, debugging systems and sundry software tools. The software is also approaching readiness for application to large-scale benchmark programs. The Manchester system implements a tagged-token dataflow model of computation. This model imposes a tag-field penalty on data values in order to maximise asynchronousness of instruction execution. It is important to know to what extent this overhead is necessary, and how to minimise it whilst maintaining acceptable asynchronousness. In comparison with more conventional architectures it is important that useful measures of cost and performance be developed. With emphasis on these issues, the paper describes the structure of the Manchester dataflow hardware and software, and outlines the system performance results so far obtained. Whilst this work is far from complete, it suggests new avenues for hardware and software development which are being followed at Manchester and elsewhere.</abstract>
	<keywords>Dataflow computer system; I/O bandwidth; Evaluate system performance</keywords>
	<publication_month_year>1985-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 4</volumes_issues>
</paper>
<paper no=12>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Second generation expert systems</paper_heading>
	<authors>Luc Steels</authors>
	<abstract>Second generation expert systems are able to combine heuristic reasoning based on rules, with deep reasoning based on a model of the problem domain. This solves a number of major problems of current expert systems, most importantly the problem of knowledge acquisition: second generation expert systems can learn new rules by examining the results of deep reasoning. The paper outlines the components of second generation expert systems and gives an example.</abstract>
	<keywords>Expert systems; Combine heuristic; Deep reasoning</keywords>
	<publication_month_year>1985-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 4</volumes_issues>
</paper>
<paper no=13>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge bases</paper_heading>
	<authors>Gio Wiederhold</authors>
	<abstract>The KBMS project at Stanford has been using knowledge based techniques to deal, among other, with issues of query and response processing in databases. We will describe some fairly simple techniques which have been found to be effective: 1.1. Disambiquation of incomplete queries; 2.2. Query transformation to reduce retrieval costs; 3.3. Use of a structural model to create implicit joins; 4.4. Use of the model to diagnose misleading responses due to faulty implicit assumptions; 5.5. Automatic reduction of output volume.</abstract>
	<keywords>KBMS; Databases; Query transformation</keywords>
	<publication_month_year>1985-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 4</volumes_issues>
</paper>
<paper no=14>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge engineering and CAD</paper_heading>
	<authors>Tetsuo Tomiyama, Hiroyuki Yoshikawa</authors>
	<abstract>The technology of Computer Aided Design (CAD) has rapidly been changing the processes of machine design, especially in detail and production design. Conventional CAD systems are necessary in production design where precise geometrical data are indispensable, but we can point out some problems of those systems from a point of view of the total machine design process. Though the main aim of introducing CAD systems is to increase designers' creativities, it is doubtful whether designers are really supported by such CAD systems. We think that knowledge engineering is useful to improve such situations. In this paper, we discuss these problems and we study the feasibilities of future CAD systems that can really help designers very intelligently and efficiently. We also show some results of our experimental systems applying knowledge engineering.</abstract>
	<keywords>Computer Aided Design (CAD); Machine design; Conventional CAD systems </keywords>
	<publication_month_year>1985-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 4</volumes_issues>
</paper>
<paper no=15>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visual terminals and user interfaces</paper_heading>
	<authors>T. Kamae</authors>
	<abstract>The evolution of computer and communication technologies enables man-to-machine interfaces through graphics. This paper describes the state of the art in visual terminals such as telewriting, facsimile and videotex in Japan. User interfaces through visual terminals facilitate the input and the understanding of information, and thus stimulate wider dissemination of computers and non-voice communication, even to the private sector.</abstract>
	<keywords>Communication technologies; Man-to-machine interfaces; </keywords>
	<publication_month_year>1985-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 5</volumes_issues>
</paper>
<paper no=16>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Data reduction of picture signals: Review on the studies in Japan</paper_heading>
	<authors>Takahiko Fukinuki</authors>
	<abstract>Studies in Japan on data reduction of picture signals for a recent quater century are reviewed, which cover redundancy reduction coding in digital area and bandwidth compression in analogue area. Main issues are in the basic studies in early 70's and practical ones for new communication use in 80's.</abstract>
	<keywords>Data reduction; Bandwidth; Analogue area</keywords>
	<publication_month_year>1985-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 5</volumes_issues>
</paper>
<paper no=17>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An overview of NTTs digital transmission networks - existing and planned</paper_heading>
	<authors>Masao Kato, Yutaka Yoshida</authors>
	<abstract>This paper presents recent utilization trends of data communications circuits, telephone and telex networks and describes the digitalization and integration plan of NTT network facilities for telecommunications services. Secondly, this paper describes public data network development activities in NTT since 1971, and the features and current status of circuit-switched and packet-switched data networks. Finally this paper describes the telephone - network - Packet - network interworking system developed by NTT and future topics related to packet switching service.</abstract>
	<keywords>Data communications circuits; Telephone; Telex networks;  NTT network </keywords>
	<publication_month_year>1985-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 5</volumes_issues>
</paper>
<paper no=18>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Robot vision</paper_heading>
	<authors>Y. Shirai</authors>
	<abstract>Robot vision is widely studied in Japan to realize flexible manufacturing systems. The study of robot vision includes the development of the following themes: input devices for range information, feature extraction for inspection or positioning, high-speed image processors, stereo vision, three-dimensional shape recovery, object recognition by model matching, and so forth. This paper describes some interesting work in these fields.</abstract>
	<keywords>Robot vision; Manufacturing systems</keywords>
	<publication_month_year>1985-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 5</volumes_issues>
</paper>
<paper no=19>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Medical image processing: An overview of and case study in the diagnosis of cardiac diseases</paper_heading>
	<authors>Michiyoshi Kuwahara, Shigeru Eiho</authors>
	<abstract>In this paper an overview is presented on R&D of medical imaging and image processing techniques in Japan during the last decade, focusing on case studies in the diagnosis of cardiac diseases. Various problems of image processing of left ventricular images obtained from x-ray, ultrasound and radionuclides are discussed. Also reviewed are boundary detection methods of left ventricular inner cavity and myocardium, analysis of left ventricular wall motion and various kinds of cardiac functions using conventional and newly developed methods, and display methods of these cardiac functions. Microcomputerized on-line image processing systems and a compound sector scan echocardiography using two probes developed for echocardiographic examinations are shown with some results in clinical applications. 3-dimensional reconstruction methods of the left ventricle and other organs from multiple 2-dimensional images are shown with 3-dimensional shapes reconstructed from images by x-ray, ultrasound and radionuclides. Moreover, 3-dimensional functional images of the left ventricle are shown as well. Transferring the 3-dimensional consecutive left ventricular images during a cardiac cycle onto a CRT display under the control of a microprocessor, we can observe the pulsating 3-dimensional left ventricle on the CRT.</abstract>
	<keywords>R&D; Ventricular images; Image processing systems; CRT</keywords>
	<publication_month_year>1985-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 5</volumes_issues>
</paper>
<paper no=20>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fifth generation and VLSI architectures</paper_heading>
	<authors>Philip C. Treleaven, Apostolos N. Refenes</authors>
	<abstract>Most Western Governments (USA, Japan, EEC, etc.) have now launched national programmes to develop computer systems for use in the 1990s. These so-called Fifth Generation computers are viewed as “knowledge” processing systems which support the symbolic computation underlying Artificial Intelligence applications. The major driving force in Fifth Generation computer design is to efficiently support very high level programming languages (i.e. VHLL architecture). Historycally, however, commercial VHLL architectures have been largely unsuccesful. The driving force in computer designs has principally been advances in hardware which at the present time means architectures to exploit very large scale integration (i.e. VLSI architecture). This paper examines VHLL architectures and VLSI architectures and their probable influences on Fifth Generation computers. Interestingly the major problem for both architecture classes is parallelism; how to orchestrate a single parallel computation so that it can be distributed across an ensemble of processors.</abstract>
	<keywords>Computer systems; “Knowledge” processing systems; VHLL architectures</keywords>
	<publication_month_year>1985-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 6</volumes_issues>
</paper>
<paper no=21>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Trends of VLSI in Japan</paper_heading>
	<authors>Hajime Sasaki</authors>
	<abstract>Although the Japanese IC industry seems gathering world attention due to success of memory business, some phenomenal changes and new concepts are anticipated entering the era of VLSI. Starting with the historical background of Japanese semiconductor industry, the status of new products development and the future forecast will be presented.</abstract>
	<keywords>IC industry; VLSI; Semiconductor industry</keywords>
	<publication_month_year>1985-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 6</volumes_issues>
</paper>
<paper no=22>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Military applications of expert systems</paper_heading>
	<authors>John F. Gilmore</authors>
	<abstract>The advent of expert systems has led to the development of advanced computer systems in areas of medicine, geology, mathematics, chemistry, vision, speech and electronics. The recent acceptance of artificial intelligence as an appropriate technology for military applications has evolved into the development of a number of defense related expert systems. This paper reviews several military applications of expert systems in the areas of advanced visual target recognition, autonomous tactical vehicles, and combat pilot aid systems. System concepts for these areas are described. Several references are provided for each. A number of other military applications of expert systems are discussed through the paper.</abstract>
	<keywords>Expert systems; Combine heuristic; Deep reasoning</keywords>
	<publication_month_year>1985-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 6</volumes_issues>
</paper>
<paper no=23>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>STARS and stripes</paper_heading>
	<authors>J.C. van Vliet</authors>
	<abstract>The overall goal of DoD's Software Initiative is to meet DoD's future software needs by an order of magnitude improvement in the state of the practice, and to hasten the transition of new technology. The STARS Program (Software Technology for Adaptable, Reliable Systems) is one of the components of the Software Initiative. Other components are the Ada1 Program and the Software Engineering Institute. The STARS Program is to result in a fully integrated environment which captures all phases of the software life cycle. From April 30 to May 2, 1985, the first DoD/Industry STARS Program Conference was organized, which brought together representatives of government, industry, and the academic community to review and discuss the STARS Program and other components of the Software Initiative. Below, we report on the presentations given at this conference.</abstract>
	<keywords>DoD's Software; STARS Program (Software Technology for Adaptable, Reliable Systems); DoD/Industry </keywords>
	<publication_month_year>1985-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 1, Issue 6</volumes_issues>
</paper>
<paper no=24>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Current status and future trends in machine translation</paper_heading>
	<authors>Makota Nagao</authors>
	<abstract>A survey of the current machine translation systems is given, which includes not only activities in Japan, but also abroad, especially European, US and Canadian activities. Then the components of a machine translation system are explained from the standpoint of software, linguistic components, and users' demands. The importance of pre-editing and post-editing is stressed. The semantic and contextual processings are essential to obtain a better translation quality, which are the future problems to attack. Attention is given to the difficulty of contemplating a pivot method in machine translation instead of transfer methods, because the projection from a word or a phrase to a concept is very difficult if we want to have a very exact concept representation and translation. A new transfer method which accompanies the pe-transfer structural adjustment and post-transfer adjustment is explained. This method was adopted by the Japanese governmental project of machine translation which was directed by the author. Various mechanisms of structural transformations in the transfer and generation processes are explained, which are necessitated by the language translation between the two languages of different language families like Japanese and English. Finally some comments are given from the standpoint of users of machine translation systems. Systems always are imperfect, and users must use them after recognizing the possibilities and the limitations of the system.</abstract>
	<keywords>Machine translation systems; Transfer methods; Language translation</keywords>
	<publication_month_year>1986-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 2</volumes_issues>
</paper>
<paper no=25>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Technologies for machine translation</paper_heading>
	<authors>R.F. Simmons</authors>
	<abstract>Advances in hardware have made available micro-coded LISP and PROLOG workstations, supported by text editing and formatting software. Some of these have been augmented with linguistic technology including large bilingual dictionaries, parsers, generators, and translators to make them powerful tools for research and development of automated translation. Some techniques of linguistic engineering for accomplishing translation are described, and it is suggested that the present barely satisfactory approach involving sentence-by-sentence translation will eventually be improved by incorporating the results of research on analyzing discourse.</abstract>
	<keywords>Micro-coded LISP; PROLOG workstations;  linguistic technology; </keywords>
	<publication_month_year>1986-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 2</volumes_issues>
</paper>
<paper no=26>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fujitsu machine translation system: ATLAS</paper_heading>
	<authors>Hiroshi Uchida</authors>
	<abstract>Due to the rapid advancement of both computer technology and linguistic theory, machine translation systems are now coming into practical use. Fujitsu has two machine translation systems, ATLAS-I is a syntax-based machine translation system which translates English into Japanese. ATLAS II is a semantic-based system which aims at high quality multilingual translation. In this paper, both the ATLAS-I and ATLAS II translation mechanisms are explained.</abstract>
	<keywords>Computer technology; Linguistic theory; Machine translation system; ATLAS II</keywords>
	<publication_month_year>1986-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 2</volumes_issues>
</paper>
<paper no=27>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Problems of machine translation systems: Effect of cultural differences on sentence structure</paper_heading>
	<authors>Yoshihiko Nitta</authors>
	<abstract>The potential and the limitation of current machine translation is discussed by comparing the output of human translation and that of virtual machine translation. Here, “virtual machine translation” means a kind of syntax-oriented literal translation which may be regarded as an idealized competence of today's practical machine translation. The above comparison shows that the main reason for the limitation or the incompleteness of current practical machine translation systems is the insufficient ability to treat “structural idiosyncrasies” of sentences. Also, some translation examples tell us that, without “understanding” the total meaning of the source sentence, it is quite difficult to manipulate the idiosyncrasies in sentence structure. Idiosyncratic gaps between source and target sentence structure usually originate in cultural differences, so that the computational treatment of these gaps is a very difficult problem. But the translation examples also give us some encouraging evidence that the principal technologies of today's not-yet-completed machine translation have sufficient potential for producing barely acceptable translation. The current practical efforts to treat such structural idiosyncrasies are also mentioned together with some long-range, basic-research type of approaches.</abstract>
	<keywords>Machine translation; Human translation; Structural idiosyncrasies</keywords>
	<publication_month_year>1986-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 2</volumes_issues>
</paper>
<paper no=28>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VENUS: Two-phase machine translation system</paper_heading>
	<authors>Kazunori Muraki</authors>
	<abstract>NEC has been developing a Japanese-English bi-directional machine translation system called VENUS (Vehicle for Natural language Understanding & Synthesis) in order to reduce the increasing cost of the manual translation of vast amounts of in-house technical documents. In addition, a translation support subsystem has been developed on the basis of VENUS, and extended to have the requisite facilities to prepare translated documents, such as document entry, editing, translation, printing, management, etc. This paper briefly introduces the current status of the VENUS translation system, and the basic idea for the system development.</abstract>
	<keywords>Machine translation system; VENUS (Vehicle for Natural language Understanding & Synthesis); VENUS translation system</keywords>
	<publication_month_year>1986-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 2</volumes_issues>
</paper>
<paper no=29>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Toshiba Machine Translation System</paper_heading>
	<authors>S. Amano</authors>
	<abstract>The Toshiba Machine Translation System consists of a 32-bit minicomputer (UX-700). It translates texts of patents, scientific and technical documents. It is based on a semantic transfer schema. The newly developed grammar system for this schema is called “Lexical Transition Network Grammar” which is very poweful for semantic analysis. Semantic analysis is performed by lexical grammars attached to each item in the lexicon. The system has three kinds of dictionaries. To get an excellent man-machine interface, the system is equipped with a bi-lingual editor, an English/Japanese word processor for pre-/post-editing and various software utilities.</abstract>
	<keywords>Machine translation system; Lexical Transition Network Grammar; Semantic analysis</keywords>
	<publication_month_year>1986-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 2</volumes_issues>
</paper>
<paper no=30>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Science and technology agency's Mu machine translation project</paper_heading>
	<authors>Makoto Nagao, Jun'ichi Tsujii, Jun'ichi Nakamura</authors>
	<abstract>This paper describes the current status of a machine translation project that aims to develop Japanese-to-English and English-to-Japanese translation systems for abstracts of scientific and technical articles. The paper describes the development of Japanse-to-English translation systems, and discusses methodologies for evaluating translation results. Particular emphasis is placed on techniques necessary for translation between Japanese and English - languages having fundamentally different structures.</abstract>
	<keywords>Machine translation project;  Japanese-to-English and English-to-Japanese translation systems; </keywords>
	<publication_month_year>1986-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 2</volumes_issues>
</paper>
<paper no=31>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Practical applications involving uncertainty</paper_heading>
	<authors>M. Small, A. Pinkerton, I. Meyer</authors>
	<abstract>This paper presents an outline description of some of the practical problems which ICL has solved using knowledge engineering tools and techniques. These problems described all involve some measure of uncertainty. The problems include the following: 1.1. Forecasting energy demand by a public utility taking into account the uncertainties regarding the weather as well as users reactions to different weather patterns. 2.2. Managing computer capacity; here the problem is both planning and control. Planning involves forecasting the impact of changes in business circumstances on the use of computer systems. Control involves comparison of actual usage and performance levels with those forecast and determining appropriate actions. 3.3. Forecasting of stores inventory applying expert knowledge to use incomplete data concerning actual stock movements together with plans and historical information to forecast future stock levels. 4.4. Corporate strategy and business modelling to forecast the likely demand for products and hence the necessary resources to support commercial ambitions. An aim of this project is to assist the contributing departments to articulate their assumptions and dependencies.</abstract>
	<keywords>ICL; Computer systems; Forecast future stock levels; </keywords>
	<publication_month_year>1986-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 3</volumes_issues>
</paper>
<paper no=32>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge resource tools for information access</paper_heading>
	<authors>Donald E. Walker</authors>
	<abstract>This paper provides an overview of a research program just being defined at Bellcore. The objective is to develop facilities for working with large document collections that provide more refined access to the information contained in these “source” materials than is possible through current information retrieval procedures. The tools being used for this purpose are machine-readable dictionaries, encyclopedias, and related “resources” that provide geographical, biographical, and other kinds of specialized knowledge. A major feature of the research program is the exploitation of the reciprocal relationship between sources and resources. These interactions between texts and tools are intended to support experts who organize and use information in a workstation environment. Two systems under development will be described to illustrate the approach: one providing capabilities for full-text subject assessment; the other for concept elaboration while reading text. Progress in the research depends critically on developments in artificial intelligence, computational linguistics, and information science to provide a scientific base, and on software engineering, database management, and distributed systems to provide the technology.</abstract>
	<keywords>Bellcore; Source materials; Machine-readable; Database management; Distributed systems </keywords>
	<publication_month_year>1986-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 3</volumes_issues>
</paper>
<paper no=33>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Business applications of artificial intelligence knowledge based expert systems</paper_heading>
	<authors>Martin L. Ernst, Helen Ojha</authors>
	<abstract>Business organizations have shown a broad range of interest in knowledge based systems, including not only expert systems but also advisory systems, to assist experts in their tasks, and checklist systems to support clerical workers. These interests can be displayed by mapping existing and planned systems along two axes: the role a system in intended to serve and the complexity of the problem environment with which the system must deal. The maps provide a vehicle for understanding the rationales employed by businesses in making decisions to build knowledge based systems. They also provide a basis for understanding what currently is practical, and what barriers exist that will limit the future growth of business knowledge based systems.</abstract>
	<keywords>Knowledge based systems; Expert systems</keywords>
	<publication_month_year>1986-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 3</volumes_issues>
</paper>
<paper no=34>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Expert database systems</paper_heading>
	<authors>R.P. van de Riet</authors>
	<abstract>An overview and some impressions are given of the first conference on Expert Database Systems in Charleston, April 1986. The emphasis in this overview is on the keynote address, the invited lecture and the discussions in the panels, because these were not published in the proceedings.</abstract>
	<keywords>Expert Database Systems; Overview; Keynote address </keywords>
	<publication_month_year>1986-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 3</volumes_issues>
</paper>
<paper no=35>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On commercial expert systems projects</paper_heading>
	<authors>H. Marchand</authors>
	<abstract>The development process of expert systems obeys to rules, which have no counterparts in traditional software engineering techniques. To a certain extent they even contradict them. AI engineers in charge of the realization of industrial expert systems must be aware of this situation and be prepared to respond to it by providing technology transfer. As long as this transfer to the customers has not happened, commercial expert system projects will lead to frustration and compromises hampering the achievement of the economic objectives set to the projects. For the time being technology transfer must be a major task of AI companies.</abstract>
	<keywords>Expert systems; Combine heuristic; Deep reasoning</keywords>
	<publication_month_year>1986-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 4</volumes_issues>
</paper>
<paper no=36>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Expert systems in management science</paper_heading>
	<authors>Philip Cooper</authors>
	<abstract>Decision support software as we know it has failed in its goal of bringing managers and the management sciences closer together. Conventional software requires computer literacy and detailed technical modeling skills. Top managers, on the other hand, are prized for an entirely different set of skills: experienced judgment, innovative thinking and the ability to see the “big picture”. Even if they could build complicated mathematical models, work closely with computers and had timely access to relevant details, most managers would not have the time. This paper contends that the gap between managerial and analytic/computer competence can be bridged with expert systems meeting the following criteria: 1.1. Require no user training. 2.2. Interact and explain themselves. 3.3. Learn from and adapt to users 4.4. Incorporate state-of-the-art analytic techniques. Palladian Software, Inc., of Cambridge Massachusetts is using artificial intelligence technology to build such expert systems for managers. An example of one application is provided.</abstract>
	<keywords>Conventional software; Palladian Software; Expert systems</keywords>
	<publication_month_year>1986-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 4</volumes_issues>
</paper>
<paper no=37>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>System structure for parallel logic programming</paper_heading>
	<authors>H. Diel, N. Lenz, H.M. Welsch</authors>
	<abstract>A system structure supporting parallel processing in general and parallel logic programming and expert system applications in particular is described. It is not based on special hardware but has rather been designed as an evolutionary extension to most existing machine architectures. It is aimed at parallel processing support for e.g. PROLOG as well as for expert system (shells) implemented in a general purpose language. A layered structure consisting of an extended machine interface and a macro language is chosen to support a range of various applications.</abstract>
	<keywords>Parallel processing; Parallel logic programming; Expert system; PROLOG</keywords>
	<publication_month_year>1986-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 2, Issue 4</volumes_issues>
</paper>
<paper no=38>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The scope and limitations of first generation expert systems</paper_heading>
	<authors>Derek Partridge</authors>
	<abstract>It is clear that expert system's technology is one of AI's greatest successes so far. Currently we see an ever increasing application of expert systems, with no obvious limits to their applicability. Yet there are also a number of well-recognized problems associated with this new technology. I shall argue that these problems are not the puzzles of normal science that will yield to advances within the current technology; on the contrary, they are symptoms of severe inherent limitations of this first generation technology. By reference to these problems I shall outline some important aspects of the scope and limitations of current expert system's technology. The recognition of these limitations is a prerequisite of overcoming them as well as of developing an awarenness of the scope of applicability of this new technology.</abstract>
	<keywords>Expert system's technology; AI; Severe inherent limitations</keywords>
	<publication_month_year>1987-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 1</volumes_issues>
</paper>
<paper no=39>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Problems with expert systems?</paper_heading>
	<authors>Reind P. Van De Riet</authors>
	<abstract>Prompted by a request to write a responder to McDermott's invited paper for the IFIP 86 congress in Dublin, this paper resulted in which we indicate some problems around Expert Systems (ES) in general and make some critical remarks on McDermott's paper. First, we place his paper in the context of expectations about ES's in general. Next, because his paper goes into some of the troubles of current Expert Systems, we will, in particular, deal with the question whether the current situation around ES's is healthy. We shall confront McDermott's opinions with opinions of other workers in the ES field.</abstract>
	<keywords>McDermott's invited paper; Expert Systems (ES); McDermott's opinions </keywords>
	<publication_month_year>1987-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 1</volumes_issues>
</paper>
<paper no=40>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Results of survey on trends in expert systems in Japan</paper_heading>
	<authors>Reind P. Van De Riet</authors>
	<abstract>In cooperation with the Institute for New Generation Computer Technology (ICOT), Japan Information Processing Development Center (JIPDEC) established the Artificial Intelligence Center in April, 1986 to promote artificial intelligence related technologies. One of the projects set for the new center involves providing user-oriented guidelines for expert system development and preparing educational programs to train expert system development engineers. The survey on the current state of expert systems in Japan was conducted as a preparatory study for the project. The aim was to obtain statistics on issues such as the objects of expert system development, application fields of expert systems, development environments and organizations, problems in training engineers, bottlenecks in building systems, and future trends in the field.</abstract>
	<keywords>Generation Computer Technology (ICOT); Japan Information Processing Development Center (JIPDEC); Artificial Intelligence; Expert system </keywords>
	<publication_month_year>1987-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 1</volumes_issues>
</paper>
<paper no=41>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The state of the art of knowledge engineering at five Japanese Research Institutes — a travel report</paper_heading>
	<authors>Reind P. Van De Riet</authors>
	<abstract>In the week preceding the VLDB conference in Kyoto (August 25–28, 1986), Japan, I visited five top research institutes in and around Tokyo. These institutes' research in the area of knowledge engineering was discussed and observed during these visits. As I visited these institutes two years ago, I was able to make some comparisons about the progress being made. Two years ago there were only one or two bold plans with hesitation and doubt about their success. Today, these plans are providing to be quite solid and promising. The plans are now in a stage of consolidation. Our Japanese colleagues are embarking on even more fascinating new projects, in particular the plans to ‘cast biologic into silicon’.</abstract>
	<keywords>VLDB; Research institutes</keywords>
	<publication_month_year>1987-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 1</volumes_issues>
</paper>
<paper no=42>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>POPE — a parallel-operating prolog engine</paper_heading>
	<authors>J. Beer, W.K. Giloi</authors>
	<abstract>An architecture is presented for the parallel execution of sequential Prolog. The architecture is based on a pipeline of unification processors and designed to work as a co-processor to a more conventional, UNIX based workstation. The unification processors execute highly optimized compiled Prolog code; however, the basic concept of the architecture could also increase the performance of interpreter based systems. It will be shown that even programs that do not exhibit any of the ‘classical’ forms of parallelism (i.e. AND-, OR-parallelism, etc.) can be effectively mapped onto the proposed architecture. The presented architecture should also prove very effective as a multi-user Prolog machine executing several independent Prolog programs in parallel. In contrast to other attempts to execute sequential Prolog in parallel we do not restrict the use of any of the standard Prolog language features such as dynamic assert/retract, CUT, etc. Simulation results show that peak execution rates of over 1000 KLIPS can be obtained.</abstract>
	<keywords>Parallel; Vector; Optimization; UNIX; CUT</keywords>
	<publication_month_year>1987-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 2</volumes_issues>
</paper>
<paper no=43>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge processing: Beware of stalemate</paper_heading>
	<authors>Václav Chvalovský</authors>
	<abstract>Although there is no doubt at present that knowledge processing is likely to shape the pattern of computer applications in the late 80s and beyond, a few signals indicate there is no justification for haste and impatience as far as the scope and impact of knowledge processing on the so-called “bread and butter” jobs in DP are concerned. Thus, to follow a more evolutional rather than revolutional path is recommended in the implementation of knowledge processing applications.</abstract>
	<keywords>Knowledge processing; Computer applications; DP </keywords>
	<publication_month_year>1987-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 2</volumes_issues>
</paper>
<paper no=44>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Why evolutionary development of expert systems appears to work</paper_heading>
	<authors>Michael S.H. Heng</authors>
	<abstract>Currently most expert systems are developed using an evolutionary method. The method has been criticised for its lack of scientific approach in problem solving. It is here argued that the method continues to be popular among expert systems builders because it can tackle four crucial problems, namely: (1) the problem of defining system requirements; (2) the problem of extracting expert knowledge; (3) the problem of understanding and structuring expert knowledge for machine manipulation; (4) the problem of maintaining the interest and enthusiasm of the domain experts. The method has some shortcomings, which calls for a disciplined style and control as well as supportive software tools.</abstract>
	<keywords>Expert systems; Combine heuristic; Deep reasoning</keywords>
	<publication_month_year>1987-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 2</volumes_issues>
</paper>
<paper no=45>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Should robots have nuclear arms? AI technology and SDI software</paper_heading>
	<authors>Ira Pohl</authors>
	<abstract>This paper analyzes the use of Artificial Intelligence (AI) in designing SDI software. Two major uses are foreseen: (1) as an aid to writing the ten million plus lines of trustworthy SDI software that will be needed; (2) as a component of the battle management software. This second use allows the software to be adequately responsive to SDI counter-measures and autonomous in a manner fitting overall SDI requirements. This paper suggests that these uses of AI in SDI are infeasible and that rather than creating greater security, they would be more likely lead to a nuclear war.</abstract>
	<keywords>Artificial Intelligence (AI); SDI software; SDI counter-measures</keywords>
	<publication_month_year>1987-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 2</volumes_issues>
</paper>
<paper no=46>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>FY 1986 research projects sponsored by the National Science Foundation</paper_heading>
	<authors>NSF</authors>
	<abstract>The National Science Foundation is an independent federal agency created the by National Science Foundation Act of 1950. Its aim is to promote and advance scientific progress in the United States. The idea of such a foundation was an outgrowth of the important contributions made by science and technology during World War II. From those first days, NSF has had a unique place in the federal government: It is responsible for the overall health of science across all disciplines. In contrast, other agencies support research focused on specific missions. NSF funds research in all fields of science and engineering. It does this through grants and contracts to more than 2,000 colleges, universities, and other research institutions in all parts of the United States. The Foundation accounts for about 28 percent of federal support to academic institutions for basic research. NSF receives more than 27,000 proposals each year for research and graduate fellowships and makes more than 12,000 awards. These go to universities, colleges, academic consortia, nonprofit institutions, and small businesses. The agency operates no laboratories itself but does support National Research Centers, certain oceanographic vessels, and Antarctic research stations. The Foundation also aids cooperative research between universities and industry and U.S. participation in international scientific efforts. NSF is structured much like a university, with grant-making divisions for the various disciplines and fields of science and engineering. The Foundation's staff is helped by advisors, primarily from the scientific community, who serve on formal committees or as ad hoc reviewers of research proposals. This advisory system, which focusses on both program direction and specific proposals, involves more than 50,000 scientists and engineers a year, NSF staff members who are experts in a certain field or area make final award decisions; applicants get verbatim unsigned copies of peer reviews and can appeal those decisions. The Knowledge and Database Systems Program of the NSF consists of the following five subject areas: 1. Knowledge Engineering; 2. Formal Models; 3. Compiled Knowledge Systems; 4. Natural Language Systems; 5. Cognitive Systems. For each of these areas a listing of concise project descriptions is given for Fiscal Year 1986.</abstract>
	<keywords>Knowledge Engineering; Formal Models; Compiled Knowledge Systems; Natural Language Systems; Cognitive Systems</keywords>
	<publication_month_year>1987-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 2</volumes_issues>
</paper>
<paper no=47>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An impression of the research activities of MCC in the area of data- and knowledge base systems — a travel report</paper_heading>
	<authors>Reind P. van de Riet</authors>
	<abstract>MCC, an abbreviation for the Microelectronics & Computer Technology Corporation, started its operations only three years ago as the answer of ten American computer industries to the Japanese Fifth Generation Computer Systems Project. This report has been written to give my impression of a visit paid to MCC in December 1986. I will report on MCC's functioning as a whole and of the database group in particular. A brief comparison is made with the current Japanese and European undertakings. All in all, I was impressed very much by the size and by the quality of MCC's efforts.</abstract>
	<keywords>MCC; Microelectronics & Computer Technology Corporation; Japanese Fifth Generation Computer Systems Project </keywords>
	<publication_month_year>1987-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 2</volumes_issues>
</paper>
<paper no=48>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>N-Expression implementations for integrated symbolic and numeric processing</paper_heading>
	<authors>Apostolos N. Refenes</authors>
	<abstract>Despite a growing interest in the integration of the symbolic, and numeric processing, the corresponding language and architecture types are still considered largely incompatible. A most serious incompatibility accrues from the diversity of the data structures they manipulate. In symbolic processing, data structures tend to be complex Symbolic-Expressions scattered through memory. These structures, known as S-Expressions, are implemented by using the binary list and its algebra of programs. In numeric applications the most common data structure is a linearly allocated array or vector. Array processing involves an entirely different algebra of operations, therefore effecting incompatabilities both at the language and architecture levels. This paper presents a brief formal semantics for the N-Expression, followed by a detailed account of how the N-Expression can be implemented in the linear computer memory, and concludes by presenting some experimental results from its implementation on a network of software-implemented microcomputers. The N-Expression attempts to support symbolic and numeric processing in a uniform way through the n-ary list and its algebra of programs. The n-ary list subsumes both the binary list (S-Expression) and the array type. Its algebra of programs is founded on a set of primitive operations that fall into three groups for: accessing, traversing and reorganising the n-ary list. For implementation the n-ary list in the linear computer memory, three novel schemes are discussed and evaluated for efficiency. The first scheme is based on a tree representing (e.g. paged memory) and shows performance speed ups of between 4–6 times over symbolic processing systems that use the traditional binary list (e.g. CDR-Coded lisp cells) to implement structured memory. The second scheme is based on a nested parentheses representation (e.g. delimited string) and shows a performance improvement factor of between 1.2–2 times over the binary list (e.g. CDR-Coding). The third scheme is based on the Dewey Decimal Notation (e.g. tabular representation for the n-ary list) and shows performance speed ups of between 5–7 times over the binary list (CDR-Coding).</abstract>
	<keywords>N-Expression; S-Expression; CDR-Coded lisp cells; Dewey Decimal Notation; CDR-Coding</keywords>
	<publication_month_year>1987-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 3</volumes_issues>
</paper>
<paper no=49>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Developing and running expert systems with PESYS</paper_heading>
	<authors>Georgios I. Doukidis, Edgar A. Whitley</authors>
	<abstract>Recently there has been a growing interest in developing expert system shells that provide a user-friendly interface for the development of expert systems. In this paper we describe PESYS (Pascal Expert SYstem Shell) which is a software tool that enables the user to express his/her knowledge in natural language. In order for PESYS to ‘understand’ the natural language formalisms ‘commonsense’ knowledge is required and a method has been developed which obtains the basic meaning from sentences. We present this method and also how it is accomplished by PESYS.</abstract>
	<keywords>Expert system; model-based diagnosis; electrical system; PESYS (Pascal Expert SYstem Shell) </keywords>
	<publication_month_year>1987-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 3</volumes_issues>
</paper>
<paper no=50>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient processing of integrity constraints in deductive databases</paper_heading>
	<authors>Phillip C. Sheu, W.S. Lee</authors>
	<abstract>One major operation in a deductive database is to verify the contents of the database with integrity constraints whenever the database is changed. The number of facts and integrity constraints in a deductive database often makes the validation process the bottleneck of the system. In this paper, we describe a set of approaches to process integrity constraints efficiently on sequential computers and on massively parallel computers.</abstract>
	<keywords>Deductive database; Efficient processing; Parallel computers</keywords>
	<publication_month_year>1987-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 3</volumes_issues>
</paper>
<paper no=51>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>From visionary ideas to products</paper_heading>
	<authors>John T. Pinkston</authors>
	<abstract>This paper presents the rationale for the creation of the Microelectronics and Computer Research Corporation (MCC), to be a cooperative research organization funded by a number of U.S. computer and electronics companies. Factors inhibiting investment in fundamental research by individual companies today are presented and analyzed, and the benefits of the consortium approach for cooperatively performing basic research and early exploratory development work are described. The present research activities of MCC in the areas of Advanced Computer Architectures, Computer Aided Design, Software Technology, and Packaging and Interconnect technology are presented. This is followed by a discussion of future directions for research in the field of computer science.</abstract>
	<keywords>Microelectronics and Computer Research Corporation (MCC); Advanced Computer Architectures; Computer Aided Design; Software Technology</keywords>
	<publication_month_year>1987-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 4</volumes_issues>
</paper>
<paper no=52>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel inference machines at ICOT</paper_heading>
	<authors>Shunichi Uchida</authors>
	<abstract>Japan's fifth generation computer systems (FGCS) project aims at the research and development of new computer technology for knowledge information processing system (KIPS) that will be required in 1990s. In this project, logic programming is adopted for the base for software and hardware system to be developed. As a primitive operation of logic programming is syllogistic inference, machines studied and built in the project are called inference machines. One of the project's target machines is a parallel inference machine (PIM) having about 1000 processing elements. Smaller scale PIMs are also planned as intermediate targets. In addition to PIMs, sequential inference machines (SIMs) have been developed for a software development tool. A personal type SIM is called PSI which is a logic programming workstation. For research and development of parallel software systems, especially, an operating system for PIM (PIMOS), a multi-PSI system which consists of several CPUs of PSI connected with a high-speed network, is also under development. In the intermediate stage plan of the project, parallel software research is emphasized and conducted more systematically. This paper describes research and development plans for the parallel inference machine in conjunction with the parallel software research.</abstract>
	<keywords>Japan's fifth generation computer systems (FGCS); Knowledge information processing system (KIPS); Parallel inference machine (PIM); Sequential inference machines (SIMs); PIMOS</keywords>
	<publication_month_year>1987-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 4</volumes_issues>
</paper>
<paper no=53>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optical components for digital optical circuits</paper_heading>
	<authors>Brian S. Wherrett, S. Desmond Smith, Frank A.P. Tooley, Andrew C. Walker</authors>
	<abstract>The present status of all-optical computing for digital logic and information processing is reviewed briefly. Computing architectures that take advantage of the parellelism and interconnect freedom that optics offers are addressed. In particular demonstrator circuits employing optically bistable components are described.</abstract>
	<keywords>Optical components; Digital optical circuits; Parellelism</keywords>
	<publication_month_year>1987-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 4</volumes_issues>
</paper>
<paper no=54>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The dutch parallel reduction machine project</paper_heading>
	<authors>H.P. Barendregt, M.C.J.D. Van Eekelen, M.J. Plasmeijer, P.H. Hartel, ... W.G. Vree</authors>
	<abstract>In November 1984 three research groups at the universities of Amsterdam, Nijmegen and Utrecht started a cooperative project sponsored by the Dutch Ministry of Science and Education (Science Council). The first phase lasting until the end of 1987 is a pilot study and has as aim to answer the following question. Is it possible and realistic to construct an efficient parallel reduction machine? The present paper gives an outline of the problems concerning parallel reduction machines and of our research towards their solutions.</abstract>
	<keywords>Dutch parallel reduction machine; Efficient parallel reduction machine</keywords>
	<publication_month_year>1987-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 4</volumes_issues>
</paper>
<paper no=55>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ECRC: A joint industrial research centre</paper_heading>
	<authors>Hervé Gallaire</authors>
	<abstract>This paper describes the policies, strategies and mechanisms of ECRC, as well as its aims and goals. It also surveys its technical program. The paper is non-technical by nature, the emphasis being put on the rationale for such a centre, and on its operations.</abstract>
	<keywords>ECRC; Technical program; Non-technical</keywords>
	<publication_month_year>1987-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 4</volumes_issues>
</paper>
<paper no=56>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance issues in dataflow machines</paper_heading>
	<authors>John Gurd, Wim Bohm, Yong Meng Teo</authors>
	<abstract>Issues affecting the performance of dataflow computers at the machine and language levels are explored. It is suggested that performance is dictated by the nature and the means of identification, distribution and control of workload in the hardware system. Dataflow is an asynchronous concurrent notation based on fine-grain message-passing in graphical programs. Dataflow machines comprise multiple processing elements and structure store modules connected together via a packet-based switching network. Workload is in the form of finegrain data packets which trigger instruction-level activity in the various components of the hardware architecture. Workload is identified by a compiler for a high-level, single-assignment language, and is distributed across the hardware components dynamically at run-time. The amount of work at any instant can be controlled by a parallelism “throttle”. The paper studies the performance of one example of a dataflow computer, the Manchester Dataflow Machine (MDFM).</abstract>
	<keywords>Dataflow computers; Hardware system; Manchester Dataflow Machine (MDFM)</keywords>
	<publication_month_year>1987-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 4</volumes_issues>
</paper>
<paper no=57>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DLM — a powerful ai computer for embedded expert systems</paper_heading>
	<authors>A. Pudner</authors>
	<abstract>There is an increasing need to operate computers in a real-time environment whereby the solution path of the task cannot be pre-determined. Such problems (eg intelligent process control, vision understanding, data fusion etc.) can be solved with an expert system approach that may necessitate interaction with procedural processes. Since expert systems are in general more effectively implemented in declarative languages (eg Lisp, Prolog, Hope), the requirement exists for a powerful AI Computer capable of executing declarative languages and, if necessary, directing the operation of real-time equipment.</abstract>
	<keywords>Declarative Language Machine (DLM); Intelligent process control; Vision understanding; Data fusion</keywords>
	<publication_month_year>1987-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 3, Issue 4</volumes_issues>
</paper>
<paper no=58>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>WAVE-1: A new ideology of parallel and distributed processing on graphs and networks</paper_heading>
	<authors>Peter S. Sapaty</authors>
	<abstract>A new ideology as well as the corresponding language WAVE-1 are presented for highly parallel and distributed processing on large amounts of tightly bound and complex structured data. They are based on parallel propagation in the united space and processing continuum, formed by active data networks, of the special recursively defined wave formulas. These formulas, spreading through the active data on multiple wavefronts, may cause arbitrary complex actions on data including creation, modification or deletion of the passed subnetworks. The proposed approach may be efficiently used in designing the new generation high-performance knowledge base machines as well as for organizing the distributed solution of complex tasks in computer networks, representing distributed information or control systems.</abstract>
	<keywords>WAVE-1; Parallel and distributed processing; Graphs and networks</keywords>
	<publication_month_year>1988-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 1</volumes_issues>
</paper>
<paper no=59>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation and performance of a complex vision system on a systolic array machine</paper_heading>
	<authors>Ed Clune, Jill D. Crisman, Gudrun J. Klinker, Jon A. Webb</authors>
	<abstract>Complex vision system are usually quite slow, requiring tens of seconds or minutes of computer time for each image. As the complexity and experimental nature of the system increases, the speed is especially low, since all components of the system must be optimized if the system is to show good performance. The FIDO system, a stereo vision system for controlling a robot vehicle, has existed for a number of years and has been implemented on a number of different computers. These computers have ranged from a DEC KL10 to the current implementation on the Warp machine, a 100 Million Floating-point Operations Per Second (MFLOPS) systolic array machine. FIDO has shown an enormous range in speed; its ancestor took 15 minutes per step, while the Warp implementation takes less than 5 seconds per step. Moreover, while early versions of FIDO moved in slow, start-and-stop steps, FIDO now runs continuously at 100 mm/second. We review the history of the FIDO system, discuss its implementation on different computers, and concentrate on its current Warp implementation.</abstract>
	<keywords>FIDO system; Complex vision system; Million Floating-point Operations Per Second (MFLOPS); Warp implementation</keywords>
	<publication_month_year>1988-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 1</volumes_issues>
</paper>
<paper no=60>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Cellular array architecture for relational database implementation</paper_heading>
	<authors>Mieczyslaw R. Muraszkiewicz</authors>
	<abstract>The design of high performance database machine architectures is a process wherein the mutual interaction of algorithms, data structures, and technology determines the final result. In the paper fully parallel algorithms for implementing the relational algebra operators are presented. Special emphasis is put on SELECT, PROJECT and JOIN. A relational database machine architecture based upon cellular arrays is proposed. A straight mapping between the relation structure and cellular array geometry, and the fact that cellular arrays can be easily implemented on a commercial scale as VLSI chips are the rationales of this proposal.</abstract>
	<keywords>Database machine architectures; Parallel algorithms; ELECT, PROJECT and JOIN; Array geometry; VLSI</keywords>
	<publication_month_year>1988-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 1</volumes_issues>
</paper>
<paper no=61>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Neurocomputing—neurons as microcomputers</paper_heading>
	<authors>Gen Matsumoto</authors>
	<abstract>Studies on neurocomputing should be directed in two ways which in turn influence each other; In one direction concrete neural-network solutions for specific important problems should be applied to substantiate its practical significance and, at the same time, the theoretical potentialities and limitations for computation be explored on such network models. Along this direction of study, there is a staggering accumulation of results. The alternative way is to seek a more profound understanding of the algorithms used by the CNS (the central nervous system) to process informations and to know more about the molecular and cellular mechanisms underlying specific computations and memory process in neurons and neural networks. From this point of view, artificial neurons developed for neurocomputing are oversimplified to simulate real neurons. Here I will show that neurons are analogous to microcomputers whose characteristics can be classified into over 50 kinds, and that electrical events observed in neurons are a part of the many manifestations associated with neural activities and are regulated by the chemical and conformational processes inside neurons.</abstract>
	<keywords>Neurocomputing; Neural-network; CNS (the central nervous system); Microcomputers</keywords>
	<publication_month_year>1988-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 1</volumes_issues>
</paper>
<paper no=62>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Data flow computing and parallel reduction machine</paper_heading>
	<authors>Makoto Amamiya</authors>
	<abstract>This paper discusses a parallel graph reduction model and its implementation in the relation to the data flow computing scheme. First, a parallel graph reduction mechanism and its relation to data flow computing are discussed. In the discussion, the data flow computing model, which introduces by-reference concept is shown to be a natural implementation of parallel graph reduction model. Then, a practical implementation of a parallel reduction machine is presented. In the implementation, a cell token flow model is proposed. By using the cell token flow concept, a given data flow program can be transformed to an efficient multi-thread control flow program. Last, an overview of the machine architecture is described.</abstract>
	<keywords>Parallel graph reduction model; Dataflow computing; </keywords>
	<publication_month_year>1988-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 1</volumes_issues>
</paper>
<paper no=63>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>BRAVE—a parallel logic language for artificial intelligence</paper_heading>
	<authors>T.J. Reynolds, A.J. Beaumont, A.S.K. Cheng, S.A. Delgado-Rannauro, L.A. Spacek</authors>
	<abstract>This paper presents the first results for the implementation of the logic language BRAVE on a parallel architecture. We explain the operational semantics of BRAVE with common programming examples and show how both and and or parallelism can be exploited and controlled using BRAVE syntax. The design of an abstract machine for the parallel execution of BRAVE is given along with the principles of compilation and example codings. Results are presented from running example programs on a three-processor prototype, using an interpreter for BRAVE written in C.</abstract>
	<keywords>BRAVE;  Parallel logic language; Artificial intelligence; C</keywords>
	<publication_month_year>1988-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 1</volumes_issues>
</paper>
<paper no=64>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Routing management in very large-scale networks</paper_heading>
	<authors>J.J. Garcia-Luna-Aceves</authors>
	<abstract>Very large-scale networks and internetworks are already a reality, and routing constitutes an essential element for the operation of such networks. Unfortunately, the overhead of an adaptive routing algorithm becomes prohibitive in a network or an internetwork with several nodes (in the order of thousands or more) and a flat routing architecture in which each node must know how to reach every other node in the system. In this paper, we discuss the problems with existing network architectures and adaptive routing algorithms when they are applied to very large networks. Then we present a new hierarchical network architecture designed to solve these problems. The two key features of the new architecture are a new loop-free routing algorithm and a new hierarchical addressing mechanism. We compare this architecture with others previously proposed from the standpoint of the savings in routing overhead and the optimality of the paths obtained.</abstract>
	<keywords>Large-scale networks; Adaptive routing algorithm; Routing management</keywords>
	<publication_month_year>1988-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 2</volumes_issues>
</paper>
<paper no=65>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Self-organizing networks</paper_heading>
	<authors>Michael Frankel, Nachum Shacham, James E. Mathis</authors>
	<abstract>With the advent of inexpensive, distributed, powerful computers has come the need for network telecommunications that can transfer data between machines and between users and machines. Fortunately, during the past two decades, major advances have been achieved in telecommunications, ranging from basic improvements in hardware and software design to the creation of such fundamental architectural concepts as the Open System Interconnect (OSI) model developed by the International Standards Organization (ISO). However, although significant progress has been made, one area in telecommunications still requires further development. Until now, we have generally designed protocols for complex telecommunication networks, founded on the assumption that these networkds are static. This assumption facilitates the design of routing algorithms, algorithms for network management, and network topology control. This assumption, however, imposes constraints on the end-user of such networks. Specifically, a user with a high-performance workstation should be able to easily move his/her machine from network to network. However, this movement cannot occur in most networks without first notifying system administrators, in order that appropriate changes in host tables are established and distributed—a process that can take days or weeks to complete. Similarly, many networks today are constrained as to how they can modify their operating parameters (topology, channeltransmission rates, data-flow rates, and alternate routing) to provide service under adverse conditions, such as network-node failures. Note, though, that these constraints are not fundamental to network designs. They have come about primarily because it has been easier to implement ‘first-generation’ networks under these ‘static’ assumptions. However, current telecommunications technology has matured to the point where we can now strive toward developing ‘second-generation’ networks, which we shall call in this paper ‘self-organizing networks’. Our approach to discussing these networks will be to begin with a description of an open system architecture and an example of a network that meets most of the attributes of self-organization. Using these introductory examples, presented in Section 2, we will proceed to discuss the generic issue, ‘binding’, fundamental to self-organizing network designs. We show that if a network is to be self organizing, dynamic binding must be supported. We will then describe research we are pursuing in developing architectures, algorithms, and protocols that permit networks to self-organize. The results of this research will be networks that permit flexible access and that are inherently robust.</abstract>
	<keywords>Open System Interconnect (OSI) model;  International Standards Organization (ISO); Complex telecommunication networks; self-organizing networks </keywords>
	<publication_month_year>1988-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 2</volumes_issues>
</paper>
<paper no=66>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Logic programs directly processed in a network of content addressable memories</paper_heading>
	<authors>Armand De Callatay</authors>
	<abstract>A task-oriented controller to manage learning robots was described in Callataÿ (1986). The main processing element is a phasic categorizer, the output of which is an instantiated symbol transmitted to many other categorizers. Categorizers are the nodes of a directed circular network processor system (DNPS). The impulses (instantiated symbols) concurrently flow within the DNPS loops. The system is radically different from conventional computers and parallel processors. Memory consists of relationships which are stored in the processing elements: these are “recording content addressable memories” (RCAM) which can do only one machine instruction type: “association for production rule processing”. A well-designed DNPS can resolve complex problems, like some rule based or logic programs. Predicates do not appear in the compiled programs. They are replaced by instantiable keywords. Production rules are represented like documents described with keywords and are processed by an information retrieval algorithm. Processing may continue even when no rule matches because analogies may be discovered by partial matching and produce a relevant pseudo-logical solution from incomplete knowledge. The active working memory is matched in parallel with the whole rule base. The DNPS controller is suggested as the central part of a brain model.</abstract>
	<keywords>Directed circular network processor system (DNPS); "Recording content addressable memories” (RCAM); Logic programs</keywords>
	<publication_month_year>1988-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 2</volumes_issues>
</paper>
<paper no=67>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel machine for multiset transformation and its programming style</paper_heading>
	<authors>J.-P. Banâtre, A. Coutant, D. Le Metayer</authors>
	<abstract>One of the most challenging problems in the field of computing science today concerns the development of software for more and more powerful parallel machines. In order to tackle this issue we present a new paradigm for parallel processing; this model, called Γ, is based on the chemical reaction metaphor: the only data structure is the multiset and the computation can be seen as a succession of chemical reactions consuming elements of the multiset and producing new elements according to specific rules. We detail some examples showing the relevance of this model for AI applications. Furthermore, due to its lack of imperative features, this language can be very naturally implemented in a distributed way. We describe an implementation of Γ on a vector architecture. The advantages of this architecture is that it needs very simple processors (with two ports) and all the processors perform the same job. So we advocate the separation of the design of programs for massively parallel machines into two steps which can be verified in a formal way: the construction of a program with implicit parallelism (Γ-program) and its translation into a network of processes.</abstract>
	<keywords>Parallel machines; dynamic data structure; cells, faces, edges; Distributed Point Objects (DPO)</keywords>
	<publication_month_year>1988-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 2</volumes_issues>
</paper>
<paper no=68>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>3-D VLSI technology in Japan and an example: a syndrome decoder for double error correction</paper_heading>
	<authors>Takakazu Kurokawa, Hideo Aiso</authors>
	<abstract>This paper consists of two parts. The first part is a survey of Three-Dimensional (3-D) VLSI technology which is considered to be one of the leading areas among the integrated circuit technologies in the next century. Furthermore the researches of 3-D VLSI in Japan are summarized as well. In the second part of this paper, as a case study, the features of 3-D VLSI are exhibited by testing various layouts of syndrome decoder for double-error correction of linear codes, which can be used for example to improve the reliability of main memories. By the layout results of DEC-BCH decoder, material as well as time savings using 3-D VLSI technology compared to the conventional 2-D VLSI technology are shown.</abstract>
	<keywords>Three-Dimensional (3-D); VLSI technology; DEC-BCH decoder; Conventional 2-D VLSI technology</keywords>
	<publication_month_year>1988-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 2</volumes_issues>
</paper>
<paper no=69>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optical computing in Japan</paper_heading>
	<authors>Toyohiko Yatagai</authors>
	<abstract>Recent research activities for optical computing techniques, including devices, components, systems, and architectures, in Japan are reviewed.</abstract>
	<keywords>Computing techniques; Including devices; Components; Systems; Architectures</keywords>
	<publication_month_year>1988-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 3</volumes_issues>
</paper>
<paper no=70>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optical link in the Delft Parallel Processor — an example of MOMI-connection in MIMD-supercomputers</paper_heading>
	<authors>L. Dekker, E.E.E. Frietman, W. Smit, J.C. Zuidervaart</authors>
	<abstract>Much attention is given to the problem how to avoid transfer-bound processing in MIMD supercomputers. The interactivity between processing tasks is compared with the interconnectability that exists between processors in the case of a multi-bus communication system. For this purpose quantitative measures are introduced for both the task interactivity and the processor interconnectability. For a MIMD computer with p processors and p busses the asymptotic speed-up is proportional to p both for tightly and loosely coupled tasks. For tightly coupled tasks a still larger speed-up can be achieved by taking more than p busses. For a MIMD computer with p processors in the case of 2-level parallelization considerably more speed-up can be obtained, but that requires at the 2nd level of parallelization p powerful interconnects (one per processor). Full processor interconnectability is ideal in the sense that no queueing problems can arise. In case of 1-level parallelization with p processors full interconnectability requires p2 (one-word wide) interconnections. In the Delft Parallel Processor (DPP), instead of using a p2-tuple bus communication system, for this purpose a multibroadcast system with p data channels has been applied, where each data channel has p taps (one per processor) and each processor has a p-tuple accessible input memory (one input per channel). In the DPP84 (with maximally 16 processors) electric data channels have been applied. But for technical reasons for large p full interconnectability is only feasible by the way of optical data channels. This optical interconnect must be provided with electro-to-optic (E/O) and opto-to-electric (O/E) transducers as long as optical computing is not yet practically possible. The resulting Electro-Optic Communication System (EOCS) will be implemented in the DPP8X. The EOCS will consist of a combination of guided-wave and a p-tuple way starcoupler technique. Special E/O and O/E transducers have to be developed as well. An intelligent Opto-Electric logic element, the POWERRAM, is realized as a prototype p-tuple accessible input memory IC, capable to accept the multidata stream at the input of a processor in one clock cycle.</abstract>
	<keywords>MIMD computer; 2-level parallelization; Delft Parallel Processor (DPP); Electro-Optic Communication System (EOCS); POWERRAM</keywords>
	<publication_month_year>1988-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 3</volumes_issues>
</paper>
<paper no=71>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Current status of JUNET</paper_heading>
	<authors>Jun Murai, Akira Kato</authors>
	<abstract>JUNET has been developed in order to provide a testing environment for studies of computer networking ad distributed processing by connecting a large number of computers and by providing actual services for the users. Mechanisms to manage resource naming, Japanese character handling, and fast dial-up link using IP protocol have been developed for the network. In this paper, the current status of JUNET is reported focusing on the above topics. Brief description of native language supports on the network software is also given.</abstract>
	<keywords>JUNET; Computer networking; Distributed processing; IP protocol</keywords>
	<publication_month_year>1988-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 3</volumes_issues>
</paper>
<paper no=72>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Study of mechanisms that support the implementation of nonlogical components of prolog in WAM-based systems</paper_heading>
	<authors>Zhang Chenxi, Tzu Yungui</authors>
	<abstract>Most of the recent efficient Prolog systems are based on the WAM (Warren Abstract Machine) [1]. But the original WAM provides little support for the implementation of nonlogical components of Prolog. In this paper, an execution model for nonlogical components proposed by the authors is presented. The ideas of modifiable code and operation modes for DBOP-predicates are proposed in the paper. The semantics of modifiable procedure code and considerations on the implementations of code databases and source databases are discussed.</abstract>
	<keywords>WAM (Warren Abstract Machine); Efficient Prolog systems; DBOP</keywords>
	<publication_month_year>1988-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 3</volumes_issues>
</paper>
<paper no=73>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A reduction architecture for the optimal scheduling of binary trees</paper_heading>
	<authors>K. Ravikanth, P.S. Sastry, K.R. Ramakrishnan, Y.V. Venkatesh</authors>
	<abstract>This paper addresses the problem of designing a parallel reduction architecture for applicative languages. An interconnection network that allows for scheduling of binary trees of arbitrary depth is presented. It is shown that using a static scheduling strategy the architecture achieves optimal performance while scheduling complete binary trees. Some issues related to the design of a machine based on this network are also discussed.</abstract>
	<keywords>Parallel reduction architecture; Optimal scheduling; Binary trees</keywords>
	<publication_month_year>1988-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 3</volumes_issues>
</paper>
<paper no=74>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A recursively scalable network VLSI implementation</paper_heading>
	<authors>G. Della Vecchia, C. Sanges</authors>
	<abstract>Object of the present work is to briefly introduce a new general class of recursively scalable network topologies for message-passing architectures, referred to as WK-recursive topologies, proposed by the authors, and to describe the VLSI implementation of a regular, direct WK-recursive based network realized at the Hybrid Computing Research Center.</abstract>
	<keywords>Recursively scalable network; VLSI implementation; WK-recursive topologies; Hybrid Computing Research Center. </keywords>
	<publication_month_year>1988-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 3</volumes_issues>
</paper>
<paper no=75>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A VLSI implementation of an architecture for applicative programming</paper_heading>
	<authors>John T. O'Donnell, Timothy Bridges, Sidney W. Kitchel</authors>
	<abstract>The Applicative Programming System Architecture contains a novel Data Structure Memory (DSM) which supports fast access operations on compact linear data structures. Several problems that arise in implementations of applicative and functional programming languages can be solved efficiently using special data representations on the DSM. Each memory word in the DSM contains a very small local processor, and there is also a tree-structured communications network within the DSM. Therefore the DSM is a massively parallel SIMD machine. This paper describes a VLSI implementation of the DSM architecture and compares its performance with implementations on a conventional sequential computer and the NASA Massively Parallel Processor.</abstract>
	<keywords>Data Structure Memory (DSM); Parallel SIMD machine; VLSI implementation; NASA Massively Parallel Processor.</keywords>
	<publication_month_year>1988-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 3</volumes_issues>
</paper>
<paper no=76>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Algorithms for solving numerical linear algebra problems on supercomputers</paper_heading>
	<authors>T.J. Dekker, W. Hoffmann, P.P.M. De Rijk</authors>
	<abstract>In this paper some numerical algorithms are considered in relation to computations on vector and parallel processors. Moreover, some results of experiments on a vector computer are reported. The algorithms considered are Gaussian elimination and Gauss-Jordan elimination for solving full linear systems, and Hestenes' one-sided Jacobi iteration to calculate the Singular Value Decomposition of a matrix.</abstract>
	<keywords>Numerical algorithms; Parallel processors; Solving full linear systems</keywords>
	<publication_month_year>1989-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 4</volumes_issues>
</paper>
<paper no=77>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel branch-and-bound algorithms</paper_heading>
	<authors>J.M. Jansen, F.W. Sijstermans</authors>
	<abstract>In this paper a parallel algorithm for branch-and-bound problems is sketched. It is designed to run on MIMD machines and exploits coarse grain parallelism. Due to the irregular and unpredictable behavior of branch-and-bound algorithms, it is hard to obtain a good load-balance. Another design issue is the minimization of the communication overhead. Our algorithm overcomes these problems by a dynamic load-balancing strategy and a dynamic way to decide when communication is really useful. After first sketching a general parallel algorithm for branch-and-bound problems, we concentrate on a particular instance of a branch-and-bound problem: the so-called knapsack problem. The performance of the algorithm for this problem is measured with a simulator for a multiprocessor system.</abstract>
	<keywords>Parallel algorithm; Branch-and-bound problems; MIMD machines </keywords>
	<publication_month_year>1989-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 4</volumes_issues>
</paper>
<paper no=78>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Some experience in distributed programming using shortest path spanning trees</paper_heading>
	<authors>Johan J. Lukkien</authors>
	<abstract>In this paper we consider a class of distributed algorithms. Algorithms in this class consist of processes that communicate using a broadcast. We show that only some local information suffices to implement such an algorithm on an arbitrary network. For a number of implementations a theoretical time complexity is derived and some experimental results are given.</abstract>
	<keywords>Distributed algorithms; Path spanning trees;  Arbitrary network</keywords>
	<publication_month_year>1989-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 4</volumes_issues>
</paper>
<paper no=79>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Practical aspects of parallel scientific computing</paper_heading>
	<authors>Henk A. Van Der Vorst</authors>
	<abstract>In this paper we study the parallel solution of a very simple problem, namely the solution of a lower bidiagonal system. It will appear that this problem exhibits many aspects of parallel processing and it has the advantage that the scene is not obscured by complicated algorithmic aspects. The following aspects of the parallel solution methods will be regarded: numerical stability, complexity, data organization, implementation and performance.</abstract>
	<keywords>Parallel scientific computing; Parallel solution; Lower bidiagonal system</keywords>
	<publication_month_year>1989-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 4</volumes_issues>
</paper>
<paper no=80>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Solving the least squares problem using a parallel linear algebra library</paper_heading>
	<authors>Johannes G.G. van de Vorst</authors>
	<abstract>One of the important issues in the construction of a parallel Linear Algebra library is the choice of the process structure. The structure that is presented in this article allows for simple functional specifications of the processes and for their compositionality. Each functional specification describes the meaning of the parallel composition of a number of instances of a single process. The communication behaviours of the instances of a process do not occur in its specification. In such a specification, matrices and vectors occur as ordinary mathematical variables. The representations of the matrices and vectors are distributed across the process instances. All library processes conform to the same communication conventions. The library processes can be composed sequentially, without requiring global synchronisation between process calls. As an example, the parallel solution of the least squares problem is discussed.</abstract>
	<keywords>Squares problem; Parallel linear algebra library; Parallel composition </keywords>
	<publication_month_year>1989-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 4</volumes_issues>
</paper>
<paper no=81>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experiments with coarse-grain parallel graph reduction</paper_heading>
	<authors>W.G. Vree</authors>
	<abstract>A reduction model based on parallel reduction of annotated coarse-grain subexpressions has been developed. In this model a kind of string-reduction is proposed on the level of parallel tasks. A special reduction strategy avoids the duplication of work normally incurred by string reduction. Within parallel tasks pure graph-reduction is performed. An experimental parallel machine has been constructed. Based on a technique of hybrid simulation speed-up figures of medium-size application programs have been measured on the experimental machine.</abstract>
	<keywords>Parallel reduction model; Coarse-grain subexpression; Parallel graph reduction</keywords>
	<publication_month_year>1989-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 4</volumes_issues>
</paper>
<paper no=82>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Targets and results from phase one and two of the fifth generation computer systems study</paper_heading>
	<authors>Takashi Kurozumi, Takao Ichiko</authors>
	<abstract>This article presents an introduction to the fifth generation computer systems. Fifth generation computers are under development to meet the knowledge information processing needs for the advanced information-oriented society of the 1990s. This clearly requires overcoming the technical limitations of conventional computers. A fifth generation computer requires problem solving and inference functions for inference of solutions to problems from accumulated knowledge, knowledge base management functions for acquirement and retrieval of knowledge, intelligent interface functions for fluent conversations using natural language, drawings, images, etc., and intelligent programming functions for automatic conversion of problem specifications submitted in natural language, drawing etc., into efficient programs. Fifth generation computers with these functions will have far-reaching impact finding applications in all areas of industry and society. However, research and development (R&D) of these computers requires an extremely wide variety of leading-edge technology, and the risks are high. Accordingly, a long-term R&D plan was established for this project that extends over ten years with the initial stage lasting from 1982 to 1984, the intermediate stage from 1985 to 1988, and the final stage will begin in 1989 and the project will end in 1991.</abstract>
	<keywords>Fifth generation computer systems; Research and development (R&D); Natural language</keywords>
	<publication_month_year>1989-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 4, Issue 4</volumes_issues>
</paper>
<paper no=83>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A brief overview of the current status of expert systems in Japan</paper_heading>
	<authors>Riichiro Mizoguchi</authors>
	<abstract>A number of expert systems have been built in many Japanese industries, and more than 250 systems are used in practice. Japan has caught the “AI fever” since the 5th generation computer project has been set up, though it is currently abating. People have realized the utility of expert systems and are developing advanced techniques to make expert sytems really useful. A brief overview of the current Japanese status of expert systems is presented here.</abstract>
	<keywords>Expert systems; AI fever</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=84>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An intelligent on-line help system: ASSIST</paper_heading>
	<authors>Kuniaki Uehara</authors>
	<abstract>This paper describes an intelligent on-line help system, ASSIST, which is designed to produce multi-sentential explanations in response to user's questions about computer terminology. One of the significant features of ASSIST is to take into account the user's knowledge when generating explanations. In order for ASSIST to have the ability to provide different explanations in accordance with the user's level of expertise, a user model is constructed throughout a session, and it is utilized to accomplish text planning to achieve the user's goal extracted from the question. In other words, during the course of text planning, ASSIST examines the user model, determines how much information is sufficient for the user, and cuts off unimportant and redundant concepts which the user already knows. Another feature of ASSIST is that focusing information is introduced to ensure that the generated explanation is coherent. Since the relationship between the resultant concepts cannot be specified by themselves, focusing information is utilized to decide on how to express them in the explanation (i.e. pronominalization and passive/active construction).</abstract>
	<keywords>An intelligent on-line help system (ASSIST); Computer terminology</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=85>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A technical analysis expert system in the stock market</paper_heading>
	<authors>Takahira Yamaguchi</authors>
	<abstract>In order to build a technical analysis expert system for judging which names should be bought in stock market, knowledge representation is discussed. The expertise comes from two kinds of knowledge: Granville's Laws from the USA and heuristics relevant to the chart indigenous to Japan. The hit rate for buying is evaluated by means of checking that selected names increase more than five per cent through two months after the point of time when the expert system judged buying. Using past real stock price data, the experiment shows that the hit rate was 53.1 per cent on the average.</abstract>
	<keywords>Technical analysis expert system; Stock market; Granville's Laws </keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=86>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A knowledge-based system for the wastewater treatment plant</paper_heading>
	<authors>Kazuo Maeda</authors>
	<abstract>This paper discusses a knowledge based system for the wastewater treatment plant. Our method gains from qualitative and experienced judgement of the operator in supervisory work such as set point scheduling, plant diagnosis and maintenance. First, we explain an architecture which consists of two parts such as an adaptive production system and a multimodal user interface e.g. video graphics, voice announcement, touch panel and mouse. Secondly we demonstrate the performance of our system. Finally we conclude that the proposed system would be a flexible and practical decision support system.</abstract>
	<keywords>Knowledge based system; Wastewater treatment plant; Adaptive production system; Practical decision support system</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=87>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ES/SDEM Software development engineering methodology for expert systems</paper_heading>
	<authors>Shunji Matsumoto</authors>
	<abstract>ES's (Expert Systems) are being developed by an increasing number of organizations, and more people are interested in this new technology. While knowledge representation and AI tools are discussed among AI enthusiasts, the notion of an ES building methodology still seems to be widely considered as an artistic endeavour. To a certain extent, this is true, because KA (Knowledge Acquisition) includes artistic aspects. However, it is important to provide a methodology to standardize the ES development process, to reduce the workload of KE's (Knowledge Engineers), and to educate new KE's. We have developed ES / SDEM (Software Development Engineering Methodology for Expert Systems) by examining successful case histories from among 120 ESHELL∗ (FUJITSU EXPERT SHELL) applications. In this paper, the outline of ES / SDEM will be given.</abstract>
	<keywords>ES's (Expert Systems);  AI enthusiasts; KA (Knowledge Acquisition); KE's (Knowledge Engineers); ES / SDEM (Software Development Engineering Methodology for Expert Systems)</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=88>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A knowledge-based framework in an intelligent assistant system for making documents</paper_heading>
	<authors>Ikuo Keshi, Naoyuki Fukuda, Yoshiji Fujimoto</authors>
	<abstract>We have developed a prototype of an intelligent assistant system for making documents in a specific domain. For one example, the domain of Japanese business letters is chosen, because it contains such formal sentences that sophisticated writing skill is required. Then we have implemented the prototype system in Prolog. The system is designed to help novices make business letters efficiently by retrieving the relevant and associated paragraphs from a number of examples in a text database. In this paper, first the associative retrieval mechanism, which is realized by using a retrieval technique on the basis of keywords, is presented. And then, a realistic approach for a content extraction and documents summarizing method, which aims at content retrieval, is described.</abstract>
	<keywords>Knowledge-based framework; Intelligent assistant system; Prototype system</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=89>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Development of a model-based intelligent training system</paper_heading>
	<authors>Masahiro Inui, Nobuji Miyasaka, Kazuhiko Kawamura, John R. Bourne</authors>
	<abstract>This paper describes an effort to develop an artificial intelligence-based generic training system (GTS) for industrial uses. Through the use of artificial intelligence technology and interactive simulation tools such as qualitative modeling, it is possible to build robust, individualized training systems. Topics covered in this paper include the system architecture, system modules and the main features. As an example, a model-based intelligent tutoring system (ITS) for a power distribution training system (PDTS) is also provided.</abstract>
	<keywords>Artificial intelligenc (AI); Generic training system (GTS); Intelligent tutoring system (ITS); Power distribution training system (PDTS)  </keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=90>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Expert systems application in plant engineering</paper_heading>
	<authors>Masataka Hiraide</authors>
	<abstract>As an example of expert systems application in plant engineering a hybrid expert system that contains both qualitative and quantitative knowledge is introduced. It is a heat exchanger design expert system (HTEXT) which has characteristic features of both derivation and formation type expert system and is incorporated in the Computer Aided Engineering (CAE) system to facilitate higher quality engineering.</abstract>
	<keywords>Expert systems application; Heat exchanger design expert system (HTEXT); Computer Aided Engineering (CAE) system</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=91>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An expert system for computer operation and user assistance</paper_heading>
	<authors>Jun Takamura</authors>
	<abstract>Various kinds of knowledge are required in order to best utilize and operate a complex computer system. Composer is a knowledge-based system with the necessary knowledge required for such operation and utilization, and, as part of a mainframe operating system, assists in tasks of operation and utilization such as system performance analysis, troubleshooting and program development.</abstract>
	<keywords>Operate a complex computer system; Knowledge-based system; Mainframe operating system</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=92>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automobile troubleshooting expert system “ATREX”</paper_heading>
	<authors>Riichi Takahashi</authors>
	<abstract>The paper reports on an expert system called ATREX (Automobile TRoubleshooting EXpert system) currently under development, which assists a service mechanic in diagnosing vehicle faults. With the recent increase in complexity of vehicles, technicians are having difficulty in keeping up with the latest automobile technology, especially regarding electronic components. Expert systems techniques, developed in Artificial Intelligence research, are expected to reduce this difficulty by means of a new way to deal with the expertise by computer. The BRAINS rule-based knowledge engineering language of the EXPERT type was chosen as our tool, in which the expertise is stylized into IF-THEN rules and stored in the knowledge base. A fault-symptom matrix representing relations between conceivable faults and observable symptoms is helpful in organizing the expertise into the IF-THEN rules. Compared with conventional programs, the knowledge-based system was found to be both flexible and maintainable.</abstract>
	<keywords>Expert system; ATREX (Automobile TRoubleshooting EXpert system); Automobile technology; RAINS; IF-THEN</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=93>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An expert system for fault diagnosis at petrochemical plants</paper_heading>
	<authors>Yoshitomo Hanakuma</authors>
	<abstract>An approach to fault diagnosis at petrochemical plants by expert systems is presented. These systems consist of an empirical and theoretical knowledge data base, and are applied to the Cyclohexane plant process. As a result, the expert system is a highly effective tool for supporting process operation.</abstract>
	<keywords>Expert systems; petrochemical plants; Cyclohexane plant</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=94>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A knowledge-based directory assistance system</paper_heading>
	<authors>Minoru Ohyama</authors>
	<abstract>This paper discusses the possibility of applying a knowledge-based system to communication services. Especially, a telephone directory assistance system, which is one of the representative intelligent communication services is discussed. First, it describes retrieval techniques and evaluation in a computer-aided keyword-based telephone directory assistance system which started in 1986. Owing to this system, retrieval performance and mean answering time are greatly improved. Furthermore, a knowledge-based telephone directory assistance system employing a natural language is discussed. In the knowledge-based system, an efficient understanding method of communication object expressed by the natural language is proposed.</abstract>
	<keywords>Knowledge-based system; telephone directory assistance system; Natural language</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=95>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A rule-based and algorithmic approach for logic synthesis</paper_heading>
	<authors>Takeshi Yoshimura</authors>
	<abstract>This paper presents a logic synthesis system based on a combined rule-based and algorithmic approach, where not only tables for transformation are described as rules, but also a two level logic minimization algorithm is registered as one of the rules. A rule interpreter fires these rules using an effective branch and bound technique. Physical constraints such as longest path lengths between registers, fan-in/out and polarity are checked whenever each rule is applied. The system was implemented in the C language on a SUN workstation and has been applied for actual circuits used in production. The results show that the system generates solutions very close to the manual implementation. They also show that the system with the logic minimization algorithm was skipped.</abstract>
	<keywords>Logic synthesis system; Combined rule-based and algorithmic approach; C language; SUN workstation</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=96>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Intelligent information systems for production management in agriculture and horticulture</paper_heading>
	<authors>Toyoki Kozai, Takehiko Hoshi</authors>
	<abstract>Some ideas on how to integrate recent rapid technical advances in computer application, biotechnology, and natural energy utilization are proposed. Then, outlines are described of (1) a hierarchically distributed knowledge-based information system for controlling a greenhouse complex, (2) an expert system development software system: MICCS, and (3) a tomato crop disease diagnosis expert system developed using MICCS.</abstract>
	<keywords>Intelligent information systems; Hierarchically distributed knowledge-based information system; Expert system development software system: MICCS</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=97>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Application of an expert system to blast furnace operation</paper_heading>
	<authors>Katsuhiko Yui, Satoshi Watanabe, Shigeru Amano, Tsuyoshi Takarabe, Takashi Nakamori</authors>
	<abstract>Applications of expert systems in the steel industry are described and a practical expert system is introduced which has been developed for blast furnace operation in the Kimitsu Works of Nippon Steel Corporation.</abstract>
	<keywords>Expert system; Blast furnace operation; Nippon Steel Corporation</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=98>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automatic programming by composing program components and its realization method</paper_heading>
	<authors>Seiichi Komiya</authors>
	<abstract>There are several methods of automatic programming, but composing program components is the only method that can automatically create large programs in a procedural language. In practical application domains, many people believe that developing a tool for developing and reusing program components is better than developing an automatic programming system. However, experiments in developing automatic programming systems actually by composing program components, suggest that this method can more efficiently support the development of practical systems. This paper presents studies derived from several experiments, and a method for developing an automatic programming system by composing program components.</abstract>
	<keywords>Automatic programming; Composing program components; Realization method; Procedural language</keywords>
	<publication_month_year>1989-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 1</volumes_issues>
</paper>
<paper no=99>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Grand challenges to computational science</paper_heading>
	<authors>Kenneth G. Wilson</authors>
	<abstract>Computational Science is at the very beginning of centuries of growth, comparable to the four centuries of experimental advances since Galileo. The Grand Challenges to Computational Science are unsolved scientific problems of extraordinary breadth and importance which will demand continuing computational advances throughout the forthcoming computational era. Supercomputers can be used to see phenomena not directly accessible to experiment in key scientific and engineering areas such as atmospheric science, astronomy, materials science, molecular biology, aerodynamics, and elementary particle physics. However, the benefits of supercomputers will be greatly increased if some major difficulties are overcome. In this paper, I address some of the tougher requirements on current grand challenge research to ensure that it has enduring value. The problems of algorithm development, error control, software productivity, and the fostering of technological advances are especially important.</abstract>
	<keywords>Grand challenges; Computational Science; Atmospheric science; Astronomy; Materials science; Molecular biology</keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=100>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Supercomputing and biomedical science</paper_heading>
	<authors>Ralph Roskies</authors>
	<abstract>Supercomputing has already had an impact on biomedical research. It will become even more important in meeting the grand challenge of improving our public health. I illustrate the impact and point out opportunities for further contributions in selected examples, including the human genome project, crystallography and the structure of proteins, heart modelling, prosthetic design, and the epidemiology of AIDS.</abstract>
	<keywords>Supercomputing; Biomedical science; Crystallography;  AIDS</keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=101>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Grand challenges in computational science: Simulations on biological macromolecules</paper_heading>
	<authors>Peter Kollman</authors>
	<abstract>We give a personal overview of the major problems in carrying out accurate and predictive computer simulations on biological macromolecules. The grand challenges include: (1) finding the collection of lowest energy minima including the global minimum for systems of many degrees of freedom; (2) accurately evaluating the energy for a large collection of reacting atoms in the presence of thousands of “classical” non-reacting atoms; and (3) calculating the free energies for important processes.</abstract>
	<keywords>Computational science; Biological macromolecules; Computer simulations; </keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=102>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Some computational studies in molecular physics</paper_heading>
	<authors>Vincent McKoy, Richard L. Dubs, S.N. Dixit</authors>
	<abstract>In this paper we will discuss some results of our recent studies of molecular photoionization. An important objective of these studies has been to provide much-needed insight into the underlying dynamics of these processes and a robust analysis and prediction of key features of the photoionization spectra of molecules. A significant requirement in such studies is that they be carried out using accurate molecular orbitals to describe the quantum mechanical motion of the photoelectron as it moves in the field of the molecular ion. Complexities arising from the nonspherical character of molecular ion potentials in the solution of the equations for these photoelectron orbitals have been a major hurdle and computational challenge. Modern high-speed computing has been a critical factor in our ability to solve these equations and to study features in photoionization spectra where the use of such orbitals is essential and not just cosmetic. Examples which we will use to illustrate this point have to do with the way photoelectron angular distributions for diatomic molecules, either fixed in space or with preferred directions in space, depend on the left- and right-handedness of the circularly polarized radiation producing the ionization, i.e. exhibit a dichroic asymmetry. Furthermore, we will show that a credible determination of the magnitude of this asymmetry in such photoelectron angular distributions was essential in establishing that measurements of this type were both feasible and useful.</abstract>
	<keywords>Molecular physics; Quantum mechanical; Computational challenge</keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=103>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational challenges in aerospace</paper_heading>
	<authors>Victor L. Peterson</authors>
	<abstract>Computer speed and memory requirements needed for meeting various computational challenges in human vision modeling, chemistry, turbulence physics research and aerodynamics are discussed and compared with the capabilities of various existing computers and those projected to be available before the mid 1990s. Example results for problems illustrative of those currently being solved in each of the disciplines are also presented. Meeting some of the challenges using currently available solution algorithms is shown to require computer speeds in excess of exaFLOPs (1018 FLOPs) and memories in excess of petawords (1015 words), if problems are to be solved in periods of time currently believed to be acceptable. Even without these levels of computer power, it is shown how work can proceed towards meeting the ultimate challenges by treating stepping-stone problems with complexity increasing to match the computational power available at any point in time. Finally, it is speculated that improvements in algorithms ultimately will reduce these requirements to levels that can be met with computers projected to be available beyond the year 2000.</abstract>
	<keywords>Computational challenges; Aerospace; Human vision modeling; Chemistry; Turbulence physics research; exaFLOPs</keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=104>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The earth system</paper_heading>
	<authors>Francis P. Bretherton</authors>
	<abstract>The ozone hole and greenhouse warming are but examples of changes taking place in our global environment at an unprecedented rate — changes that will have profound implications for our children and grandchildren. They present to the scientists of the world a tremendous challenge: to develop an understanding of the underlying interactions between human activities and the functioning of the complete Earth system — its atmosphere, oceans, ice, solid Earth and biota. Central to the strategy for developing a knowledge base for decision makers now and in the future are comprehensive numerical models of this system. Such models will constantly strain the capacity of the most powerful computers available. Their development also requires techniques for handling a wide range of spatial and temporal scales and stiff systems of equations, the processing of very large volumes of data, and advanced distributed data management and information systems.</abstract>
	<keywords>Earth system; Comprehensive numerical models; Advanced distributed data management; </keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=105>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The impact of supercomputing capabilities on U.S. materials science and technology</paper_heading>
	<authors>William D. Wilson, Robert J. Asaro, Robert W. Dutton, Juan M. Sanchez, ... Wilhelm</authors>
	<abstract>A committee was formed under the auspices of the National Academy of Sciences to identify areas of materials science and engineering where a major impact might be realized, resulting from the emergence of supercomputer technology. A great number of examples of exciting individual computational science were identified: Atomistic and electronic structure calculations on metals, semiconductors, and polymers; statistical mechanical studies of alloy phase diagrams; and fundamental modeling of fracture and deformation in metals, ceramics, and glasses are among those that are vigorously exploiting supercomputer technology. Moreover, electronic structure effects on a scale of angstroms are being coupled with microstructural aspects on a scale of micrometers, and these are further coupled to continuum effects on a scale of centimeters. Supercomputers are emerging as powerful and cost-effective tools, not only for the furtherance of materials science, but also for linking this science with engineering, design, and manufacturing. In this paper each example stands alone, consisting of a brief technological background followed by the specific supercomputer examples.</abstract>
	<keywords>Supercomputer technology; Semiconductors; Polymers; Statistical mechanical</keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=106>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational modeling of microstructures</paper_heading>
	<authors>P.E. McHugh, A.G. Varias, R.J. Asaro, C.F. Shih</authors>
	<abstract>This article describes four examples of analysis of the deformation and failure of microstructures. These examples are part of a wider study of micromechanical processes in materials within, what amounts to, a new field, Computational Micromechanics. The first two are concerned with the development of shear bands in ductile single crystals and polycrystals. The third describes the deformation and failure of composite materials. The fourth deals with the failure of interfaces through separation. Physical theories for crystallographic slip and interface decohesion have been formulated and implemented in finite element models. Supercomputers allow these theories to be applied to problems involving large geometries and complex deformation modes.</abstract>
	<keywords>Computational modeling; Computational microstructures; Crystallographic</keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=107>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Exploiting parallelism in computational science</paper_heading>
	<authors>Gary M. Johnson</authors>
	<abstract>The full exploitation of numerical simulation as an independent approach to the solution of engineering and scientific problems requires computing capability far exceeding that which is presently available. In this paper, the computing requirements posed by challenging problems in several disciplines are examined and contrasted with contemporary supercomputer resources. Of the means available to help fill the gap between the demands of computational science and the performance level of present-generation supercomputer systems, parallel processing appears to have the greatest potential for near-term success. Parallel computer architectures are reviewed and categorized according to processing units, memory, and interconnection scheme. Philosophies of parallel processing are discussed. They are distinguished by the number and size of the parallel tasks which they employ. Scientific problems are examined for parallelism inherent at the physical level. Typical algorithms and their mappings onto parallel architectures are discussed. Computational examples are presented to document the performance of scientific applications on present-generation parallel processors. Projections are made concerning software developments and machine architectures.</abstract>
	<keywords>Numerical simulation;  Present-generation supercomputer systems; Parallel processing</keywords>
	<publication_month_year>1989-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issues 2–3</volumes_issues>
</paper>
<paper no=108>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An optimized broadcasting technique for WK-recursive topologies</paper_heading>
	<authors>G Della Vecchia, C Sanges</authors>
	<abstract>This paper describes a study carried out on behalf of a research within the Strategic Project on Parallel Computation, supported by the National Research Council of Italy, where the Hybrid Computing Research Center is collaborating with the Department of Computer Science of the University of Pisa on the design of a massively parallel system based on VLSI components having a direct network message passing architecture. Within this project the Authors purposely devised a new general class of communication network topology they called WK-recursive, as an effort to meet the demand of scalable and efficient communication structures for very large parallel systems. Object of the present paper is to describe an optimized technique to perform effective broadcasting operations on networks belonging to the WK-recursive class, one prototype of which has been realized at the Hybrid Computing Research Center. This technique is characterized by a number of properties which will be profitably exploited in the design of the distributed operating system for such a massively parallel system, especially as far as the kernel run-time support is concerned.</abstract>
	<keywords>Parallel Computation; WK-recursive topologies; VLSI components; Parallel systems</keywords>
	<publication_month_year>1990-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 4</volumes_issues>
</paper>
<paper no=109>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The monitoring of inter-process communications in distributed systems</paper_heading>
	<authors>U de Carlini, R Vaccaro, U Villano</authors>
	<abstract>The decomposition of an application into processes and the allocation of the latter to the available processors is one of the most crucial problems in the software design phase of message-based distributed systems. In this paper the authors describe a tool which, by monitoring the communications between processes allocated to separate processing elements, is able to provide indications on the effectiveness of the partitioning and mapping actions carried out. The authors first show, with reference to systems built with Transputer family components, the tool's structure, which is highly independent from the system interconnection network; secondly, they show how to make an initial reduction of the data gathered during the monitoring phase.</abstract>
	<keywords>message-based distributed systems; Monitoring  communications; Mapping actions</keywords>
	<publication_month_year>1990-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 4</volumes_issues>
</paper>
<paper no=110>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel software development in the DISC programming environment</paper_heading>
	<authors>G Iannello, A Mazzeo, C Savy, G Ventre</authors>
	<abstract>This paper describes the architecture of DISC, a system for parallel software development. The system is designed for programming computer systems having several autonomous units, not memory-sharing, and linked by means of a communication network. The system consists of three parts. The concurrent programming language DISC (DIStributed C), which is an extension of the C language based on the concurrent mechanisms envisaged by the CSP computational model. The programming environment, designed to promote software engineering techniques in the development of distributed-programs. The language run-time support, which provides for the distributed execution of programs.</abstract>
	<keywords>Parallel software development; Communication network; DISC (DIStributed C); CSP computational model</keywords>
	<publication_month_year>1990-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 4</volumes_issues>
</paper>
<paper no=111>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Two-level grammars for data conversions</paper_heading>
	<authors>Manfred Ruschitzka</authors>
	<abstract>Heterogeneous computer systems that are interconnected in today's computer networks lack efficient, general-purpose translation facilities for remote data. The provision of such facilities is potentially quite costly since the format of remote data is a function of the attributes of the remote architectures, operating systems, and software applications that maintain the data. This paper introduces a novel translation technique that parameterizes the translation process according to these attributes. Its formal specification is based on environment grammars, parameterized two-level grammars that lend themselves to the specification of classes of data languages with similar structures. We present a formal definition of environment grammars, discuss properties that permit their efficient parsing, and describe a data parsing method based on these properties. Examples illustrate the use of environment grammars for two different types of data languages. The viability of this parameterized technique has been demonstrated by an operational translation subsystem for data of heterogeneous relational database management systems.</abstract>
	<keywords>Heterogeneous computer system; Novel translation technique; Parameterized two-level grammars; Data parsing method</keywords>
	<publication_month_year>1990-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 4</volumes_issues>
</paper>
<paper no=112>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SUPRENUM - A European made supercomputer</paper_heading>
	<authors>Bernd Schwister, Karl Solchenbach</authors>
	<abstract>SUPRENUM is the German supercomputer project aiming at the development and construction of a distributed-memory multiprocessor system. Within the SUPRENUM project many application codes are either parallelized or completely new developed. Using the concepts of the Abstract SUPRENUM Machine and some programming environment tools these applications can be parallelized rather easily and straightforward.</abstract>
	<keywords>SUPRENUM; German supercomputer project; Distributed-memory multiprocessor system.</keywords>
	<publication_month_year>1990-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 5, Issue 4</volumes_issues>
</paper>
<paper no=113>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation of prolog databases and database operation builtins in the WAM-Plus model</paper_heading>
	<authors>Zhang Chenxi, Ci Yungui, Liu Bo</authors>
	<abstract>Prolog databases and database operation built-ins are very important components of Prolog systems. However, they are much more difficult to be implemented in compiler-based systems than in interpreter-based systems. In this paper, schemes for managing Prolog databases and algorithms for implementing database operation built-ins in the framework of the WAM-Plus are presented.</abstract>
	<keywords>Prolog databases; Database operation built-ins; Prolog systems; WAM-Plus </keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=114>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An integrated implementation of prolog database operations</paper_heading>
	<authors>Li Liangliang, Ci Yungui</authors>
	<abstract>In a compiler-based Prolog system, the Prolog database virtually consists of two components, a code database to store compiled clauses, and a source database to retain, in some way, the source forms of the clauses. An integrated database architecture is presented, where the two sub-databases are tightly combined at the clause level, and therefore can be managed uniformly. Based on the architecture, a uniform implementation of Prolog database operations is then presented. With respect to the architecture, the management, and the implementation of clause indexing, some design alternatives are also discussed.</abstract>
	<keywords>Compiler-based Prolog system; Prolog database operations; Compiled clauses</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=115>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Clause representations in a compiler-based prolog database</paper_heading>
	<authors>Li Liangliang, Ci Yungui</authors>
	<abstract>Methods of how clauses are represented in compiler-based Prolog systems are presented. There are three different methods so far proposed, i.e. source-copying based, source-assertion based and decompilation based. Each of the three is investigated, and related implementation techniques and algorithms are presented in detail. Also presented is a simple comparison of the three methods in terms of space and time overheads.</abstract>
	<keywords>Compiler-based Prolog systems; Clause representations; Implementation techniques</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=116>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The OR-forest-based parallel execution model of logic programs</paper_heading>
	<authors>Sun Chengzheng, Ci Yungui</authors>
	<abstract>A new framework for parallel execution of logic programs is presented in this paper, including the OR-forest description and the abstract process model. The OR-forest description cannot only retain the merits of the OR-tree description in achieving high independence of computation units and high degree of OR-parallelisms, but also overcome two drawbacks of the OR-tree description: the lack of the ability for describing AND-parallel execution and the redundancies in describing OR-parallel execution. In our abstract process model, AND-parallelisms are realized by processes simultaneously searching multiple trees, and OR-parallelisms are realized by processes simultaneously searching multiple branches of the trees. Software simulations have shown that the OR-forest-based execution system, incorporated with an automatic partition algorithm, can exploit AND-OR-parallelisms in both deterministic and nondeterministic programs.</abstract>
	<keywords>Parallel execution; Logic programs; OR-forest description; OR-tree; OR-parallelisms</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=117>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A compiling approach for exploiting AND-parallelism in logic programs</paper_heading>
	<authors>Hwang Zhiyi, Hu Shouren</authors>
	<abstract>In this paper, we present a compiling approach for exploiting AND-parallelism in logic programs. The approach consists of three phases: analysis of entry modes; derivation of exit modes; and determination of execution graph expressions. Compared with other approaches [1–4], this approach, with the compile-time program-level data-dependence analysis of logic programs, can more efficiently exploit AND-parallelism in logic programs. Two precompilers, based on our approach and DeGroot's approach [2] respectively, have been implemented in the SES-PIM system [9]. Through compiling and running some typical benchmarks in SES-PIM, we conclude that our approach can, in most cases, exploit as much AND-parallelism as the dynamic approach [10] does under “producer-consumer” scheme, and needs less dynamic overhead while exploiting more AND-parallelism than DeGroot's approach.</abstract>
	<keywords>Compiling approach; AND-parallelism in logic programs; DeGroot's; SES-PIM system</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=118>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Hierarchy and statistical heuristic search</paper_heading>
	<authors>Bo Zhang, Ling Zhang</authors>
	<abstract>In this paper we present a framework for a quotient structure model of hierarchy. As an application of this model, a new statistical heuristic search technique is introduced. Its computational complexity is discussed theoretically and some experimental results are showed as well.</abstract>
	<keywords>Statistical heuristic search technique; Computational complexity; Quotient structure model</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=119>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A type-theoretic approach for program development</paper_heading>
	<authors>Wei Li</authors>
	<abstract>A paradigm of program development using type theories is given after analyzing some typical examples. In order to carry this approach forward, a language ALT is designed, a linguistic description of a generalized higher order typed lambda calculus with Π, Σ types and Π, Σ kinds (supertype). Four examples are given to show how ALT can be used to implement many concepts of software engineering and artificial intelligence, they are intuitionistic logic, Peano arithmetic, approximate reasoning and program transformation. ALT is described formally, using a structural operational approach.</abstract>
	<keywords>ALT language; Linguistic description; Artificial intelligence</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=120>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Attribute theory in learning systems</paper_heading>
	<authors>Zhongzhi Shi, Jianchao Han</authors>
	<abstract>The ability to learn is one of the most essential characteristics of human intelligent behaviour. For many decades, learning has attracted the interest of many researchers, and therefore, the fields concerned with the developing computational theories of learning processes and building learning machines are central to the progress of artificial intelligence. In this paper, we will discuss the structure of the attribute set and present an attribute theory for machine learning. In order to explain the functions we will give an algorithm AL for learning by analogy based on attribute theory.</abstract>
	<keywords>Human intelligent behaviour; Computational theories; Artificial intelligence (AI); Machine learning (ML)</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=121>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and evaluation of a relational knowledge base prototype machine</paper_heading>
	<authors>Wang Zhiying, Ci Yungui</authors>
	<abstract>In this paper we describe a relational knowledge base prototype machine, which is based on the relational knowledge model. A unification mechanism is used as the retrieval mechanism. This machine consists mainly of a control processor (Intel 86/310), four processing units (Intel 86/12) dedicated to unification operations, a shared memory and a disk system. The performance evaluation of the system is also described. It is proven that for a given group of benchmarks the speed of the prototype is over 20 times higher than a PROLOG system running on an Intel 86/310.</abstract>
	<keywords>Relational knowledge base prototype machine;  Relational knowledge model; PROLOG system</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=122>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A C-oriented tool for building second generation expert systems</paper_heading>
	<authors>Zhaohui Wu, Tao Yang, Feng Ni, Zhijun He, Rui Zhao Yu</authors>
	<abstract>Considerable interests have been expressed in developing a tool for building Second Generation Expert Systems (ESs). The C-Oriented tool ZIP/E++ is developed to verify the authors' ideas on what characteristics such tools should have. New mechanism such as four-level inference control scheme with news posting/delivering model and built-in explanation facility are introduced; rich semantical knowledge representation and multiple inference engines are integrated; a knowledge acquisition shell ZIFS is offered to help domain experts to acquire the domain knowledge; a truth maintenance system for ESs built by ZIP/E++ and multiple knowledge bases organizing, scheduling and intercommunication are provided; a knowledge-based expert system building model CIA-model also is considered. The most important features of ZIP/E++ are: (1) It is a general tool suitable for building various domain ESs; (2) It is a flexible tool for integrating different kinds of inference engines and knowledge representations into a single system; (3) It is a tool with deep-level reasoning model for coupling heuristic reasoning and domain reasoning [6]; (4) It is a very efficient tool for problem solving; (5) It is a tool with well-developed environment to support the ESs building.</abstract>
	<keywords>Second Generation Expert Systems (ESs); C-Oriented tool; ZIP/E++; CIA-model</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=123>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An ordered model combining dataflow with control flow and its implementation</paper_heading>
	<authors>Liu Guizhong, Ci Yungui</authors>
	<abstract>Dataflow computation can exploit high parallelism, but its overhead is great. In this paper, we shall discuss why we must modify dataflow graph first. Then, a model combining dataflow with control flow is presented; third, a practical architecture SDS-1 according to DFGC is described. The SDS-1 exploits two levels of parallelism; we shall mainly discuss its parallelism at instruction level. Finally, the performance of SDS-1 is analyzed.</abstract>
	<keywords>Dataflow computation; Dataflow graph; Combining dataflow control flow; SDS-1; DFGC</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=124>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Approaches to decentralized control of job scheduling for homogeneous and heterogeneous parallel computer systems</paper_heading>
	<authors>Yueming Hu, Zhiliang Xie, Xinda Lu</authors>
	<abstract>This paper presents two methods for job scheduling in parallel computer systems, the conflict vector scheduling method and the resource vector scheduling method for heterogeneous and homogeneous systems respectively. By constructing these vectors according to system resource requirement of arriving jobs, these methods can efficiently solve the collision problem of system resource requirement. Both intertask communication and inter-task sequential dependency are considered. These two methods have the advantages of lower real time scheduling overhead, less context switching, less communication delay and providing deadlock prevention. Also presented in this paper are simulation results of the two job scheduling methods.</abstract>
	<keywords>Parallel computer systems; Resource vector scheduling method;  Job scheduling methods.</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=125>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Processor self-scheduling for parallel loops in preemptive environments</paper_heading>
	<authors>Yang Xuejun, Chen Haibo, Ci Yungui, Chen Fujie, Chen Lijie</authors>
	<abstract>Processor self-scheduling schemes for parallel loops can be used in a non-preemptive environment to reduce the scheduling overhead significantly. In a preemptive environment system like Cray X-MP/4, the schemes can cause serious problems. In this paper, we describe the problems of processor self-scheduling schemes in a preemptive environment. A solution for resolving these problems is also presented.</abstract>
	<keywords>Processor self-scheduling; Parallel loops; Non-preemptive environment; Cray X-MP/4</keywords>
	<publication_month_year>1990-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 1</volumes_issues>
</paper>
<paper no=126>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Attractor neural networks and biological reality: associative memory and learning</paper_heading>
	<authors>Daniel J Amit</authors>
	<abstract>It is argued, in the light of recent neurophysiological findings, that neural networks (cell assemblies) whose connectivity involves strong feedback and which consequently can sustain patterns of high spiking activity (reverberations), are plausible descriptions of cognitive function in certain cortical regions. Such networks can store a high number of different memories and perform associative retrieval effectively, even in the presence of high levels of noise, synaptic damage and neuronal sloppiness. Such Attractor Neural Network (ANNs) produce emergent signals which indicate the completion of a computation. Moreover, the existence of the attractors provides a candidate for memory on the shortest scale, which does not call for synaptic changes; as such the attractors are natural candidates for Hebbian learning. Recent extensive data analysis of recordings from performing monkeys puts rather severe upper bounds on the spike rates in some regions of cortex, in which enhanced rates are observed. These findings favor a class of models of ANNs which have stochastic attractors whose spike rates are tunable by the level of the overall inhibition. While these models include significant modifications of the original ANNs, they belong conceptually to the same class, in terms of computation, effectiveness and robustness. Finally, we remark on a possible reinterpretation of recent empirical biochemical findings that can provide a realistic mechanism for Hebbian learning.</abstract>
	<keywords>Neural networks; Attractor Neural Network (ANNs); Hebbian learning</keywords>
	<publication_month_year>1990-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 2</volumes_issues>
</paper>
<paper no=127>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Ising spin neural networks with spatial structure</paper_heading>
	<authors>A.C.C Coolen</authors>
	<abstract>We study Ising spin models for neural networks with spatial structure, in which patterns are stored according to Hebb's rule. From the microscopic dynamics given by the master equation we derive a partial differential equation for the position- and time-dependent correlations between the system state and the stored patterns. This equation can be used to study networks with finite range connections (not necessarily symmetric), the behaviour of domain boundaries and information transport. For systems with finite range connections we can compute the range of macroscopic order, and, as a result, the connection range that is needed in hardware realisations of attractor neural networks.</abstract>
	<keywords>Ising spin models; Neural networks; Spatial structure; Hebb's rule</keywords>
	<publication_month_year>1990-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 2</volumes_issues>
</paper>
<paper no=128>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>LAIOS: a parallel execution of PROLOG by data copies</paper_heading>
	<authors>Jean Duprat</authors>
	<abstract>LAIOS (Lattice for Artificial Intelligence Oriented System) is a project of a multiprocessor architecture oriented to artificial intelligence applications. The execution model is the AND/OR tree; the control is realized by automata and authorizes OR parallelism and AND pipelining. PROLOG programs are compiled into frame nodes and execution dynamically creates process nodes. The data flows along the branches of the execution tree. In each process node, a computation is realized by a universal operator. The data are physically copied step by step, making parallelism and nondeterministic implementations easier. An architecture is defined, supporting such a model. It uses self-defined</abstract>
	<keywords>LAIOS (Lattice for Artificial Intelligence Oriented System); AND/OR tree; PROLOG programs;  Parallel execution; KLIPS</keywords>
	<publication_month_year>1990-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 2</volumes_issues>
</paper>
<paper no=129>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Smart memory architecture and methods</paper_heading>
	<authors>Tony R Martinez</authors>
	<abstract>This paper discusses potential functionalities of smart memories. Smart memory entails the tight coupling of memory and logic. A specific architecture called the memory processor model is proposed. The model seeks to alleviate the von Neumann bottleneck, take advantage of technology trends, improve overall system speed, and add encapsulation advantages. Speed is increased through locality of processing, communication savings, higher-level functionality, and parallelism. Data objects are accessed through descriptors, which give the memory a meta-knowledge concerning the objects, allowing for nontraditional access mechanisms. Both data types and operations are programmable, and the model is streamlined for memory operations and services. Innovative processing schemes, coupled with emerging technology densities, allow for substantial fine-grain parallelism in traditional and novel memory operations. Three important paradigms introduced are descriptor processing, where operations are accomplished without access to the actions data, associative descriptor processing, supporting highly parallel access and processing, and the single-program multiple-data method, allowing parallelism by simultaneous processing of data objects distributed amongst multiple smart memories. Examples of specific operations are presented. This paper presents initial studies into the smart memory mechanism with the goal of describing its potential and stimulating further work.</abstract>
	<keywords>Smart memory; Memory processor model;  Associative descriptor processing; Single-program multiple-data</keywords>
	<publication_month_year>1990-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 2</volumes_issues>
</paper>
<paper no=130>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulation studies on the performance of an organizational model for graph reduction</paper_heading>
	<authors>K Ravikanth, P.S Sastry, Y.V Venkatesh</authors>
	<abstract>This paper deals with the performance evaluation of an organizational model for parallel graph reduction, based on the de Bruijn interconnection scheme. For this machine, two scheduling policies, one of which is a static mechanism, and the other, a dynamic one based on the concept of diffusion scheduling, are considered. It is shown through simulations that the simple static scheduling policy does reasonably well and the performance cannot be significantly improved by the dynamic policy which is more complex and incurs higher overheads.</abstract>
	<keywords>Parallel graph reduction; Organizational model; Bruijn interconnection scheme; Static mechanism</keywords>
	<publication_month_year>1990-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 2</volumes_issues>
</paper>
<paper no=131>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experiments in mimd parallelism</paper_heading>
	<authors>Anthony J.G Hey</authors>
	<abstract>The paper reviews the problems inhibiting the widespread use of parallel processing by both industry and software houses. The two key issues of portability of code and of generality of parallel architectures are discussed. An overview of useful computational models and programming paradigms for parallel machines is presented along with some detailed case studies implemented on transputer arrays. Valiant's results on optimally universal parallel machines are reviewed along with the prospects of building truly general-purpose parallel computers. Some remarks on language and software tool developments for parallel programming form the conclusion to the paper.</abstract>
	<keywords>Parallel processing; Parallel machines; Mimd parallelism </keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=132>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ASPEN: a concurrent stream processing environment</paper_heading>
	<authors>Brian K Livezey, Richard R Muntz</authors>
	<abstract>In this paper, we describe ASPEN, a concurrent stream processing environment. ASPEN is novel in that it provides a programming model in which programmers use simple annotations to exploit varying degrees and types of concurrency. The degree of concurrency to be exploited is not fixed by the program specification or by the underlying system. Increasing or decreasing the degree of concurrency to be exploited during execution does not require rewriting the entire program, but rather, simply reannotating it. Examples are given to illustrate the varying types of concurrency inherent in programs written within the stream processing paradigm. We show how programs may be annotated to exploit these varying degrees of concurrency. We briefly describe our implementation of ASPEN.</abstract>
	<keywords>ASPEN; Concurrent stream processing environment;  Stream processing paradigm </keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=133>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The LADY programming environment for distributed operating systems</paper_heading>
	<authors>Dieter Wybranietz, Peter Buhler</authors>
	<abstract>The LADY programming environment has been developed to support the design, implementation, testing, debugging and monitoring of distributed systems with special focus on operating systems. This paper describes the structuring model of the system implementation language LADY. In LADY, a distributed system is viewed as a collection of cooperating objects connected through typed interfaces. The language offers dynamic modifications of the program structure such as generation and deletion of objects and their interconnections as well as migration and checkpointing of objects. Further features include the extensibility of programs and recursive definition of objects. The structuring facilities of LADY are demonstrated by an example and experiences with the language are discussed.</abstract>
	<keywords>LADY programming environment; Distributed systems; Operating systems</keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=134>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel object-oriented descriptions of graph reduction machines</paper_heading>
	<authors>David Bolton, Chris Hankin, Paul Kelly</authors>
	<abstract>Abstract machine descriptions of parallel computer architectures must capture communications and concurrency characteristics at a high level. Current design techniques and notations are weak in this respect. We present a layered method for refinement of a requirements specification through to a detailed systems architecture design. This paper concentrates on the two highest layers, the logical model, which is a requirements statement, and the systems architecture, which specifies logical processes and explicit communications. While requirements are expressed in a language that matches the problem domain, we suggest that a parallel object-oriented notation is most appropriate for the systems architecture layer. Refinements within this layer reflect implementation details (e.g. structure sharing and distribution of work among processing elements). We introduce a parallel object-oriented notation based on rewriting concepts and use it to refine the design of a parallel graph reduction machine to execute functional programs. The Paragon notation used is a natural extension of a graph rewriting language and the work forms the basis for a structured explication of parallel graph rewriting in which all communications are made explicit.</abstract>
	<keywords>Parallel graph reduction machine; Parallel object-oriented notation; Logical model</keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=135>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The feasibility of a general-purpose parallel computer using WSI</paper_heading>
	<authors>Paul Anderson, Paul Kelly, Phil Winterbottom</authors>
	<abstract>COBWEB is a design for a wafer-scale computer intended to execute general-purpose programs written in a high-level, functional programming language. The results of a study of the feasibility of the hardware aspects of the architecture are presented. We factor the design into a communications network and a set of identical processing elements. The design, simulated performance and predicted yield of the communications network are reviewed, and this is used to constrain the processing element's characteristics by means of a simple performance model.</abstract>
	<keywords>COBWEB; General-purpose programs; Functional programming language; Communications network; WSI </keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=136>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Achieving low cost synchronization in a multiprocessor system</paper_heading>
	<authors>Rajiv Gupta, Michael Epstein</authors>
	<abstract>The barrier is a commonly used mechanism for synchronizing processors executing in parallel. Upon reaching a barrier a processor must idle until all processors reach the barrier. In this paper, the fuzzy barrier, a mechanism that reduces the idling of processors is presented. The idling is reduced by using software techniques to find useful instructions that can be executed by a processor while it awaits synchronization. The fuzzy barrier mechanism has been implemented both in hardware and software. The hardware implementation eliminates busy waiting at barriers, provides a mask that allows disjoint subsets of processors to synchronize simultaneously, and provides multiple barriers by associating a tag with a barrier. The software implementation of the fuzzy barrier provides significant reduction in the synchronization overhead over the software implementation of the fixed barrier. Compiler techniques are presented for constructing barrier regions which consist of instructions that a processor can execute while it is waiting for other processors to reach the barrier. The larger the barrier region, the more likely it is that none of the processors will have to stall. Initial observations show that barrier regions can be large and the use of program transformations can be used to increase their size.</abstract>
	<keywords>Synchronizing processors; Multiprocessor system;  Fuzzy barrier mechanism; Low cost</keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=137>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The expressive power of parallelism</paper_heading>
	<authors>Joachim Parrow</authors>
	<abstract>We explore an algebraic language for networks consisting of a fixed number of reactive units, communicating synchronously over a fixed linking structure. The language has only two operators: disjoint parallelism, where two networks are composed in parallel without any interconnections, and linking, where an interconnection is formed between two ports. The intention is that these operators correspond to the primitive steps when constructing networks, and that they therefore are conceptually simpler than the operators in existing process algebras. We investigate the expressive power of our language. The results are: (1) Definability of behaviours: with only three simple processing units, every finite-state behaviour can be constructed. (2) Definability of operators: we characterise the network operators which are definable within the language; these turn out to include most operators previously suggested for describing parallelism. Our results hold for any congruence between trace equivalence and observation equivalence.</abstract>
	<keywords>Disjoint parallelism; Algebraic language; Describing parallelism; Definability of behaviours; Definability of operators</keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=138>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Compositionality in the temporal logic of concurrent systems (extended abstract)</paper_heading>
	<authors>F.S de Boer</authors>
	<abstract>In this paper we develop a new method for describing in temporal logic in a compositional manner sequential composition, its iterated version (loops) and its interaction with (nested) parallelism. The logic we will use will be a linear time logic with as sole temporal operator the until. We will illustrate this method in the construction of a compositional proof system for a CSP-like language. We will prove this system to be sound and (relative) complete.</abstract>
	<keywords>Temporal logic; distributed; concurrent systems; verification methods</keywords>
	<publication_month_year>1990-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 3</volumes_issues>
</paper>
<paper no=139>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Heuristic search in PARLOG using replicated worker style parallelism</paper_heading>
	<authors>Henri E Bal</authors>
	<abstract>Most concurrent logic programming languages hide the distribution of processes among physical processors from the programmer. For parallel applications based on heuristic search, however, it is important for the programmer to accurately control this distribution. With such applications, an inferior distribution strategy easily leads to enormous search overheads, thus decreasing speedup on parallel hardware. To solve this problem, various language extensions for concurrent logic languages have been proposed, such as mapping notations and priorities. We present an alternative approach that does not require any new language features. Our solution is to use the replicated workers paradigm in a concurrent logic language (PARLOG). This paradigm has thus far mainly been used in parallel procedural languages, such as Linda and Orca. We show that it is just as useful for logic languages. We have implemented two parallel applications, the Traveling Salesman Problem and alpha-beta search, using this approach. Also, we have done some performance measurements of these programs on a multiprocessor. These experiments show that significant speedups can be obtained in this way.</abstract>
	<keywords>Parallel applications; Heuristic search; Parallel hardware; PARLOG</keywords>
	<publication_month_year>1991-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 4</volumes_issues>
</paper>
<paper no=140>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel logic system on a multicomputer architecture</paper_heading>
	<authors>M Cannataro, G Spezzano, D Talia</authors>
	<abstract>This paper describes the implementation of a logic programming language on a massively parallel architecture. This implementation is based on the AND/OR Process Model which allows the exploitation of both AND and OR parallelism in logic programs. A distributed memory model is used, and a decentralized control mechanism has been designed. The multicomputer, which the system has been implemented on, consists of a network of Inmos Transputers. The AND/OR processes are implemented as Occam processes mapped onto the Transputer nodes. After the presentation of the system architecture and a deep discussion of the distributed memory management, some preliminary performance results are discussed.</abstract>
	<keywords>Parallel logic system; AND/OR Process Model; Multicomputer architecture </keywords>
	<publication_month_year>1991-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 4</volumes_issues>
</paper>
<paper no=141>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>POOSS: A Parallel Object-Oriented Stable Storage</paper_heading>
	<authors>Sun Chengzheng, L.O Hertzberger, B.J.A Hulshof, Rogier Wester</authors>
	<abstract>This article describes the design and implementation of a parallel object-oriented stable storage (POOSS) system in the context of the PRISMA project. The main issues of the POOSS system include atomic access of generic objects, fault tolerance, parallelism, and the issues in designing a fast disk file system. These issues are approached by constructing the POOSS system as a hierarchical one, with each layer attacking different issues, resulting in a system which is easier to understand, implement and maintain. The multiple layers of the POOSS system are interfaced by a restricted set of UNIX compatible routines. By exposing the internal interfaces of different layers to external users, the POOSS system is able to provide the application programs with multiple services of a different nature: the normal local file service, the parallel global file service, the parallel stable file service, and the parallel object-oriented stable file service. Implemented in a 100-node parallel machine environment, the POOSS system could not only process multiple files simultaneously, but also be partitioned into multiple independent subsystems. The performance measurement of the current implementation has shown good results for the system and also identified bottlenecks, candidates for further optimizations.</abstract>
	<keywords>Parallel object-oriented stable storage (POOSS) system; PRISMA project; Fault tolerance; Parallelism; UNIX compatible routines </keywords>
	<publication_month_year>1991-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 4</volumes_issues>
</paper>
<paper no=142>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On function languages and parallel computers</paper_heading>
	<authors>Amos R Omondi</authors>
	<abstract>This paper is a discussion of functional languages and parallel computers. It is aimed at an audience that has a background in computer architecture, but not necessarily in the area of functional languages. It therefore constitutes an introductory survey of functional languages, on the one hand, and a non-introductory discussion of parallel computers, on the other. The aim is to highlight some important issues regarding the use of adequacy of these languages and also on the design of parallel computers to interpret them. The concluding thesis of put forth is twofold: one, that to widen their scope of applicability, functional languages need to include more features of nondeterminism and may need to be integrated with features from conventional languages; two, that the right sort of architectures for such extended languages may well be less-specialised ones with a von Neumann flavour.</abstract>
	<keywords>Functional languages; Parallel computers; Computer architecture</keywords>
	<publication_month_year>1991-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 4</volumes_issues>
</paper>
<paper no=143>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DIALOG — A dataflow model for parallel execution of logic programs</paper_heading>
	<authors>Kang Zhang, Ray Thomas</authors>
	<abstract>The paper presents a dataflow execution model, DIALOG, for logic programs which operates on an intermediate virtual machine. The virtual machine is granulated at clause argument level to exploit argument parallelism through unification. The model utilises a new variable binding scheme that eliminates dereference operations for accessing variables, and therefore supports OR-parallelism in the highly distributed dataflow environment. The model has been implemented in Occam. A conventional dataflow architecture in support of the model has been simulated as a testbed for the evaluation. The simulation indicates some encouraging results and suggests future improvements.</abstract>
	<keywords>DIALOG; Dataflow execution model; Logic programs; OR-parallelism; Conventional dataflow</keywords>
	<publication_month_year>1991-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 6, Issue 4</volumes_issues>
</paper>
<paper no=144>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The evolution of information technologies in the 90s and its impact on applications</paper_heading>
	<authors>Jean-Jacques Duby</authors>
	<abstract>The intent of this paper is to present a scenario for the evolution of Information Technologies in the present decade and to attempt to analyze the impact of such an evolution on the applications of Information Technologies and on their uses. It will start focusing on the ‘core’ technologies of computers, i.e. memory, logic and storage, and also computer architecture. It will then look at input/output technologies based on the advances made in the computer core technologies, with special attention to user directed input/output, namely the user terminal or workstation. While the preceding technologies are likely to follow an evolutionary trend, pretty much in line with the one that they followed in the previous decade, the following two parts of this paper are likely to introduce some kind of revolutions: first, with Communication Technologies, which will go through a discontinuity in the 1990s that will bring performance increases of several orders of magnitude; second, with what could be called the invasion of standards, which will become pervasive in all areas of Information Technologies and applications. The last domain covered will be software technologies. The conclusion will be built upon all these evolutions to attempt to construct a most likely evolution for applications.</abstract>
	<keywords>Core technology; workstation; communication technology; standards; software technology; information technology</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=145>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Software engineering</paper_heading>
	<authors>Marco Maiocchi</authors>
	<abstract>The paper discusses the role of Software Engineering disciplines in software production. Despite the strong relevance generally and usually given to technological aspects and to methods, the paper presents the process of softwre development as an industrial process to be controlled: from this point of view proper organization is the key point for success, and methods must follow; technology is the last marginal aspect. Organization, methods and techniques are located and discussed, and trends towards Total Quality and new development environment are presented.</abstract>
	<keywords>Software quality assurace; software engineering; software quality; software total quality</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=146>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Trends in parallel and distributed computing</paper_heading>
	<authors>L.O Hertzberger</authors>
	<abstract>In this article an overview is given of the trends in parallel and distributed computing research as well as in the usage of these techniques for applications in science and engineering. An outline is given of the major developments in application modeling, and research in languages and operating systems for distributed and parallel computing. It is illustrated that the migration of existing software towards parallel platforms is a major problem for which some experimental solutions are under development now. The importance of a further integration of distributed and parallel computing is argued.</abstract>
	<keywords>Parallel computing; distributed computing; parallel operating systems; parallel languages; performance modeling; MIMD; shared and distributed memory machines; workstations; abstract machine models</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=147>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Future database technology: driving forces and directions</paper_heading>
	<authors>Peter C Lockemann, Alfons Kemper, Guido Moerkotte</authors>
	<abstract>Database systems have increasingly become an integral part of modern business. Owing to this success they seem to invite an ever growing number of application areas to utilize their potential. Not all these attempts are an unqualified success: Indeed, today's mature database techniques seem tailored to a comparatively narrow application segment, and new techniques are needed to meet the demands of the non-traditional applications. In turn, these techniques often have to await progress in other technologies. To plan for the future, both providers and users of database systems will have to judge what directions database technology will take in the future. The paper attempts to predict some of these directions. In order to avoid purely subjective speculations, the paper introduces a systematic basis by hypothesizing that developments have been driven in the past, and will so in the future, by three forces: new applications, new technologies, and new and evolving standards. The hypothesis is first tested by retracing the more recent and the current developments, at the same time providing the reader with a brief survey of the present state of the art. The hypothesis is then applied to the expected trends in these forces, leading to a number of propositions on where database technology will move in the near and not so near future. The more ambitious projections will finally be tempered by pointing out some of the retarding moments.</abstract>
	<keywords>Database systems; database technology; database applications; standardization</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=148>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge-based systems and artificial intelligence: emerging technology</paper_heading>
	<authors>Jim Howe</authors>
	<abstract>Besides adding value to existing products, the technological spin off from Artificial Intelligence is making it easier for people to get access to, and take advantage of, different kinds of knowledge about everyday tasks. In the context of descriptions of recent successful practical applications, this paper offers a brief review of the state-of-progress in a number of distinct areas, natural language processing, including speech processing, expert systems and computer vision, leading to the conclusion that knowledge-based systems technology shows great promise for the future.</abstract>
	<keywords>Knowledge-based systems; Artificial Intelligen; ceexpert systems; natural language processing; machine translation; speech recognition; computer vision</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=149>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Neural networks and computing</paper_heading>
	<authors>Françoise Fogelman Soulié</authors>
	<abstract>In this paper, we give a general presentation of neural networks, showing their links and differences with Artificial Intelligence and neurosciences. We provide the general formalism of neural networks and describe two neural networks learning algorithms: gradient backpropagation and learning vector quantization. We then illustrate the behavior of these algorithms on a particular application in speech processing. We then discuss what a neural networks programming environment should provide and present some of the existing commercial products. We end the paper by the presentation of MimeNice, which is an example of such an environment developed within the Esprit Pygmalion-Galatea projects.</abstract>
	<keywords>Neural networks; learning algorithm; neurocomputing; neurocomputer; neural networks programming environment</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=150>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Network architectures</paper_heading>
	<authors>Neil F Dinn</authors>
	<abstract>This paper provides a broad overview of issues associated with global telecommunications network architecture. It covers interworking requirements needed to accomodate network and service differences associated with the telecommunications facilities in different countries. It also addresses the needs, and problems, of multinational telecommunications customers with respect to planning and implementing their global networking. These problems are, of course, complicated by the proliferation of service and technology options, along with the need to coordinate and interwork among all the service and product vendors. Some of the potential evolutionary benefits of ISDN are identified, along with a terse overview of PCN.</abstract>
	<keywords>Architecture; network; network management; customer networks; global networks; interworking; digital hierarchies; broadband ISDN</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=151>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Trends in computer-integrated manufacturing</paper_heading>
	<authors>H.-P Wiendahl, R Garlichs</authors>
	<abstract>The development of CIM systems has to be seen as an evolutionary process that builds on separate subsystems and proceeds stepwise to higher levels of integration of technical information flow. Nowadays CIM components and structures are often already implemented in large scale companies with mass or serial production whereas in small and medium size enterprises with customer or small series production the integration is not very proceeded. Due to the lack of know-how within these companies external organizations like CIM-centers should support the introduction of new solutions for the realization of CIM and carry out the necessary transfer of knowledge. Another aspect for the realization of CIM is the standardization of interfaces. The concept of technical interfaces is characterized by the MAP and TOP efforts which have already lead to practical results. In the future the importance of the standardization of ‘organizational interfaces’, which can be characterized by decision integration, will increase. The solution of this problem could be the building of general CIM-models like CIM-OSA or the GRAI-Grid which cover the organizational dimension of CIM and which can be used for many companies. The next step in the complete integration of companies seems to be the consideration and integration of administrative areas characterized by the term ‘Computer-Integrated Office (CIO' and moreover the consideration of the logistical aspects. The combination of CIM and CIO and the realization of the complete logistical chain will be the challenge of the future.</abstract>
	<keywords>CIM; CIB; system architectures; production modeling; enterprise modeling; structured introduction of CIM</keywords>
	<publication_month_year>1991-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 1</volumes_issues>
</paper>
<paper no=152>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Formal, model-oriented software development methods: From VDM to ProCoS & from RAISE to LaCoS</paper_heading>
	<authors>Dines Bjørner, Anne Elisabeth Haxthausen, Klaus Havelund</authors>
	<abstract>The first author has been invited to reminisce over almost 20 years of his research into and application of formal methods in software development. The paper deals with model-oriented methods. The title of this paper mentions two methods: VDM (Vienna Development Method) and RAISE (Rigorous Approach to Industrial Software Engineering), the latter derived from the former, and two projects: ProCoS (Provably Correct Systems) and LaCoS (Large-scale Correct Systems). LaCoS is a follow-up on RAISE. ProCoS adheres to the same principles as VDM as VDM and RAISE, but with a twist! The paper will outline the technical/scientific essence of the VDM and RAISE methods, and explain the ProCos and LaCoS project structures. The paper will first outline five central rôles of software developers: problem domain specialists, programmers, software engineers, resident computation scientists and managers. We will briefly illustrate examples of specifications in VDM and RAISE. The aim of the paper is to record that formal methods in software development are now well-developed and broadly accepted in Europe.</abstract>
	<keywords>Programming methodology; formal method; VDM; RAISE; model-oriented semantics; non-determinism; concurrency; parallellism; abstract data types; algebraic semanticsmodules</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=153>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Insights from empirical studies of the software design process</paper_heading>
	<authors>Bill Curtis</authors>
	<abstract>Empirical research on software development has shown that the design phase exercises extreme leverage over project outcomes. This paper reviews research performed on the design process and proposes several research questions whose answers are crucial to improving productivity and quality. One implication of these results is that project outcomes are largely determined before a project begins.</abstract>
	<keywords>Software design; design process; software process; programming teams; software psychology; distributed cognitionsoftware organization; sempirical studies of programmers</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=154>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Cubemat: A parallel interconnection model consisting of hypercube and global bus connections</paper_heading>
	<authors>Susumu Shibusawa</authors>
	<abstract>In this paper, an interconnection model for parallel computation, the Cubemat, is proposed, and an efficient layout and data transmission operations on it are presented. The node connections of the Cubemat consist of binary cube and global bus connections. The binary-reflected Cubemat, in which nodes are placed in the binary-reflected order in two directions on a plane, contains explicit mesh connections or torus in the order of node positions. When the number of nodes is large, the layout of the binary-reflected Cubemat fills only 4/9ths of the space of the simple Cubemat where the nodes are embedded in two directions in the ascending order. Even though each hypercube terminal of nodes in the Cubemat can send and receive data in the same time, data transmission between nodes through global connections is more efficient than the hypercube connection for small quantities of data. This paper also presents a method of data transformation between the ascending order and the binary-reflected order which is performed in O(log n) time on the n-node Cubemat using hypercube connection.</abstract>
	<keywords>Parallel processing; hypercube; global bus connections; Cubematbinary-reflected codedata transmission</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=155>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the power of global-bus in mesh-connected architectures</paper_heading>
	<authors>Hiroshi Umeo, Thomas Worsch, Roland Vollmar</authors>
	<abstract>We study the computational power of global bus systems (GB, for short) augmented with a mesh-connected computer (MCC, for short). First we show that the GB is a useful tool for designing optimum-time parallel algorithms for MCCs and for showing correctness of those algorithms once designed. We do this by giving some design examples which utilize the GB very efficiently. Secondly we give a fundamental technique for the elimination of GBs. As an application of the technique, we will show that a rich variety of GBs on one- and two-dimensional MCCs can be eliminated without any loss of time efficiency.</abstract>
	<keywords>Systolic array; fine-grain mesh-connected computer; glôbal bus controllocal bus; systolic algorithm</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=156>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The GF11 parallel computer: Programming and performance</paper_heading>
	<authors>Manoj Kumar, Yurij Baransky</authors>
	<abstract>GF11 is a parallel computer operational at IBM's T.J. Watson Research Center. It is based on the SIMD (Single Instruction Multiple Data) model of parallel computing. GF11 attains its peak execution rate of 11.3 Gigaflops by using 566 identical processing elements, each capable of delivering 20 Megaflops. The processors communicate through a 576× 576 Benes network. The network provides 11.3 Gigabytes/sec of communication bandwidth to the processors and allows the processors to dynamically reconfigure themselves into arrays of various dimensions and sizes or other patterns such as a tree, hypercube, etc. GF11 has several architectural enhancements to circumvent the limitations of the standard SIMD model. Preliminary benchmarking efforts on some applications indicate that near peak performance can be sustained on most applications. The architecture of GF11 is summarized in this paper and implementation of matrix multiplication, shallow water equations, and galaxy formation codes are discussed.</abstract>
	<keywords>SIMD (Single Instruction Multiple Data) computers; massively parallel computing; parallel processing; Benes network</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=157>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A novel paradigm of parallel computation and its use to implement simple high-performance hardware</paper_heading>
	<authors>R.W Hartenstein, A.G Hirschbiel, K Schmidt, M Weber</authors>
	<abstract>This paper introduces a novel (non-von Neumann) paradigm of parallel computation supporting a much more efficient implementation of parallel algorithms. Acceleration factors of up to more than 2000 have been obtained experimentally on the MoM architecture for a number of important applications - although using a hardware being more simple than that of a single RISC microprocessor. The machine organization and the most important hardware features of xputers are briefly introduced. The programming paradigm and its flexibility is illustrated by simple DSP and image processing examples.</abstract>
	<keywords>Data-drivenultra micro parallelism; sparse controlfine granularity scheduling; control overhead; high level synthesis</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=158>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A prototype of a highly parallel dataflow machine EM-4 and its preliminary evaluation</paper_heading>
	<authors>Yuetsu Kodama, Shuichi Sakai, Yoshinori Yamaguchi</authors>
	<abstract>This paper discusses the design and implementation of a prototype of a highly parallel dataflow machine, the EM-4, mainly from the viewpoint of the system architecture and maintenance. The EM-4 is a next generation dataflow computer whose target structure has more than a thousand PEs. A prototype with 80 PEs was completed in April 1990. Distinctive features of the EM-4 system architecture are: (1) a strongly connected arc model, (2) two simple and fast synchronization mechanisms, (3) versatile pipeline design, (4) a RISC based parallel architecture and (5) an interconnection network with extra facilities. Distinctive features of the maintenance architecture are: (1) tree structured parallel maintenance mechanisms dedicated to testing, initializing, debugging and monitoring the system, (2) maintenance operations executed independently of the system operations and (3) simplified and effective implementation of maintenance functions. After examining these features, the implementation and preliminary evaluation of the EM-4 prototype are described. The design features and the evaluation results show that an efficient and highly testable parallel computer is realized, based on the modified dataflow schemes and the versatile maintenance schemes.</abstract>
	<keywords>Highly parallel computer; dataflow machine; performance evaluation; maintenance architecture; strongly connected arc modelcircular omega network; direct matching scheme</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=159>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experience of parallel AI programming with parallel Lisp</paper_heading>
	<authors>Hiroshi G Okuno</authors>
	<abstract>Parallelism or parallel execution is expected to improve the performance of Artificial Intelligence (AI) systems so that they can be applied to much wider areas. One of the major problems with parallelizing AI systems is the lack of methodology for parallel AI programming. This paper discusses key issues in parallel AI programming with parallel Lisp. By implementing two AI systems, OPS5 (a rule-based system) and ATMS (an intelligent database system), three main problems are observed: difficulty in identifying the most time-consuming small tasks, frequent access to global data, and over-sequential execution. Solutions to these problems are presented, including hierarchical decomposition of tasks, runtime control of multiprocessing, reader-writer locks and lazy and speculative computations.</abstract>
	<keywords>Parallel Lisp; OPS5; ATMS; run-time control; dynamic spawning; parallel AI; reader-writer lockhierarchical decomposition</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=160>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Problem-solving environments for parallel computers</paper_heading>
	<authors>David A Padua</authors>
	<abstract>Man-machine interaction can take place at different levels of abstraction ranging from the machine-instruction level to the problem-specification level. A problem-solving environment should provide restructuring and debugging tools to make the interaction at these different levels possible and to allow the efficient use of the target machine. Restructurers translate from specifications to programs or from programs to more efficient versions. When the target machine is parallel, the restructurers should include techniques for the automatic exploitation of parallelism. Debuggers are necessary to test for correctness and to evaluate performance at the different levels. Debuggers for parallel programs have to deal with the possibility of nondeterminacy.</abstract>
	<keywords>Parallel computing; compilers; problem-solving environments; programming environments</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=161>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The connection machine opportunity for the implementation of a concurrent functional language</paper_heading>
	<authors>Claudio Bettini, Luca Spampinato</authors>
	<abstract>A concurrent functional language is presented, Parlam, which includes primitives for explicit control of parallel reductions. An abstract machine for Parlam (ParSEC) is derived from Landin's SECD machine by extending it for dealing with parallel computation. Several issues and problems about its implementation on a massively parallel computer are discussed and a specific implementative model for ParSEC under the Connection Machine is described.</abstract>
	<keywords>Concurrent languages; functional languages implementation; lazy evaluation; massively parallel architectures</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=162>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance effects of program structures on a snoop-cached multiprocessor system</paper_heading>
	<authors>Nagatsugu Yamanouchi</authors>
	<abstract>This paper discusses performance characteristics of a snoop cache system implemented on an experimental multioprocessor TOP-1. The cache state transitions and possible performance degradation by the snooping overhead are analyzed for three sample programs, single processor compilation, a simple reader-writer problem and repeated lock under the two snoop protocols, broadcast and invalidation. Their performance degradation on the real hardware is measured together with the cache access statistics data. The result provides a number of hardware and software design suggestions: the protocol setting configuration of KPU to the invalidation and the UPUs to the broadcast protocol performs better, typical parallel programs with shared variables perform better in the broadcast protocol, straightforward coding of tight snoop cache usage, such as the reader-writer example, does not saturate the TOP-1 but in the broadcast protocol, and a careful coding of the busy-wait lock loop effectively reduces bus traffic.</abstract>
	<keywords>Multiprocessor; snoop cache; cache performance</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=163>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experience porting Mach to the RP3 large-scale shared-memory multiprocessor</paper_heading>
	<authors>Henry H.Y Chang, Bryan Rosenburg</authors>
	<abstract>The Research Parallel Processing Prototype (RP3) is a research vehicle developed at the IBM T.J. Watson Research Center to explore the hardware and software aspects of highly parallel computation. The RP3 is a shared-memory machine designed to be scalable to 512 processors; a 64 processor machine has been in operation since October 1988. A parallel programming environment based on Mach has been developed, and a variety of programming models have been tested on the machine. The Mach kernel has been extended to support a rich set of software-controllable architectural features such as non-coherent caches, local and interleaved storage, and performance monitors. This paper describes the experience of porting Mach to the RP3, focusing both on the performance tuning process and on exploiting the RP3 architecture. Performance was significantly improved by concentrating on kernel activities, such as spin locking and busy-wait synchronization, that have global performance impact. However, identifying the real sources of a congestion was often more difficult than providing solutions.</abstract>
	<keywords>MACH; multiprocessor operating systems; pin-lock synchronization; UNIX</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=164>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Current research status and future direction of the fifth generation computer project</paper_heading>
	<authors>Koichi Furukawa</authors>
	<abstract>The fifth generation computer project was launched in 1982, aiming at developing parallel computers dedicated for knowledge information processing. It was commonly believed that knowledge processing based on symbolic computation is very difficult to parallelize. We made a conjecture that logic programming technology will solve the difficulty. During the project, we succeeded in inventing a parallel logic programming language, GHC, and its realistic extention, KL1. We devoted our effort in developing parallel hardware called PIM, dedicated for KL1. We are now developing software technology on top of KL1 and PIM. We succeeded in developing PIM's operating system totally in KL1, which runs on Multi-PSI, a current experimental parallel hardware for developing KL1 programming. We are now concentrating our effort on developing mapping techniques, such as load balancing and priority control, for deriving real spead up in parallel execution. We are also trying to develop more realistic and complicated application programs such as VLSI-CAD systems, case based reasoning systems and parallel theorem provers which, we hope, will run efficiently on PIM.</abstract>
	<keywords>Fifth generation computer; logic programming; constraint logic programming; concurrent logic programming; knowledge information processing</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=165>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The status quo and prospect of Josephson junction device technology</paper_heading>
	<authors>Ushio Kawabe</authors>
	<abstract>As part of the Japanese national supercomputer project, research and development have been conducted on high-speed logic and memory LSIs employing Josephson junction (JJ) devices which have superior performance to silicon devices. This project has made rapid progress in the LSI technology of JJ devices since niobium was employed as junction material, with which four typical systems of a small scale were tested to realize high-speed performance. The recent advances of the LSI tested, and the prospect of Josephson junction device technology are discussed.</abstract>
	<keywords>Niobium; aluminum oxide barrier; Josephson junction device; logicmemory; LSI; RAM; processor; cryogenics</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=166>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Present status of HEMT LSI technology</paper_heading>
	<authors>Masayuki Abe</authors>
	<abstract>The HEMT is a very promising device for ultra-high-speed LSI/VLSI applications because of the high-mobility GaAs/AlGaAs heterojunction structure. The present status of LSI circuit implementations for high-speed scientific computing system are reviewed, focusing on both HEMT and inverted-type HEMT structures in the submicrometer-dimensional range. A 1.1-Kgate HEMT bus driver logic LSI has been developed to demonstrate high-speed data transfer in a highly parallel processing system at room temperature. A cryogenic 3.3-Kgate HEMT random number generator logic LSI with maximum clock frequency of 1.6 GHz has also been developed to demonstrate high-clock-rate system operations at liquid nitrogen temperature. An inverted-HEMT 8-bit digital-to-analog converter with maximum clock frequency of 1.2 GHz has been developed to demonstrate high-speed high-resolution graphics operations at room temperature.</abstract>
	<keywords>HEMT; GaAs/AlGaAs; heterojunction FET; memory; logic; LSI/VLSI; cryogenichigh-speed parallel processorrandom number generator; video signal generator</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=167>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Sigma system: Concepts and structure</paper_heading>
	<authors>Noboru Akima, Shuji Nakata, Akiyoshi Nishijima, Tetsuya Kubo, Yasuo Konishi</authors>
	<abstract>The Sigma (Software Industrialized Generator and Maintenance Aids) project is a collaborative effort by industry, with support from the Japanese government, to deal with the software crisis. It aimed at realizing a common infrastructure from which to produce quality software at high productivity, and made efforts for industrializing the software production process by providing a computerized software development environment and a nationwide communications network. This paper describes an outline of the Sigma project, the Sigma system concepts and its macroscopic structure.</abstract>
	<keywords>SIGMA (Software Industrialized Generator and Maintenance Aids); software crisis; CASE (Computer-Aided Software Engineering); software platform; workstationopen architecture; software tool</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=168>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SPARC: A scalable processor architecture</paper_heading>
	<authors>Anant Agrawal, Robert B Garner</authors>
	<abstract>SPARC defines a general purpose 32-bit scalable processor architecture. The simple yet efficient nature of the architecture allows cost effective and high-performance implementations across a range of technologies. Since its first implementation done in Fujitsu's C20K gate array, a number of implementations have been announced in various technologies including bipolar ECL. All these designs implement the same instruction set. Thus an application program behaves identically and produces the same results on all SPARC platforms executing the operating systems that support the architecture.</abstract>
	<keywords>RISC, SPARC; processor; architecture; ISA; instruction set; register windows; workstation; performance; NC Berkeley; Sun Microsystems; Sun4</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=169>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A perspective of OSI standardization: Object-oriented architecture for distributed processing</paper_heading>
	<authors>Yoshikazu Kobayashi</authors>
	<abstract>After more than ten years of efforts, OSI has become real. Many service and protocol standards had been developed based on a single “communication” architecture, known as OSI Basic Reference Model. Now, the focus in ISO/IEC JTC 1/SC 21 is the development of a “distributed processing” architecture, a long-term project known as Reference Model of Open Distributed Processing. This paper presents a perspective of the OSI standardization based on an object-oriented approach for distributed application processing.</abstract>
	<keywords>OSI; object oriented; distributed processing; object management system; external object; conceptual object; internal object</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=170>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Object-oriented databases for new applications</paper_heading>
	<authors>Won Kim, Mark Scheevel, Chris Tomlinson</authors>
	<abstract>As applications have grown more sophisticated, they have placed greater demands on underlying database technology. In this paper we examine some of the shortcomings of conventional database technologies in the face of these demands, and then discuss how emerging object-oriented database technology addresses these shortcomings in several specific application areas: user interface applications, multimedia databases, CAD databases, statistical and scientific databases, and management of heterogeneous database systems. We conclude with a list of remaining challenges and some suggestions for further research.</abstract>
	<keywords>Object-oriented; object-oriented databases; statistical databases; inheritance; computer-aided engineering and design</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=171>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Transaction models for federative distributed database systems</paper_heading>
	<authors>Wolfgang Johannsen</authors>
	<abstract>Federative Distributed Database System (FDDBS) are composed out of independent nodes in a computer network. Each server node comprises a Database Management System (DBMS) offering services of different quality to the outside world, i.e. remote users at client systems. In addition local services are offered to local users. Server nodes are independent from other notes. This reflects the characteristic of an dynamically growing and shrinking federation of databases in open systems. Typically user applications are distributed and should be supported by management functions that guarantees certain service quality. The most important are atomicity and isolation. Units of works that provides such characteristics are distributed transactions. In this paper we discuss the advantages and disadvantages of two transaction models with respect to FDDBSs. Beside traditional distributed (global) transactions we consider a model based on the Saga approach. We show how both can be integrated in one environment. The approach taken increases node autonomy and allows for a higher degree of parallelism without loss of the advantages of traditional distributed transactions.</abstract>
	<keywords>Distributed transactions; node autonomy; federative systems; serializability; sagas</keywords>
	<publication_month_year>1992-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issues 2-3</volumes_issues>
</paper>
<paper no=172>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An alternative to expert systems for electrical diagnosis</paper_heading>
	<authors>Philippe Deves, Cécile Fischer, Patrick Taillibert</authors>
	<abstract>Although the use of an expert system for the detection and location of faults in the power supply subsystem of the TDFI satellite1 gives good practical results, there are also a certain number of drawbacks: • - the rules are application-dependent and therefore useless for the diagnosis of other satellites, • - the amount of knowledge required is quite large and contains a lot of redundant information, • - testing and updating of the system are tricky, • - it is far from sure that all possible faults can be taken into account. The diagnosis theory based on models of correct behavior of the components introduced more recently by de Kleer and Williams seems more suited to this type of problem. Using our program CATS/DIANA, we have been able to validate this approach for analog electronic circuits. We have also tested the method on the diagnosis of satellite power supply subsystems; the results were similar to those obtained using an expert system, but without the drawbacks. This paper describes the two methods and compares the results.</abstract>
	<keywords>Expert system; model-based diagnosis; electrical system</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=173>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An AI approach to automate the preliminary design phase of electronic equipment for satellites</paper_heading>
	<authors>Jean Patrick Tsang, Gilles Cardonne</authors>
	<abstract>This communication reports progress in the automation of the mental process of designing artifacts as a means to shorten design time and improve the quality of artifacts. This investigation is currently being conducted in the SMACK project, a common enterprise between Alcatel Alsthom Recherche and Alcatel Espace to develop a smart preliminary design system applied to the electronic equipment for satellites case-study. We present principles of the SMACK project and those of our running prototype SMACK. Innovative AI features of SMACK are discussed. They include reasoning paradigms we devised to automate the generative and patching activities of design (respectively functional and delta reasonings) and also a complexity-buster constraint-propagation technique which draws upon phenomena exhibited in design by assembly environments.</abstract>
	<keywords>Automated design; functional reasoning; delta reasoning; constraint propagation; boomerang effect</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=174>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ARIANEXPERT: A knowledge based system to analyze ARIANE's mission data</paper_heading>
	<authors>Jean-Marc Brenot, Yann Parrod, Didier Aubin, Christian Parquet</authors>
	<abstract>The ARIANE launcher post mission analysis is done at ARIANESPACE. This activity is called the ‘level 0 post flight analysis’ (PFA) and is carried out after each launch by about 60 engineers who are working together under the leadership of ARIANESPACE. The PFA is one of the most critical of ARIANE operations, for several reasons: • - The launch rate (8 a year for ARIANE 4) leaves a very short time to carry out all the verification work. Moreover, the PFA is a mandatory step before authorizing the next launch. • - The complexity of the ARIANE launcher results in a very high demand on the PFA engineers. Moreover, there are problems of availability of people with relevant expert knowledge (characterized by a substantial staff turn-over during the 10 year life duration of ARIANE 4) which could potentially result in errors or omissions. It is very important to be able to take into account the experience of the preceding flights and to record the results and the knowledge accumulated for each launch. • - The quality and the reliability of the PFA mainly depends on the accessibility of data and on the used methodology. Because the PFA is still largely done manually, and does not benefit from improved methodologies and advanced technologies providing computerized support for data processing and diagnosis, ARIANESPACE has sponsored MATRA ESPACE for the development of a knowledge based system, called ARIANEXPERT, for supporting the PFA activity. This system combines AI techniques and numerical analysis techniques, together with advanced graphical capabilities. A prototype has been delivered in April 1990 and has been used since 6 months by ARIANESPACE during real PFAs. Several lessons have been drawn from this operational experience and are described in this paper. They concern: • - The utility and justification of the use of AI techniques mostly coming from the explanation capabilities and the stress put on capturing the expert knowledge. • - The difficulties associated with the integration of such systems in the exploitation of ARIANE due to the introduction of very new tasks. • - The user point of view which evolved from reluctant to convinced.</abstract>
	<keywords>Procedural expert system; operator assistant; space operations; diagnosis; data analysis; fault detection</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=175>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge-based information retrieval</paper_heading>
	<authors>Ingeborg Sølvberg, Inge Nordbø, Agnar Aamodt</authors>
	<abstract>A knowledge-based system is used as a front-end to a very large database to increase the relevance of the information being retrieved. The subject domain of the data base is modelled in a semantic network and the queries to the database are expanded according to the semantic model. An experiment has been performed on a bibliographic database, by developing the prototype KNOWIT, a knowledge-based front-end to the information retrieval system ESA-QUEST1. An experimental evaluation shows that the number of relevant bibliographic references retrieved with the knowledge-based front-end is significantly improved, without compromising the precision of the retrieval.</abstract>
	<keywords>Knowledge-based system; information retrieval; semantic model; semantic quest; ESA-QUEST</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=176>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>NEPTUNE: A new expert planning tool for users in a network environment</paper_heading>
	<authors>G Ohlendorf, N Schielow</authors>
	<abstract>MARS (mission activities and resources scheduler) is a planning and scheduling tool for both automatic and interactive generation of spacecraft timelines of realistic complexity, which has been extensively tested, e.g. for HERMES, EURECA, or Spacelab mission planning. In its new version it has been extended to include full distributed mission planning capability (as required for the COLUMBUS mission planning scenario) under the ESA contract NEPTUNE (new expert planning tool for users in a network environment). This paper describes the main features of the planning tool MARS in its new NEPTUNE version and sketches its applicability to the distributed mission planning scenario and for other (non-space) domains.</abstract>
	<keywords>Planning and scheduling; distributed planning; time constraint propagation mechanism; resource constraint windows; backtracking methods; replanning; rule based scheduling</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=177>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>OPTIMUM-AIV: A planning and scheduling system for spacecraft AIV</paper_heading>
	<authors>M.M Arentoft, J.J Fuchs, Y Parrod, A Gasquet, ... H Vadon</authors>
	<abstract>This paper presents a project undertaken for the European Space Agency (ESA). The project is developing a knowledge based system for planning and scheduling of activities for spacecraft assembly, integration and verification (AIV). The system extends to the monitoring of plan execution and the plan repair phases. The objectives of the contract are to develop an operational kernel of a planning, scheduling and plan repair tool, called OPTIMUM-AIV, and to provide facilities which will allow individual projects to customize the kernel to suit its specific needs. The kernel shall consist of a set of software functionalities for assistance in the initial specification of the AIV plan, in the verification and generation of valid plans and schedules for the AIV activities, and in interactive monitoring and execution problem recovery for the detailed AIV plans. Embedded in OPTIMUM-AIV are external interfaces which allow integration with alternative scheduling systems and project databases. The current status of the OPTIMUM-AIV project, as of May 1991, is that the architectural design of the system has been agreed on by ESTEC/ESA and detailed design and implementation is now underway, expecting a final delivery in October of 1991.
</abstract>
	<keywords>Spacecraft AIV; knowledge based system; kernel</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=178>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A review of Multi-Agent Systems techniques, with application to Columbus User Support Organisation</paper_heading>
	<authors>T.J Grant</authors>
	<abstract>This paper reviews an advanced Knowledge-Based Systems architecture based on multiple agents. An agent is a software entity with autonomous processing capabilities and a private database, which acts on its environment on the basis of information it receives from its environment, perceives, processes, retains and recalls. Multi-Agent Systems (MAS) are systems of agents which coordinate their knowledge, goals, skills and plans jointly to take action or to solve problems, including the problem of inter-agent coordination itself. This paper shows how such an architecture is being applied by a Dutch consortium to a part of the Columbus User Support Organisation (USO). In the first part of the paper, Multi-Agent Systems techniques are placed into the context of Distributed Artificial Intelligence (DAI). Technical issues at the system architectural and agent structural levels are highlighted. The needs for intentionality (i.e. the ability of one agent to model another), acting, planning (both reactive and generative), and learning are identified. The second part of the paper shows how MAS techniques may be applied to the In Orbit Infrastructure Ground Segment (IOI-GS) and, in particular, to the Columbus USO. The relevant features of the IOI-GS and the USO, including mission and experiment control hierarchies, are outlined, identifying potential applications of MAS techniques. The functions of a national User Support Operations Centre (USOC) are listed, and two functions are described in more detail, as they are expected to be implemented in The Netherlands' USOC (named the ‘Dutch Utilisation Centre’). A uniform MAS-based architecture is presented that is designed to support both functions. Prototype implementations of key parts of this architecture are described. Conclusions are drawn from the work performed so far.</abstract>
	<keywords>Multi-Agent Systems; reactive systems; plan generation; machine learning; planning operator induction; object-oriented design; European Space Agency; Columbus space station; Dutch Utilisation Centre</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=179>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ATOS — An AI-based space mission operations system</paper_heading>
	<authors>Herwig A Laue</authors>
	<abstract>Artificial Intelligence is generally recognised as one of the key technologies for future spaceflight, and a number of ambitious applications for on-board use have been proposed already. Such applications still require a good deal of basic research and development, but on-ground applications could make an impact already in the medium term, and will for some time represent the major part of AI use for space missions. ESA has started the development of a future integrated and mission-independent spacecraft control data processing system called the Advanced Technology Operations System (ATOS) at the European Space Operations Centre, which will employ artificial intelligence techniques in supporting the operations staff during all mission preparation and implementation phases, in order to cope reliably with complex mission operations and to achieve optimal efficiency in the use of human resources. ATOS will consist of a number of knowledge based software modules, such as • Automated mission planning • Automated operations preparation • Computer assisted operations • Advanced operator training, centred around a Mission Information Base configured for the particular satellite mission, the common data repository for all information required to conduct the mission and operate the spacecraft. The Mission Information Base will, in addition to numerical data presently found in conventional spacecraft control systems, contain a large amount of ‘knowledge’ about the spacecraft and its mission, which is currently available only in paper documents or embedded in software. It will be implemented as a physically and logically distributed set of databases each representing a particular field of mission information, such that the knowledge can be dynamically shared between different intelligent spacecraft control applications.</abstract>
	<keywords>Space mission; satellite operations; Artificial Intelligence; expert system; knowledge base</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=180>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SATEXPERT: A knowledge-based system for spacecraft control</paper_heading>
	<authors>Jean-Pierre Courtin, Pierre-Yves Schmerber, Jean Christophe Chautard, Sylvain Martin, ... Nathalie Porté</authors>
	<abstract>The primary target of the SATEXPERT project is the development and the marketing of a family of products which is intended to extend the classical functionalities of a satellite control center (SCC). These products are built upon a source description of all satellite related knowledge in order to assure the consistency of information that SATEXPERT has to exploit in the operating phase. Therefore, a modeling language was defined allowing the experts to express satellite knowledge in a simple and suitable manner. The main products concern the satellite survey and control including diagnosis and manoeuvre performance. In particular, SATEXPERT will be able to detect and diagnose any space segment anomaly and subsequently perform the adequate reconfiguration operation. SATEXPERT will offer other capabilities during the satellite design, manufacturing and operational phases such as: • satellite design cross-check and validation, interfaces control (electric, TM/TC lists, …), • full satellite dynamic simulation, • generation and validation of the satellite operational documentation, • deep analysis of the on-board anomalies.</abstract>
	<keywords>Knowledge-based system; spacecraft control</keywords>
	<publication_month_year>1992-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 7, Issue 4</volumes_issues>
</paper>
<paper no=181>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel processing: the power and the portability. Experiments with ‘reusable toolkits’</paper_heading>
	<authors>Neil Carmichael, Michael Norman</authors>
	<abstract>Serial architectures and conventional languages (especially Fortran) have been the basis for almost all current industrial applications of high performance computing. Whilst the future of computing is certain to be ‘increasingly parallel’, and some successful applications of today's parallel processing technology have already been demonstrated, adoption by the main industrial users of conventional supercomputing has been limited. One reason for this is the perceived risk of investing in a rapidly developing (or, alternatively, immature) technology where specific systems (hardware or software) may become obsolete in a short period of time. It is thus reasonable to develop programming techniques, or software, which exploit available parallel power, but limit dependence on the details of specific parallel architectures. Such methods should also provide a measure of portability between machines from different manufacturers. This article discusses two examples (cellular automata and imaging) and endeavours to illustrate the ‘state of the art’ by giving an account of work done at, or in collaboration with, the Parallel Computing Centre at the University of Edinburgh.</abstract>
	<keywords>Parallel processing; portability; libraries; graphics; cellular automata; transputers; MIMD</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=182>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel distributed seismic migration</paper_heading>
	<authors>G.S Almasi, T McLuckie, J Bell, A Gordon, D Hale</authors>
	<abstract>We report significant speedup for seismic migration running in parallel on network-connected IBM RISC/6000 workstations. A sustained performance of 15 Mflops is obtained on a single entry-level model 320, and speedups as high as 5 are obtained for six workstations connected by ethernet or token ring. Our parallel software uses remote procedure calls provided by NCS (Network Computing System). We have run over a dozen workstations in parallel, but speedups become limited by network data rate. Fiber optic communication should allow much greater speedups, and we describe our preliminary results with the fiber optic serial link adapter of the RISC/6000. We also present a simple theoretical model that agrees well with our measurements and allows speedup to be predicted from a knowledge of the ratio of computation to communication, which can be determined empirically before the program is parallelized. We conclude with a brief discussion of alternative software approaches and programming models for network-connected parallel systems.</abstract>
	<keywords>Parallel computing; workstation clusters; seismic migration; parallel programming</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=183>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Loosely coupled workstations in a radiological image information system</paper_heading>
	<authors>G Bucci, R Detti, S Nativi, V Pasqui</authors>
	<abstract>This paper presents the work done by the research group of the University of Florence within the context of the TELEMED Project. TELEMED aims at developing a pilot application for experiencing Integrated Broadband Networks for transmitting medical, anatomical and radiological information. A large part of the project consists in the development of a multisite Image Reference Data Base (IRDB), a database containing radiological images (and associated information) approved by a scientific committee because of their particular interest. The overall architecture is based on a set of IRDB installations which are accessed from Viewing Stations (VS) located at the different partners sites. A viewing station is a workstation whose software provides special functionalities to support the presentation of images coming from IRDB. The IRDB is based on a Server Interface that provides services for accessing information contained in a Relational Database as well as images contained in a Bulk Storage. The interaction between VS and IRDB follows the client/server paradigm and uses an OSI Level-7 protocol, named Telemed Communication Language (TCL). This protocol makes the client-server dialog independent of the underlying communication subsystem, which may be as simple as a LAN or as complex as a transnational communication link. Both IRDB and VS are implemented on workstation-class machines, running UNIX. An Ethernet LAN connects the viewing stations to the IRDB, thus implementing a loosely coupled workstations system.</abstract>
	<keywords>Client/server architecture; multimedia database; radiological images; user interface; workstation</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=184>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Particle motion simulation — a parallel distributed workstation application for real-time</paper_heading>
	<authors>R.C Sinkwitz</authors>
	<abstract>Particle motion simulation subdivides graphical objects into a set of small particles behaving according to specific rules. Real-time particle motion simulation makes it possible to simulate natural phenomena like smoke or Brownian motion for demonstration and verification purposes. The paper describes how the enormous computational requirements can be solved using distributed parallel computation on a set of workstations.</abstract>
	<keywords>Particle simulation; parallel distributed computing; workstations; real-time applications</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=185>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Workstation hardware and software support for parallel applications</paper_heading>
	<authors>Susan Flynn Hummel</authors>
	<abstract>High-powered RISC microprocessors and their system software, e.g. compilers and operating systems, have primarily been optimised for use as quasi-autonomous workstations. However, as workstations become more common and the ratio of workstations to workers grows, collections of workstation-class microprocessors are increasingly being viewed as cost-effective vehicles for speeding up applications. It is therefore necessary to re-evaluate current workstation systems, and in particular, to identify what features should be included or omitted to allow their use as components of parallel machines. In this paper, we consider this assignment of labor to workstation system hardware and software, and make the case for vamping up the layer of system software, run-time supervisors, that manage parallel applications.</abstract>
	<keywords>Workstations; parallel; distributed machines; operating systems, run-time supervisors; speedup; scalability; visualization</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=186>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>UNIX on a loosely coupled architecture: The CHORUS/MiX approach</paper_heading>
	<authors>Lawrence Albinson, Dominique Grabas, Pascal Piovesan, Michel Tombroff, ... Hossein Yassaie</authors>
	<abstract>In the CHORUS/MiX1 distributed operating system architecture the microkernel provides system servers with generic services which are independent of a particular operating system; these services include scheduling, memory management and inter-process communications. In turn, co-operating system servers provide at the application programmer's interface a particular operating system personality. The CHORUS/MiX implementation of UNIX2 is based on USL source code, but is significantly re-structured into a set of system servers. This re-structuring has resulted in a modular and adaptable system which is well suited to distribution across a loosely coupled parallel architecture. The CHORUS/MiX system is further being developed to provide what has been termed single site semantics (SSS). This will make it possible to create the illusion of UNIX running on a single processor whilst taking advantage of the availability of a number of loosely coupled processors. The IMS T90003 Transputer will be one of the first processors on which CHORUS/MiX SSS will be implemented.</abstract>
	<keywords>Distributed system; soperating systems; UNIX, parallelism</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=187>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A distributed-memory, high-performance workstation</paper_heading>
	<authors>R Bisiani, O Martin</authors>
	<abstract>We have built and are now evaluating a high performance workstation, called PLUS, that is seen and felt by a user exactly as a conventional workstation because it offers the same input/output functionality and it executes the same software environment (Unix/Mach and X) of a workstation but it offers more processing power. The paper describes the architecture and presents some evaluation data.</abstract>
	<keywords>Parallelism; distributed memory; shared memory</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=188>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Extending multiprogramming to a DMPP</paper_heading>
	<authors>Peter Steiner</authors>
	<abstract>Chagori is a multi-user, multi-tasking virtual memory operating system with demand paging. It is designed for the K2 distributed memory parallel processor (DMPP) under development at our laboratory. Chagori is one of the first operating systems for DMPPs that allows multiple users to concurrently run and to interactively debug large parallel programs on a DMPP. This paper concentrates on scheduling and paging issues that arise from extending the principles of multiprogramming to tightly coupled parallel programs running on a machine which provides both coarse and fine grain communication.</abstract>
	<keywords>Distributed memory parallel processor; operating system; multiprogramming; virtual memory; fine grain communication; gang scheduling</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=189>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed hierarchical scheduling with explicit grain size control</paper_heading>
	<authors>Rutger Hofman, Willem G Vree</authors>
	<abstract>Distributed control, in this case for scheduling, is a necessity for scalable multiprocessors. Distributed control suffers under incomplete knowledge about the system state: knowledge about remote nodes is outdated, because of message travel times, and it is often limited to a neighbourhood. For our Amsterdam Parallel Reduction Machine APERM-2, we propose a distributed hierarchical scheduling algorithm, because such a control structure suffers less, from this information bottleneck. The programming discipline of our system allows us to do an educated guess towards new tasks' execution time and inherent parallelism. We use these to derive a consistent load metric and a sophisticated allocation criterion. A natural mapping of new tasks on scheduler levels is found. A pilot simulation study shows encouraging results in comparison with well-known distributed scheduling algorithms.</abstract>
	<keywords>Multiprocessor scheduling; heuristics; distributed scheduling; hierarchical algorithmg; rain size control</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=190>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A comparative study of five parallel programming languages</paper_heading>
	<authors>Henri E Bal</authors>
	<abstract>Many different paradigms for parallel programming exist, nearly each of which is employed in dozens of languages. Several researchers have tried to compare these languages and paradigms by examining the expressivity and flexibility of their constructs. Few attempts have been made, however, at practical studies based on actual programming experience with multiple languages. Such a study is the topic of this paper. We will look at five parallel languages, all based on different paradigms. The languages are: SR (based on message passing), Emerald (concurrent objects), Parlog (parallel Horn clause logic), Linda (Tuple Space), and Orca (logically shared data). We have implemented the same parallel programs in each language, using real parallel machines. The paper reports on our experiences in implementing three frequently occurring communication patterns: message passing through a mailbox, one-to-many communication, and access to replicated shared data.</abstract>
	<keywords>Parallel programming; SR; Emerald; Parlog; Linda; Orca</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=191>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation of a synchronous communication in a loosely coupled system: A correctness proof</paper_heading>
	<authors>Andrea Masini, Marco Danelutto</authors>
	<abstract>Temporal logic can be used to formally prove many interesting properties of concurrent systems. In particular, they have been used to prove liveness and safety properties of such systems. In this work we will demonstrate how temporal logic can be used to proof the correctnes of the implementation of a synchronous communication primitive. This primitive (actually a send/receive pair of primitives) is implemented onto a simple model of loosely coupled systems, in terms of an even simpler abstraction of such systems only supporting atomic read and write operations onto a distributed set of flags. Thus, first we will introduce a formalism (Read Write Concurrent Systems) that is intended to model, at a very low abstraction level, loosely coupled systems. Then we will introduce another formalism (Communicating Systems) that is also meant to model loosely coupled systems, and moreover supplies as a primitive the mechanism of synchronous communication. We will given an implementation of Communicating Systems in terms of the Read Write Concurrent Systems. Using temporal logic, we will finally show a methodology that can be used to prove the correctness of such implementation.</abstract>
	<keywords>Temporal logic; distributed; concurrent systems; verification methods</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=192>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using CSP languages to program parallel workstation systems</paper_heading>
	<authors>A Mazzeo, S Russo, G Ventre</authors>
	<abstract>During the last decade one of the most relevant events in the computer market has been the large diffusion of workstations. In both industrial and research environments a huge amount of computing is done on personal workstations. Despite the rapid growth in networking technologies, however, a network of workstations cannot be easily seen as a global computational resource, although it represents a large amount of computing power. Moreover, its inherent parallelism is not accessible without a heavy effort to modify existing software and/or to develop new code. It is our belief that the CSP model is suitable to develop distributed applications for a particular class of such systems that can be defined Parallel Workstation Systems. This thesis has been tested in the course of the DISC project. In DISC, the language implementation of the CSP model tries to minimize the programming effort toward the development of parallel applications, and a friendly programming environment, integrated in the native workbench of workstations, is provided.</abstract>
	<keywords>Concurrent languages; parallel programming environments; networks of workstations</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=193>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The distributed and parallel NIC server</paper_heading>
	<authors>Andrei Heilper, Neta Amit, Doron Cohen</authors>
	<abstract>The NIC-Server is a high-performance, low-overhead experimental system which supports distributed and parallel programming in FORTRAN using asynchronous RPC. The minimal requirements from users are natural to FORTRAN programmers; ANSI C implementation on top of the lower layers of Sun-RPC, and the built-in scheduler, ensure good portability across systems.</abstract>
	<keywords>Distributed processing; computer interfaces; programming languages</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=194>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Developing a software interface for the dynamically reconfigurable SuperNode multiprocessor</paper_heading>
	<authors>J.M Adamo, N Alhafez, J Bonneville, C Bonello, ... L Trejo</authors>
	<abstract>A software interface for SuperNode is presented which is based on a virtual machine that allows dynamically reconfigurable programs to be easily implemented. The software interface actually constitutes an entire programming environment, known as C_ NET. The latter appears to the programmer as a three-compartment high level language that implements the virtual machine and makes it possible to symbolically manipulate all the resources of SuperNode.</abstract>
	<keywords>Dynamically reconfigurable machines; phase-sequencing programming; control busswitch programming</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=195>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Heterogeneous network-based concurrent computing environments</paper_heading>
	<authors>V.S Sunderam</authors>
	<abstract>Concurrent computing on networks of heterogeneous machines has gained tremendous attention and popularity in recent months. This computing model encompasses a wide spectrum of environments, ranging from very coherent systems to loosely coupled workstations, but usually excludes distributed operating systems. Many such systems are operational (several more are under development) and have been used for an impressive range of applications. Heterogeneous network computing environments have the potential for multi-gigaflop performance, provide excellent support for applications with multifaceted requirements, enable straightforward provisions for fault-tolerance, and are easy to install, maintain, and use. These systems enhance the power, versatility, and usefulness of existing hardware, and support well-understood, pragmatic, programming models. In this paper, we will survey the state of the art in heterogeneous network computing, beginning with a description of representative systems and applications. We will also attempt to identify the most pressing needs as well as the foreseeable obstacles, and pose a sampling of the major challenges that face researchers and developers in heterogeneous network computing.</abstract>
	<keywords>Concurrency; heterogeneous computing; distributed systemshigh-performance; practical experiences</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=196>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A methodology for the development and the support of massively parallel programs</paper_heading>
	<authors>Marco Danelutto, Roberto Di Meglio, Salvatore Orlando, Susanna Pelagatti, Marco Vanneschi</authors>
	<abstract>The most important features that a parallel programming language should provide are portability, modularity, and ease of use, as well as performance and efficiency. Current parallel languages are only characterized by some of these features. For instance, most of these languages allow programmers to efficiently exploit the massively parallel target machine. Unfortunately, the estimation of the performance of each application is usually made by the programmer, without the support of any tool. Moreover, the programs produced by using such languages are not portable or easily modifiable. Here, we present a methodology to easily write efficient, high performance and portable massively parallel programs. The methodology is based on the definition of a new explicitly parallel programming language, namely P3L, and of a set of compiling tools that perform automatic adaptation of the program features to the target architecture hardware. Target architectures taken into account here are general purpose, distributed memory, MIMD architectures. These architectures provide the scalability and low cost features that are necessary to tackle the goal of massively parallel computing. Following the P3L methodology, the programmer has just to specify the kind of parallelism he is going to exploit (pipeline, farm, data, etc.) in the parallel application. Then, P3L programming tools automatically generate the process network that implements and optimizes, for the given target architecture, the particular kind of parallelism the programmer indicated as the most suitable for the application.
</abstract>
	<keywords>Massive parallelism; optimization tools; MIMD architectures; high level languages</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=197>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programming tools for distributed multiprocessor computing environments</paper_heading>
	<authors>Thomas Bemmerl, Bernhard Ries</authors>
	<abstract>With the increasing availability of multiprocessor platforms based on different types of architectures (shared memory, distributed memory, network based), users will be increasingly faced with heterogeneous and distributed multiprocessor computing facilities. Programming environment concepts have to be found which enable the user to program and use these heterogeneous computing resources, consisting of different multiprocessor architectures, in a transparent manner. Topics which have to be addressed and tools which have to be developed are parallel programming models, monitoring systems, debuggers, performance analyzers, animation systems and dynamic load-balancing schemes. The paper describes the integrated tool environment TOPSYS (TOols for Parallel SYStems), available for the iPSC/2 and iPSC/860 hypercube, as well as its adaptation to a network of UNIX-based workstations.</abstract>
	<keywords>Shared memory multiprocessors; distributed memory multiprocessors; networks of workstations; parallel programming models; parallel programming tools; heterogeneity</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=198>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed shared virtual memory on RISC System/6000 clusters and large scale computations: Two case studies</paper_heading>
	<authors>M Bernaschi, M Blount, P Sguazzero, M Vitaletti</authors>
	<abstract>Many of today's moderately parallel and massively parallel multicomputers are non-shared memory systems, using the message passing programming paradigm for parallel applications. This is significant, since the shared memory paradigm is a well established programming paradigm on high performance computers, like the vector multiprocessors (vmp). Distributed shared virtual memory (dsvm) provides a way of using the shared memory programming paradigm on a set of loosely coupled computer systems. The work to date suggests that one can implement some moderately parallel applications on dsvm loosely coupled systems and achieve performances comparable to those on message passing non-shared memory systems. This paper reports first on DSVM6K, a dsvm system implemented in the AIX version 3 operating system on IBM's RISC System/6000, and then describes MTF6K, a prototypal implementation of MultiTasking Fortran on a cluster of RISC/6000 engines: MTF is a component of the IBM VS FORTRAN library which provides FORTRAN programmers with coarse grain, explicit multitasking primitives. In the last two sections of the paper two large scale applications are analyzed, a large scale 2D seismic modeling application and a 3D aerodynamic simulation.</abstract>
	<keywords>Virtual shared memory; Fortran interface; seismic; aerodynamic</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=199>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed multimedia systems</paper_heading>
	<authors>Sape J Mullender</authors>
	<abstract>Multimedia systems will allow professionals worldwide to collaborate more effectively and to travel substantially less. But for multimedia systems to be effective, a good systems infrastructure is essential. In particular, support is needed for global and consistent sharing of information, for long-distance, high-bandwidth multimedia interpersonal communication, greatly enhanced reliability and availability, and security. These systems will also need to be easily usable by lay computer users. In this paper we explore the operating system support that these multimedia systems must have in order to do the job properly.</abstract>
	<keywords>Distributed systems; multimedia systems; global sharing; interpersonal communication; storage; reliability; availability; security</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=200>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using full message exchange for probabilistic clock synchronization (extended abstract)</paper_heading>
	<authors>G Alari, A Ciuffoletti</authors>
	<abstract>We present a new solution to the external clock synchronization problem, based on two works: [1] and [2]. We first introduce these two papers, then present our protocol and analyze its performance by a simulation. Finally we propose a new approach for reading remote clocks and observe its effectiveness in the context of our protocol.</abstract>
	<keywords>Synchronization; distributed systems</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=201>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High level communication mechanisms for distributed parallel computers from an adaptive message routing</paper_heading>
	<authors>M Cannataro, G Spezzano, D Talia</authors>
	<abstract>High level communication primitives play an important role in the implementation of distributed applications on parallel computers. This paper shows how these primitives may be developed from an efficient routing algorithm. The routing algorithm used here has many positive characteristics including deadlock-freedom, guaranteed message arrival, and automatic local congestion reduction. The experimental results show that the routing algorithm is effective to support process to process communication on a parallel computer. The goal is the implementation of a virtual communication level which allows the implementation of topology independent parallel applications.</abstract>
	<keywords>Parallel processing; communication systems; message routing; broadcast</keywords>
	<publication_month_year>1992-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 1-3</volumes_issues>
</paper>
<paper no=202>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Loosely-coupled processes</paper_heading>
	<authors>Jayadev Misra</authors>
	<abstract>The notions of ‘loosely-coupled’ and ‘tightly-coupled’ processes have been used in an intuitive way for many years by system designers. In this paper, we propose a precise characterization. All point-to-point message-communicating processes are loosely-coupled under our characterization. Furthermore, a number of common paradigms of process interactions - e.g. broadcast or accumulating a result - satisfy our characterization of loose-coupling. Restricting our designs to loosely-coupled processes has several advantages, among them a design methodology in which each process implements a progress property and a simple solution to the cache coherence problem.</abstract>
	<keywords>Loose-coupling; tight-coupling; cache-coherence; designs of asynchronous systems; implementing progress properties</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=203>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A high-speed memory organization for hybrid dataflow / von Neumann computing</paper_heading>
	<authors>Herbert H.J Hum, Guang R Gao</authors>
	<abstract>Multi-threaded node architectures have been proposed as a promising means to tolerate latency of memory accesses in a multiprocessor system. In this paper, we propose a novel organization of high-speed memories, known as the register-cache, for a multi-threaded architecture. As the term suggests, it is organized both as a register file and a cache. Viewed from the execution unit, its contents are addressable as ordinary CPU registers using relatively short addresses. From the main memory perspective, it is content addressable, i.e. its contents are tagged just as in conventional caches. In this register-cache organization, a number of registers are grouped into a block of registers where a register in a block is accessed using an offset from the address of the block, an offset value which is embedded in the compiler generated code. The binding of register block locations to register-cache line addresses is adaptively performed at runtime, thus resulting in a dynamically allocated register file. In this execution model, a program is compiled into a number of instruction threads called super-actors. A super-actor becomes ready for execution only when its input data are physically residing in the register-cache and space is reserved in the register-cache to store its result. Therefore, the execution unit will never stall or ‘freeze’ when accessing instruction or data. Another advantage is that since registers are dynamically assigned at runtime, register allocation difficulties at compile-time, e.g. allocating registers for subscripted variables of large arrays, can be avoided. Architectural support for overlapping executions of super-actors and main memory operations are provided so that the available concurrency in the underlying machine can be better utilized. The preliminary simulation results seem to be very encouraging: with software pipelined loops, a register-cache of moderate size can keep the execution unit usefully busy.</abstract>
	<keywords>Memory organization; computer architecture; multi-threaded architecture; hybrid dataflow/von Neumann architecture; register-cache; super-actor machine</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=204>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel associative combinator evaluation II</paper_heading>
	<authors>Martin Waite, Bret Giddings, Simon Lavington</authors>
	<abstract>A new evaluation model for SK combinator expressions is presented and used as a basis for the design of a novel processor. The resulting machine architecture resembles a dataflow ring, but executions are constrained to be fully lazy. An automatic dynamic load sharing mechanism for a distributed multiprocessor architecture is suggested, and initial simulation results are presented.</abstract>
	<keywords>Novel parallel architecture; fine-grained combinator reduction</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=205>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The function processor: A data-driven processor array for irregular computations</paper_heading>
	<authors>Jesper Vasell, Jonas Vasell</authors>
	<abstract>The Function Processor is a data-driven processor array architecture, i.e. a regular structure of locally interconnected processing elements called Function Cells, which operate according to the data flow execution principle. By means of a compilation method developed for this architecture, data flow graphs for functional programs can be created and mapped onto the processor array, so that each Function Cell is assigned the execution of one graph node. The main result presented is a Function Cell architecture which has been designed to support the functionality required by these data flow graphs. We also give a brief description of a programming method for the architecture. Furthermore, some results from an implementation are presented.</abstract>
	<keywords>Reconfigurable processor array; dataflow computing; functional programming; irregular computation</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=206>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A configuration approach to parallel programming</paper_heading>
	<authors>Jeff Magee, Naranker Dulay</authors>
	<abstract>This paper advocates a configuration approach to parallel programming for distributed memory multicomputers, in particular, arrays of transputers. The configuration approach prescribes the rigorous separation of the logical structure of a program from its component parts. In the context of parallel programs, components are processes which communicate by exchanging messages. The configuration defines the instances of these processes which exist in the program and the paths by which they are interconnected. The approach is demonstrated by a toolset (Tonic) which embodies the configuration paradigm. A separate configuration language is used to describe both the logical structure of the parallel program and the physical structure of the target multicomputer. Different logical to physical mappings can be obtained by applying different physical configurations to the same logical configuration. The toolset has been developed from the Conic system for distributed programming. The use of the toolset is illustrated through its application to the development of a parallel program to compute Mandelbrot sets.</abstract>
	<keywords>Parallel programming environment; parallel programming language; configuration language; transputers</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=207>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Mapping uniform recurrences onto small size arrays</paper_heading>
	<authors>Vincent Van Dongen</authors>
	<abstract>Given a regular application described by a system of uniform recurrence equations, systolic arrays are commonly derived by means of an affine transformation; an affine schedule determines when the computations are performed and an affine processor allocation where they are performed. Circuit transformations are then applied on the resulting circuit when the application needs to be mapped onto a smaller size array. This method is in two steps and thus can hardly be optimized globally. We hereafter present a different method for designing small size arrays. We derive them in one step by means of an affine schedule and a near-affine processor allocation. By doing so, we can generalize the optimization technique for affine mapping to be applicable here. The method is illustrated on the band-matrix multiplication and on the convolution algorithms.</abstract>
	<keywords>Uniform recurrence; systolic array designnear-affine mapping; automatic partitioning</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=208>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Control structures for data-parallel SIMD languages: semantics and implementation</paper_heading>
	<authors>Luc Bougé, Jean-Luc Levaire</authors>
	<abstract>We define a simple language which encapsulates the main concepts of SIMD data-parallel programming, and we give its operational semantics. This language includes a unique data-parallel control structure called multitype conditioning and escape. We show that it suffices to express all data-parallel extensions of usual scalar control structures of C, as found in C∗, MPL, POMPC, etc. Moreover, we give a formal correctness proof for two different implementations of this new statement, respectively by a single context stack, and by a set of counters. Thus, this simple language appears as an interesting basis to study data-parallel SIMD programming methodology.</abstract>
	<keywords>Semanticsdata parallel languages; SIMD; massively parallel architecture; Connection Machine; structured operational semantics; program equivalenceC∗MPLPOMPC</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=209>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>FORK: A high-level language for PRAMs</paper_heading>
	<authors>T Hagerup, A Schmitt, H Seidl</authors>
	<abstract>We present a new programming language designed to allow the convenient expression of algorithms for a parallel random access machine (PRAM). The language attempts to satisfy two potentially conflicting goals: On the one hand, it should be simple and clear enough to serve as a vehicle for human-to-human communication of algorithmic ideas. On the other hand, it should be automatically translatable to efficient machine (i.e. PRAM) code, and it should allow precise statements to be made about the amount of resources (primarily time) consumed by a given program. In the sequential setting, both objectives are reasonably well met by the Algol-like languages, e.g. with the RAM as the underlying machine model, but we are not aware of any language that allows a satisfactory expression of typical PRAM algorithms. Our contribution should be seen as a modest attempt to fill this gap.</abstract>
	<keywords>Parallel language; synchronization; shared memory; processor allocation</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=210>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards a single model of efficient computation in real parallel machines</paper_heading>
	<authors>Pilar de la Torre, Clyde P Kruskal</authors>
	<abstract>We propose a model of parallel computation, the YPRAM, that allows general parallel algorithms to be designed for a wide class of parallel models. The basic model captures locality among processors, which is measured as a function of two parameters; latency and bandwidth. We design YPRAM algorithms for solving several fundamental problems: parallel prefix, sorting, sorting numbers from a bounded range, and list ranking. We show that our model predicts, reasonably accurately, the actual known performances of several basic parallel models — PRAM, hypercube, mesh and tree — when solving these problems.</abstract>
	<keywords>Models of parallel computation; parallel algorithms; efficient parallel algorithms; PRAM; parallel processing; processor networks; latency; bandwidth; YPRAM</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=211>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Structural operational semantics for AKL</paper_heading>
	<authors>Seif Haridi, Sverker Janson, Catuscia Palamidessi</authors>
	<abstract>The Andorra Kernel Language (AKL) is a concurrent constraint programming language. It can be seen as a general combination of logic programming languages such as Prolog, GHC, and Parlog, the first of which provides don't know nondeterminism, and the last two of which are concurrent logic programming languages. The constraint system is an independent parameter of the language description. In this paper, we revisit the description of Janson and Haridi [10], adding the formal machinery which is necessary in order to completely formalize the control of the computation model. To this we add a formal description of the transformational semantics of AKL. The semantics is a set of or-trees which also captures infinite computations.</abstract>
	<keywords>Logic programming; constraint programming; concurrency; control of nondeterminism</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=212>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Invariants and paradigms of concurrency theory</paper_heading>
	<authors>Ryszard Janicki, M Koutny</authors>
	<abstract>We introduce a new invariant semantics of concurrent systems which is a direct generalisation of the causal partial order semantics. Our new semantics overcomes some of the problems encountered when one uses causal partial orders alone. We discuss various aspects of the new invariant model. In particular, we outline how the new invariants can be generated by I-safe inhibitor Petri nets.</abstract>
	<keywords>Concurrency; semantics; causal partial orders</keywords>
	<publication_month_year>1992-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 8, Issues 4</volumes_issues>
</paper>
<paper no=213>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multicomputer molecular dynamics</paper_heading>
	<authors>P.J Mitchell, D Fincham</authors>
	<abstract>Molecular dynamics simulation is an important computational technique in the study of the liquid and solid states of matter. Molecular motions are treated explicitly by numerical integration of the equations of motion. This involves the calculation of inter-molecular interactions. There are two basic parallelisation strategies: particle decomposition and spatial decomposition. By combining some of the features of both we have been able to develop a parallel version of the fastest known sequential algorithm, the link cell neighbor list. The program has been implemented on a Meiko Computing Surface using Fortran and CSTools. We relate the performance of the method and its scaling properties to the performance of the hardware and communications software.
</abstract>
	<keywords>Molecular dynamics; particle decomposition; spatial decomposition; link cell</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=214>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementing finite difference ocean circulation models on MIMD, distributed memory computers</paper_heading>
	<authors>A.R Clare, D.P Stevens</authors>
	<abstract>This paper considers the use of parallel computers for ocean modelling. After a brief review of the topic, the authors describe the experience of porting a simplified ocean model onto the Computing Surface. The parallel implementation is based on a straightforward domain decomposition. The use of CSTools in the code is briefly discussed. The performance of the parallel code, when run on Inmos T800 transputers and Intel 1860's, is compared with the performance of a serial implementation when run on a range of commonly used serial computers (including a CRAY X-MP and an AMDAHL VP1200).</abstract>
	<keywords>High performance computing; parallelism; CFD; ocean circulation models</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=215>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel 2-d hydrodynamic FORTRAN code for astrophysical applications on a Meiko computing surface</paper_heading>
	<authors>F Reale, M Barbera, S Sciortino</authors>
	<abstract>We have parallelized a FORTRAN two-dimensional hydrodynamic Flux Corrected Transport (FCT) code for astrophysical applications, using CSTools on a Meiko Computing Surface hosted by a SUN 4/370 workstation and fully integrated in a Unix/Ultrix Local Area Network. The processor topology maps the domain decomposition and message exchanges are required between adjacent processors. We have developed and used a library of high-level network and communication routines, based on CSTools primitives, and applicable to other parallel codes. We adopted a master-slave paradigm, and code parallelization took around twelve man hours. On a standard hydrodynamic problem we obtain a parallel efficiency above 90%, on arrays up to four 20 MHz T800 transputers, and performances in the range of several Mflops on Meiko boards with faster Intel 1860 processors.</abstract>
	<keywords>Transputer; processor arrays; parallel/concurrent computing; hydrodynamics; astrophysics</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=216>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evaluation of a database application on a Meiko multisparc Oracle platform</paper_heading>
	<authors>Iain Cramb, Norman Winterbottom</authors>
	<abstract>The evaluation of a heterogeneous distributed-memory Meiko Oracle platform is discussed, along with an investigation into the possible cost-performance benefits and implementation issues associated with the transfer of a commercial database application from sequential machines to a parallel one. The discussion includes an overview of the hardware, the software, and comments on system cost performance in comparison with conventional machines. Finally, there are some general remarks on system scalability with respect to performance and efficiency.</abstract>
	<keywords>Parallel processing; parallel Oracleparallel databases; relational databases; transputerssparcs</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=217>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Searching databases using parallel genetic algorithms on a transputer computing surface</paper_heading>
	<authors>Jun Cui, Terence C Fogarty, John G Gammack</authors>
	<abstract>Although many organisations have large databases containing useful information, few believe they are using these to maximum effect. When databases are very large, such as those provided by satellite observations or in highly computerised industries such as banking, searching them to discover the inherent information becomes a problem in itself. The volume of data involved is often simply too large to be tractable in real time using single processor technology. In this paper we describe the use of a parallel architecture, the Meiko computing surface, in conjunction with techniques from artificial intelligence in finding optimised information inherent in such databases, and give results which show the value of our approach.</abstract>
	<keywords>Genetic algorithms; transputer; crossover; mutation; Travelling Salesman Problem; parallel; database</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=218>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallelising a financial system</paper_heading>
	<authors>J.A Keane</authors>
	<abstract>This paper reports on the parallelising of a financial system that calculates actuarial liabilities. The purpose of such a system is to evaluate the liabilities of a pension scheme by calculating the expected cost of each member to the system in the future. An analysis of the system has been carried out and two levels of parallelism have been detected. From the more coarse-grained of these levels a parallel model has been proposed. A parallel simulation of this model has been developed both on a Meiko Computing Surface distributed store machine populated with Inmos transputers and Intel i860 microprocessors, and on a network of workstations to enable detailed evaluation of the potential speedup. Figures are given and various metrics are used to analyse the speedup and efficiency factors. The potential for making use of virtual shared memory and lightweight thread packages at this coarse-grained level of parallelism is considered. The more fine-grained level of parallelism identified is discussed with a view to massively parallel architectures.</abstract>
	<keywords>Granularity of parallelism; financial systems; parallel programming models</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=219>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An implementation of a portable instrumented communication library using CS tools</paper_heading>
	<authors>Andrew Grant, Robert Dickens</authors>
	<abstract>The Portable Instrumented Communication Library (PICL), developed at Oak Ridge National Laboratory, is a high level communications library which has been implemented on a range of distributed memory parallel machines such as the Intel IPSC/2 and the NCUBE. The library is implemented on top of the native message passing libraries for those machines, hence, programs written using PICL are portable at source code level between the different machines. PICL has an associated tool, called ParaGraph, which allows tracing information generated by PICL programs to be displayed graphically. This can give some insight into program efficiency, load balancing and communications overhead. This paper describes an implementation of PICL on a Meiko Computing Surface using CS Tools. The use of its associated animated graphical display system, ParaGraph, and of some other types of profiling tools is also discussed.</abstract>
	<keywords>PICL; ParaGraph; communication libraries; CS tools; Meiko Computing Surface; distributed memory parallel computers; performance monitoring; portability</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=220>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>NAUTA: A network administration utility for transputer architectures</paper_heading>
	<authors>G Marino, G Succi, G Levo, R Pavesio</authors>
	<abstract>MIMD reconfigurable architectures including transputer arrays, are becoming very popular because of their natural approach to parallelism and the fact that they grant large flexibility. However, they leave a crucial task to the programmer: he must divide the body of the whole program in many parallel modules, and define the network's topology in order to best adapt it to the communication needs arising when the various modules must communicate with each other. The computation and communication loads should be well distributed over the network available resources, in order to ensure good performances. Maybe this goal cannot be accomplished at the very first attempt, thus it would be useful to measure the traffic along the various links of the network in search for ‘bottlenecks’ which cause delays. This would suggest which changes to the distribution of computation duties and to the routing of messages which would lead to an improvement of the global traffic's fluency. NAUTA was developed to cope with the problems of administrating the topology of the network and analyzing its subsequent influences upon the general behaviour, when running an application on a transputer architecture. It offers the user a graphical interface to set up the network configuration and to load the processes constituting the application onto the processors. Finally, NAUTA gives an overview upon the runtime behaviour of the processors and the traffic between them, in order to highlight possible bottlenecks. This overview is presented in the form of a set of runtime-evolving ‘Strip charts’, providing a good degree of comprehension to a user's glance. The present version of NAUTA was designed to operate inside the environment of a transputer computing system with CSTools, the application development toolset for multiprocessor systems distributed by Meiko Ltd. However, since NAUTA is essentially an interface between the application programmer and the computing environment, not many details of the latter need to be known, perhaps apart from some small constraints on the realization of the topology which could be removed when handling a different system to Meiko's. The NAUTA approach is quite general and portable to other kinds of MIMD machines. The graphical world where NAUTA was conceived is that of XWindows, and in particular massive use was made of the XtIntrinsics [1] and the Athena Widgets [2]: in this way the development of NAUTA's interface proceeds with the aid of the great modularity supported by the Widgets and it is very easy and intuitive for a user to learn how to use it [3].</abstract>
	<keywords>Parallel programming tools; transputer; MIMD architectures</keywords>
	<publication_month_year>1993-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 1</volumes_issues>
</paper>
<paper no=221>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An overview and appraisal of the Fifth Generation Computer System project</paper_heading>
	<authors>R.P van de Riet</authors>
	<abstract>An overview will be given in this paper of the original plans of the Japanese Fifth Generation Computer Systems project and they will be compared with what has actually been achieved in the project's ten years of work. This is done by looking at the results demonstrated at the 1992 FGCS Conference, in the form of some 20 application programs all running on the PIM (Parallel Inference Machine) hardware and written in (systems written on top of) KL1. KL1 is the language developed for the FGCS project for the operating system controlling hundreds of processors, as well as for applications. It is derived from Prolog and could be called a Logic Programming language. Also the results of a team of international experts, asked by ICOT to evaluate the success of the FGCS project, will be given in a condensed form.</abstract>
	<keywords>Fifth Generation Computer Systems project; Parallel Inference Machine; Logic programming language KL1</keywords>
	<publication_month_year>1993-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 2</volumes_issues>
</paper>
<paper no=222>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Japanese national Fifth Generation project: Introduction, survey, and evaluation</paper_heading>
	<authors>Edward Feigenbaum, Howard Shrobe</authors>
	<abstract>Projecting a great vision of intelligent systems in the service of the economy and society, the Japanese government in 1982 launched the national Fifth Generation Computer Systems (FGCS) project. The project was carried out by a central research institute, ICOT, with personnel from its member-owners, the Japanese computer manufacturers (JCMs) and other electronics industry firms. The project was planned for ten years, but continues through year eleven and beyond. ICOT chose to focus its efforts on language issues and programming methods for logic programming, supported by special hardware. Sequential ‘inference machines’ (PSI) and parallel ‘inference machines’ (PIM) were built. Performances of the hardware-software hybrid was measured in the range planned (150 million logical inferences per second). An excellent system for logic programming on parallel machines was constructed (KL1). However, applications were done in demonstration form only (not deployed). The lack of a stream of applications that computer customers found effective, and the sole use of a language outside the mainstream, Prolog, led to disenchantment among the JCMs.</abstract>
	<keywords>FGCS; inference enginelogic programming; parallel architecture</keywords>
	<publication_month_year>1993-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 2</volumes_issues>
</paper>
<paper no=223>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evaluation of KL1 and the inference machine</paper_heading>
	<authors>Henri E Bal</authors>
	<abstract>The paper first briefly describes the KL1 language and its implementation on the Parallel Inference Machine (PIM). KL1 is a successor of Flat GHC and adds features intended for writing efficient, production-quality programs that exploit physical parallelism. KL1 has been used for writing the PIMOS operating system. An implementation of KL1 on the PIM must solve several problems, such as: the representation of KL1's flat global name space on PIM's hierarchical memory; memory management; distributed unification; goals scheduling; and meta-control. The language and its implementation are evaluated and compared with related systems.</abstract>
	<keywords>KL1; PIM; PIMOS</keywords>
	<publication_month_year>1993-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 2</volumes_issues>
</paper>
<paper no=224>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Appraisal of parallel processing research at ICOT</paper_heading>
	<authors>Evan Tick</authors>
	<abstract>A major goal of the Fifth Generation Computer Systems project was the development of high-performance multiprocessors for symbolic applications. The key technologies envisioned as critical were fine-grain concurrent languages and custom microprocessors for directly executing those languages. In contrast to this top-down design approach were most other research laboratories, concentrating on further refining scalable architectures by speeding up communication. One main evolutionary trend was towards threaded architectures that supported latency hiding by fast task switching and low-cost message passing. This article attempts to compare these top-down and bottom-up approaches.</abstract>
	<keywords>Multiprocessing; threaded architectures; pipelining; logic programming</keywords>
	<publication_month_year>1993-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 2</volumes_issues>
</paper>
<paper no=225>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evaluation of ICOT's natural language research</paper_heading>
	<authors>James Barnett, Kenji Yamada</authors>
	<abstract>The Natural Language Processing group at ICOT was small and underwent a considerable turnover in personnel. As a result, the topics of research were quite diverse, though the general areas of interest were similar to those in the West during the same period. The primary emphases were on parsing, the development of grammar formalisms, and the design of an experimental discourse processing system. One feature of interest was the incorporation into their grammar formalisms of the traditional Japanese linguistic theory. Some of the software and the linguistic data are publicly available as a part of the ICOT Free Software and are useful to researchers who are working on Japanese.</abstract>
	<keywords>Natural language processing; parsing; Japanese grammar</keywords>
	<publication_month_year>1993-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 2</volumes_issues>
</paper>
<paper no=226>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automated theorem-proving research in the Fifth Generation Computer Systems Project: Model generation theorem provers</paper_heading>
	<authors>Mark E Stickel</authors>
	<abstract>One of the successful outcomes of the Fifth Generation Computer Systems Project is the development of Model Generation Theorem Provers (MGTPs). MGTPs have solved previously open problems in finite algebra, produced rapid proofs of condensed detachment problems, and are providing an inferential infrastructure for knowledge-processing research at ICOT. They successfully exploit the Fifth Generation Project's KLl logic programming language and parallel inference machines to achieve high performance and parallel speedup. This paper describes some of the key properties of MGTPs, reasons for their successes, and possible areas for future improvement.</abstract>
	<keywords>Automated theorem proving; Model generation; MGTP; SATCHMO</keywords>
	<publication_month_year>1993-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 2</volumes_issues>
</paper>
<paper no=227>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An evaluation of the FGCS data & knowledge base system — expectations and achievements</paper_heading>
	<authors>Shojiro Nishio</authors>
	<abstract>In this paper the developments of the Data & Knowledge Base System will be described such as they evolved in the FGCS project and an appraisal will be given. The Data & Knowledge Base System consists primarily of the Kappa and Quixote systems. Kappa is the lower level system, in particular Kappa-P is the parallel DBMS implemented on a parallel inference machine. Quixote is essentially a constraint logic programming language with object-oriented features, such as object identity, complex objects, encapsulation, type hierarchy and methods. Quixote is used for powerful applications, such as a legal system and a system for molecular biological databases.</abstract>
	<keywords>Data and knowledge bases systems; deductive databases; object-oriented databases; fifth generation computer systems; logic programming</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=228>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An adaptive granularity control algorithm for the parallel execution of functional programs</paper_heading>
	<authors>Gad Aharoni, Amnon Barak, Yaron Farber</authors>
	<abstract>An on-line (run time), adaptive algorithm for granularity control of the parallel execution of functional programs is presented. This algorithm utilizes the inherent parallelism of functional programs, yet it endeavours to limit excessive parallelism. It can be applied to programs without any prior knowledge of their behaviour, and it does not require the programmer to add explicit parallel annotations. The algorithm takes advantage of the ability to expose, at run time, the shape of the underlying computation graph of the program. It also balances between the amount of local computation and the costs of remote execution. Based on these considerations, the algorithm allocates tasks to processing elements in a way that tries to minimize the overall execution time of the program. Continuous sampling of the computation graph allows self-tuning of the algorithm, when the underlying graph changes gradually.</abstract>
	<keywords>Granularity; parallel processing; fine-grained computation; functional programming; on-line; competitive algorithms</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=229>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experience with a clustered parallel reduction machine</paper_heading>
	<authors>M Beemster, P.H Hartel, L.O Hertzberger, R.F.H Hofman, ... J.C Mulder</authors>
	<abstract>A clustered architecture has been designed to exploit divide and conquer parallelism in functional programs. The programming methodology developed for the machine is based on explicit annotations and program transformations. It has been successfully applied to a number of algorithms resulting in a benchmark of small and medium size parallel functional programs. Sophisticated compilation techniques are used such as strictness analysis on non-flat domains and RISC and VLIW code generation. Parallel jobs are distributed by an efficient hierarchical scheduler. A special processor for graph reduction has been designed as a basic block for the machine. A prototype of a single cluster machine has been constructed with stock hardware. This paper describes the experience with the project and its current state.</abstract>
	<keywords>Clustered architecture; parallelism; functional programs</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=230>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parlog based real-time distributed logic environment</paper_heading>
	<authors>M Díaz, J.M Troya</authors>
	<abstract>The high complexity of distributed computer systems requires new programming languages and tools which will avoid many of the details that are necessary to take into account with the traditional languages. The new proposals of declarative languages and the advances in compilation techniques will make these languages an alternative to imperative languages in some areas. We propose a real-time distributed environment based on a concurrent logic language. With this approach we try to provide the advantages of a declarative language to real-time distributed system. Because of the interactive features of the distributed systems, these cannot be distributed systems, these cannot be described in usual logical or functional terms and it is necessary to use reactive languages. There is a class of logic languages with a reactive behaviour named Concurrent Logic Languages which are well suited for this kind of problem. We have extended one of these languages (Parlog) with real-time and communications primitives, and we have developed a distributed environment which can create and control remote processes in a very easy and efficient way. The remote execution of processes is based on a remote metacall that we have developed. A prototype of the environment has been implemented using metainterpretation. The real-time extension includes the definition of real-time primitives and the modification of the execution model of Concurrent Logic Languages. We give a new operational semantics for the extended language. We also describe a tool for translating extended state machine specifications to the environment. This will allow the creation of executable prototypes on distributed computer systems and the validation of complex system specifications in an easy way.</abstract>
	<keywords>Distributed programing; concurrent logic languages; real-time systems; semantics models; formal description techniques</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=231>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A data cache for Prolog architectures</paper_heading>
	<authors>Lanfranco Lopriore</authors>
	<abstract>By capturing the references to the Prolog data space, the cache architecture presented in this paper reduces the memory bandwidth requirements of Prolog programs. For each Prolog stack, the cache contains a contiguous portion, called the window, of the memory area reserved to contain that stack. The programs control the cache activity by means of a set of commands, which make it possible to position the windows and to change their size. The cache architecture takes advantage of the memory referencing characteristics of Prolog programs to obtain an efficient utilisation of the cache storage resources while reducing both the complexity of the logic for cache space management and the memory traffic generated by the cache operations.</abstract>
	<keywords>Cache; Prolog; stack</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=232>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The M2 hierarchical multiprocessor</paper_heading>
	<authors>Yen-Jen Oyang, David Jinsung Sheu, Chih-Yuan Cheng, Cheng-Zen Yang</authors>
	<abstract>This paper discusses the design and development of a bus-based hierarchical multiprocessor named M2. The primary design goal of the M2 is to derive a multiprocessor architecture that features much higher degree of scalability than the shared-memory shared-bus architecture to meet the ever increasing processing power demanded by large database/knowledgebase computing and transaction processing. If compared with other hierarchical multiprocessors, the M2 is distinctive in its memory configuration, which is aimed at avoiding severe inter-CPU interference due to page-swapping events. If compared with a group of multiprocessors connected by a local area network, the M2 enjoys higher scalability due to higher bandwidth of the backplane bus.</abstract>
	<keywords>Hierarchical multiprocessor, scalability; shared-memory shared-bus multiprocessor; message-passing; distributed memory</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=233>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A logical-time-based approach to decentralised resource allocation in distributed systems</paper_heading>
	<authors>John G Vaughan</authors>
	<abstract>The Time Frame Resource Request (TFRR) scheme is presented as a method for achieving temporal agreement in distributed systems. The scheme is based on prior agreement between sender and receiver processes on the exchange of a data message. The implications of this agreement for the advancement of virtual time are examined in detail. A global synchronisation based on TFRR can be used to solve the distributed mutual exclusion problem. In a group of n potential contenders, the solution necessitates the sending of at most n + 1 messages per contending process and n − 1 messages per non-contending process for each critical section entry.</abstract>
	<keywords>Logical time; distributed synchronisation; mutual exclusion</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=234>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Exploiting OR-parallelism in logic programs: A review</paper_heading>
	<authors>Kang Zhang</authors>
	<abstract>Different forms of parallelism have been identified in logic programs for efficient implementations on multiprocessor systems. Among these, OR- and AND-parallelism have been the focus for exploitation for the past decade. Research in such exploitation has led to the proposals and implementations of various execution models and logic programming systems. This paper attempts to summarise, in the form of a structured review, the major activities in the area of exploiting OR-parallelism. Problems arising in implementing OR-parallelism and various models and working systems featuring different techniques for solving these problems are discussed.</abstract>
	<keywords>Logic programming; Prolog; OR-parallelism; multiprocessor systems</keywords>
	<publication_month_year>1993-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 3</volumes_issues>
</paper>
<paper no=235>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PREVISE: A knowledge-based system to support the preparation and verification of space operations procedures</paper_heading>
	<authors>A de Saint Vincent, F Lecouat, G Leonis, F Allard</authors>
	<abstract>This paper presents the Previse system for preparing and verifying operations procedures, developed in the frame of a project conducted by Matra Marconi Space for the European Space Agency ESA/ESTEC. This knowledge-based system supports an evolutive procedures language (syntax and vocabulary). It improves the efficiency of procedures preparation by providing powerful editing facilities, by checking the validity of the syntax used by procedures writers, and by automating the formatting task. It also provides a set of verification mechanisms allowing to detect different types of semantic errors in procedures.</abstract>
	<keywords>Procedures generation; Procedures verification; Knowledge-based system</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=236>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SE-TC2: Telecom 2 expert system (The first expert system in a CNES Satellite Control Centre)</paper_heading>
	<authors>Christophe Bastien-Thiry</authors>
	<abstract>In this paper the author describes the first expert system in CNES designed to operate in a Satellite Control Centre (SCC) from the project point of view. The SE-TC2 is mainly dedicated to fault isolation on the TELECOM 2 satellite platform. We explain why we believe in this kind of system and how it should be designed and developed in the future within the SCC project. The first part describes the origins of this system and gives details of how the needs were confirmed by the end of an R&T study carried out from 1986 to 1989. This study showed the feasibility of such an approach and allowed us to specify the functionalities an operational system should offer. The paper then goes on to discuss the framework in which the project was completed. We give enough details on the system's architecture to be able to show why we think it is likely to be accepted by users in accordance with their current habits. We also clarify the kind of knowledge used in the system and explain why it is so crucial to use a form and content well-known to potential users. There follows a brief section on the services the system is supposed to provide and its restrictions. Some of these limitations are due to financial or technical reasons, but others are desirable insofar as we consider the way a satellite is currently controlled using this generation of SCCs and the role of humans in the control ‘loop’. The SE-TC2 was installed in the TELECOM 2 SCC in February 1993. It was decided to install it for a ten-month period before putting it into operation. We describe why this period was chosen and how we are using it to best advantage. A major aspect in the system's design is its ability to improve the knowledge added by the users themselves, so we are able to focus on this feature, which is vital with respect to the system's relevance.</abstract>
	<keywords>Knowledge-based system; Model-based raisoning; Satellite Control Center</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=237>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A use of case-based reasoning technique in building expert systems</paper_heading>
	<authors>Dinh Phuoc Vo, David Macchion</authors>
	<abstract>Matra Marconi Space France and Aramiihs (Action de Recherche et Application Matra Irit en Interaction Homme Système) laboratory have used and evaluated Case Based Reasoning (CBR) techniques in two projects: • - The first project is about the development of a system dedicated to help satellites AIT/AIV (Assembly Integration and Test/Validation) test engineers to cope with incidents occurring during test activities. The project is funded by the EGSE System Section of ESTEC (European Space Research and Technology Centre.). • - The second project is related to the building of a knowledge-based system for diagnosis assistance in AIT/AIV activities of Ariane4 Vehicle Equipment Bay (VEB). The project is financed by internal funding of MMS-F. In the two projects, CBR technique is neither used the same way nor with the same purpose. In the first project, CBR technique is used to find out or suggest the cause of an anomaly when an incident appears. Confronted with the occurrence of an incident, the system will refer to its characteristics (test context, symptoms…) that are considered as relevant to retrieve previous similar incidents. In the second project, CBR technique is combined with Rule Based Reasoning and Model Based Reasoning ones to form the reasoning core of a Hybrid Knowledge Based System. When an incident occurs, the system proposes to test engineers a diagnosis approach based on the combination of different knowledge (coded into rule, cases and models). Aramiihs is a research unit where engineers from MMS and researchers from the IRIT (Institut de Recherche en Informatique de Toulouse) CNRS (Centre National de la Recherche Scientifique) collaborate on problems concerning new types of man-system interaction.</abstract>
	<keywords>Case Based Reasoning; Hybrid systems; Ariane4 test; Satellite test</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=238>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>QUATRAIN — A design support tool and a data processing sequence supervisor</paper_heading>
	<authors>François D'Heygère, Pierre Mariot, Jean-Baptiste Renard</authors>
	<abstract>Equipment and systems design is a complex activity involving numerous inter-linked sub-tasks and multiple methods of resolution. However, it is possible to identify a family of activities to formalize where the approaches and the knowledge can be elicitated and, thus, to build new tools embedding the methodological know-how. The expected advantages of such a tool are obvious: reduction of time of design, dissemination of know-how linked to an operation, reinforcement of quality assurance, standardization of the approaches between different teams, etc. In a computerized design support environment, one usually finds data and inter-operating tools (or modules) dedicated to specific tasks. The novelty proposed in this architecture consists in assigning the whole supervision part or the piloting of the activity (sequence of the different design tasks, etc.) to a specific module. This module knows on the one hand, each available tool (or modules) and condition of use (why, with what, how?) and on the other hand, the different standardized procedures validated by experiment allowing to solve certain kinds of problems. To meet these need, Aèrospatiale and Framentec-Cognitech have built the tool called QUATRAINTM, with the following goals: • to develop a generic solution, applicable to the whole applications family of design support; • to develop these process supervision interfaces rapidly, and to offer a methodological structuring framework during the development of the system; • to offer high-level data management functions (taking into account data dependency, management of releases, exchanges between user management), chronological account of events, connection of tasks; • to offer the means for including in the tool: expertise rules, methodological approaches (What to do in case of problem? What are the data to be modified? Which parameter to choose? What are the useless/ mandatory steps?, etc.); • to offer a graphical representation of the design process and of the state of progress through an interactive, easy to set up, easy to use user interface. For this, QUATRAIN uses an original process description language that associates two concepts: • the description of data and of manipulated tasks that statically defines the graph of every operation to apply in order to produce expected results, thus the dependencies between data; • the formalization of ‘methodological’ rules that indicates dynamically the succession of operations, the checking points and the needed interactions according to the aim of the tasks in progress and the intermediate obtained results. The first applications built by Aérospatiale and Framentec-Cognitech and other industrial companies confirm its genericity, its ease of learning, its application rapidity and its capacity to be integrated in varied design environments. - ARIANE4 mission analysis (AEROSPATIALE). - ARIANE4 exploitation of flight data (AEROSPATIALE). - Far-end sky study (AEROSPATIALE). - Calculation of large elements structures (FRAMATOME) - Vehicle architecture pre-analysis (GIAT). - Oilfield modelling (ELF). - Dimensioning of electrical networks study (EDF). - Calculation of nuclear power plant cores (CEA). All these applications have in common a willingness to offer to the users the means of dealing with software tools more and more powerful and more and more complex.</abstract>
	<keywords>Data processing sequence supervisor; Concurrent engineering; Computer-aided design; Software methodology; Declarative description language</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=239>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Issues in practical model-based diagnosis</paper_heading>
	<authors>R.R Bakker, P.C.A van den Bempt, N.J.I Mars, D.-J Out, D.C van Soest</authors>
	<abstract>The model-based diagnosis project at the University of Twente has been directed at improving the practical usefulness of model-based diagnosis. In cooperation with industrial partners, the research addressed the modeling problem and the efficiency problem in model-based reasoning. Main results of this research are that (1) re-use of electronically available descriptions of systems for diagnostic purposes is possible, and (2) efficient reasoning can be realized using PDE, hierarchic models, and a simple diagnostic strategy. We have built a prototype diagnostic system which shows the technical feasibility of model-based diagnosis in a computer system. The project was concluded in July 1993 by the development of the ‘Diagnostic Toolbox’. The Diagnostic Toolbox supports the modeling of technical systems for diagnostic purposes, and it contains several model-based reasoning methods.</abstract>
	<keywords>Model-based diagnosis; Modeling; Computational efficiency; Tools</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=240>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A common framework for reasoning on uncertainty both at symbolic and numerical levels</paper_heading>
	<authors>Thierry D Fualdes, Claude J Barrouil</authors>
	<abstract>A sound cooperation between man and machine consists in leaving the machine deal with semantical low level info, and the man deal with higher level concepts. However, one difficult problem to solve for implementing such an organization is to define the relevant info to exchange. When there exists a decision level that reasons on symbolic data, then there is a similar twofold problem: how to process numerical data to produce symbolic info? How to manage the continuous real time system to match symbolic objectives? The aim of this paper is to present a common framework for dealing both with symbolic and numerical data. An application is proposed for planning and controlling the motion of an autonomous mobile robot in an incompletely known environment. This work is being currently carried out in the framework of the VAP project (Mars rover, partnership with CNES, CEA, CNRS and INRIA within the RISP group), and of the DARDS project (DRET's military surveillance autonomous vehicle). It is also linked with the French hierarchical organization of symbolic and numerical levels program PRC-IA of Artificial Intelligence.</abstract>
	<keywords>Robotics; Active perception; Localization</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=241>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The elements of an open KBS infrastructure</paper_heading>
	<authors>H.N Smith, K.J Poulter</authors>
	<abstract>This paper describes the elements of an open knowledge based system infrastructure. These are: standard knowledge representation systems; a knowledge interchange format; a knowledge manipulation and query language; common shared ontology and an agent based software engineering model. These elements provide the means to achieve Knowledge Sharing and Reuse between heterogeneous knowledge based systems. The elements have been adopted as the basis of the conceptual design for the Advanced Technology Operations System ATOS being designed by the European Space Agency ESA.</abstract>
	<keywords>Open Knowledge Based Systems; Knowledge sharing and reuse; Advanced Technology Operations System ATOS</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=242>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ViVa: A systematic approach to verification, improvement and validation of knowledge-based systems</paper_heading>
	<authors>Erik Hollnagel</authors>
	<abstract>The growing dependency on KBS, and their gradual spread into safety related functions, has made it essential that their validity can be established. The ViVa project tries to solve this problem in a practical way by developing a tool-set and method that will support a user in doing the following: define and follow a suitable KBS life-cycle, select V & V tools appropriate for each stage, and carry out V & V of relevant aspects of the KBS; elicit application specific validation knowledge; investigate and advise on the cause of any noted discrepancies or anomalies; and carry out necessary refinements of the KBS. The paper provides an introduction to ViVa, including initial descriptions of its functionality and architecture.</abstract>
	<keywords>Knowledge-based system; Verification; Validation; Improvement</keywords>
	<publication_month_year>1993-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 9, Issue 4</volumes_issues>
</paper>
<paper no=243>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SYMPATIX: A SIMD computer performing the low and intermediate levels of image processing</paper_heading>
	<authors>T Collette, H Essafi, D Juvin, J Kaiser</authors>
	<abstract>The goal of this project is to improve the performance of the parallel computer SYMPATI2. This SIMD processor based system performs with a good efficiency the low level image processing operations, but this efficiency is drastically cut when handling an intermediate level class of algorithms. This study emphasis the drawbacks encountered to perform such operations. The main one is the interconnection between processors. So, a new interconnection network, called the open intelligent network, is proposed and added to SYMPATI2 to form SYMPATIX. This network detailed below allows irregular transfers of data between the different processing elements of the new system. Furthermore, this network allows the efficient interconnection of specific modules. In this paper, the architecture is evaluated on representative algorithms of image processing. Then, a behavioural model of SYMPATIX is described using a hardware description language, the VHDL. Our SIMD computer efficiency has been considerably upgraded for the low and intermediate levels of image processing. Furthermore, its application area was extended. The last part of the paper describes the performance obtained with simulations.</abstract>
	<keywords>Parallel processing; SIMD; Intermediate level of image processing; Interconnection networks; VHDL system simulation</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=244>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance evaluation of cache architectures in tightly coupled multiprocessor systems</paper_heading>
	<authors>Jean-Marc Kuntz</authors>
	<abstract>By using trace-driven simulation, we have evaluated the performance of several cache architectures of tightly coupled multiprocessor mainframes. Our traces were obtained on a monoprocessor running a transaction processing benchmark. These primary traces were then processed for use with multiprocessor simulations. Our new method for providing multiprocessor traces has been validated by comparison with measurements. One- and two-level cache architectures for 1 to 8 processors have been modelled, in order to determine the miss ratios in caches up to 2 Mb. Transaction processing is a very critical application for multiprocessors and the miss ratios here are about 4 times higher than with scientific applications. In addition, the influence of the coherence protocol on the cache performance is analysed and relative differences of 20 to 30% in the miss ratios are shown.</abstract>
	<keywords>Multiprocessor; Cache; Coherence protocols; Performance evaluation; Trace-driven simulation; Transaction processing; Debit-credit benchmark</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=245>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Architecture of parallel management kernel for PIE64</paper_heading>
	<authors>Yasuo Hidaka, Hanpei Koike, Hidehiko Tanaka</authors>
	<abstract>We describe the architecture of the parallel management kernel for the parallel inference engine PIE64, focusing on how to treat load distribution and scheduling in highly parallel symbolic processing. Since the kernel manages automatic load distribution and scheduling, the task remaining for the programmer is to employ parallel algorithms with sufficient concurrency. A programmer need no longer be concerned with load distribution, load partitioning, load assignment, parallelism explosion, exhaustion of resources, or execution efficiency. We also describe an evaluation of the load distribution method.</abstract>
	<keywords>Parallel operating system; Load distribution; Load partitioning; Scheduling; Parallel inference machine</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=246>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Improving the efficiency of virtual channels with time-dependent selection functions</paper_heading>
	<authors>José Duato</authors>
	<abstract>In previous papers, a new theory for the design of deadlock-free adaptive routing algorithms for wormhole and store-and-forward networks as well as two design methodologies have been proposed. Also, a new adaptive routing algorithm, obtained from the application of the former theory to the binary n-cube, has been evaluated using both, a uniform and an exponential distribution for message destination. The results are good, especially for large networks and a uniform distribution for message destination. When locality is exploited, the results are comparatively worse, mainly due to the reduction in channel bandwidth produced by channel multiplexing. In this paper, we analyse the advantages and disadvantages produced by the use of virtual channels, proposing a new approach to maximize their efficiency. This approach uses time-dependent selection functions, associating a threshold to some virtual channels. Those channels cannot be selected by a message unless it is waiting for longer than the corresponding threshold. The evaluation of the new selection function for the binary n-cube shows an important improvement, especially when locality is exploited.</abstract>
	<keywords>Adaptive routing; Interconnection networks; Multicomputers; Performance evaluation; Selection function; Virtual channels; Wormhole routing</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=247>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programming massively parallel architectures with sequential object oriented languages</paper_heading>
	<authors>J.-M Jézéquel, F Bergheul, F André</authors>
	<abstract>Most parallel object oriented languages (OOL) are currently using a general parallelism model based on communicating sequential processes. This approach makes it difficult to program massively parallel systems in an easy and efficient way. So we propose to use another form of parallelism, known as data parallelism. We describe how it can be integrated in a given OOL in a clean and elegant fashion, using only already existing concepts — i.e. without modifying the OOL syntax and semantics — to exploit the potential power of massively parallel systems. To illustrate our ideas, we present an application of this approach to a well known parallel paradigm, along with experimental performance results.</abstract>
	<keywords>Massively parallel systems; Parallel programming paradigms; Object oriented languages</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=248>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optimal implementation of and-or parallel Prolog</paper_heading>
	<authors>Gopal Gupta, Vítor Santos Costa</authors>
	<abstract>Most models that have been proposed, or implemented, so far for exploiting both or-parallelism and independent and-parallelism have only considered pure logic programs (pure Prolog). We present an abstract model, called the Composition-Tree, for representing and-or parallelism in full Prolog. The Composition-Tree recomputes independent goals to ensure that Prolog semantics is preserved. We combine the idea of Composition-Tree with ideas developed earlier, to develop an abstract execution model that supports full Prolog semantics while at the same time avoiding redundant inferences when computing solutions to (purely) independent and-parallel goals. This is accomplished by sharing solutions of independent goals when they are pure (i.e. have no side-effects or cuts in them). The Binding Array scheme is extended for and-or parallel execution based on this abstract execution model. This extension enables the Binding Array scheme to support or-parallelism in the presence of independent and-parallelism, both when solutions to independent goals are recomputed as well as when they are shared. We show how extra-logical predicates, such as cuts and side-effects, are supported in this model.</abstract>
	<keywords>Parallel logic programming; Or-parallelism; And-parallelism; Goal recomputationGoal reuse; Environment representation; Paged binding arrays</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=249>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the semantics of μ Log</paper_heading>
	<authors>Jean-Marie Jacquet, Koenraad De Bosschere</authors>
	<abstract>The paper aims at a semantic study of the integration of blackboards in logic programming. To that end, a new logic programming framework involving Linda-like primitives is proposed first. It is dedicated to no particular logic language but rather focuses on the key concepts and control operators. As natural consequences, it subsumes existing concrete proposals [2,4,6] and provides a general framework well-suited for their semantic analysis. Five semantics are described and compared. They range in the operational, declarative and denotational types and are issued both from the logic programming and the imperative traditions. They are composed of two operational semantics, describing respectively the success/failure sets, and various failures, of two declarative semantics, extending the classical Herbrand interpretation and immediate consequence operator, and of one denotational semantics, defined compositionally and on the basis of process-like histories. The mathematical tools mainly used are complete lattices and complete metric spaces.</abstract>
	<keywords>Logic programming; Semantics; Concurrency theory; Blackboard; Linda</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=250>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Paragon specifications: Structure, analysis and implementation</paper_heading>
	<authors>Paul Anderson, David Bolton, Paul Kelly</authors>
	<abstract>Paragon is a notation for specifying object behaviours using sets of rewrite rules, where rewriting is controlled by synchronous and asynchronous message passing, and where objects may be dynamically created as a rewriting side-effect. This paper overviews Paragon, and introduces a simple classification scheme for analysis of Paragon specifications. Restrictions on specifications are discussed in consideration of implementation feasibility and efficiency constraints. Implementation schemes based on the analysis and restrictions are defined. In particular, a translation strategy for static systems is detailed and motivated with a worked example. To reinforce the low-level nature of the derived implementation the translation is defined in terms of a digital hardware description language. Schemes for the implementation of general dynamic systems are also considered.</abstract>
	<keywords>Concurrency; Rewriting; Object-oriented programming; Hardware description languages</keywords>
	<publication_month_year>1994-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issue 1</volumes_issues>
</paper>
<paper no=251>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>HPCN in Europe: A personal perspective</paper_heading>
	<authors>D.J Wallace</authors>
	<abstract>This paper gives an overview of the Report of the High Performance Computing and Networking Advisory Committee (HPCN-AC) of the Commission of the European Communities. Of particular concern is the exploitation of the opportunity of the present window for parallel computing. We begin by reviewing the Report in the context of earlier programmes and proposals on the European scale, and comment briefly on the status of the implementation of the recommendations in the light of programmes in the United States and Japan. The views expressed are personal ones; this is not a formal presentation of the HPCN-AC.</abstract>
	<keywords>High Performance Computing; Parallel computing</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=252>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High-performance computing and communications</paper_heading>
	<authors>Rick Stevens</authors>
	<abstract>This paper has two parts. The first part discusses the US High-Performance Computing and Communications program — its goals, funding, progress, revisions, and research in high-performance computing systems, advanced software technology and applications, national research and education networking, information infrastructure technology, and basic research and human resources. The second part of the presentation covers specific work conducted under this program at Argonne National Laboratory. Argonne's efforts focus on computational science research, software tool development, and evaluation of experimental computer architectures. In addition, we describe collaborative activities at Argonne in high-performance computing, including an Argonne/IBM project to evaluate and test IBM's newest parallel computers and the Scalable I/O Initiative being spearheaded by the Concurrent Supercomputing Consortium.</abstract>
	<keywords>High-Performance Computing and Communications (HPCC) program; Computational science; Scalable I/OSPl computer; National Information Infrastructure (NII)</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=253>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>HPCN applications in finance</paper_heading>
	<authors>D.J Parsons</authors>
	<abstract>The business drivers which face all organisations often translate into a set of generic systems/technical infrastructure requirements. The author believes that there is a high degree of correlation between both business and technical requirements and the generally accepted advantages of parallel systems. Examples of applications using parallel technology are listed and early applications of parallel technology in Barclays Bank are described. However, there are still many challenges to be addressed if the promised potential of parallel systems is to be profitably exploited.</abstract>
	<keywords>Applications; Data management; Development tools; Fault tolerance; Information visualisation; Management information; Transaction processing</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=254>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Transport vehicle crash, safety and manufacturing simulation in the perspective of high performance computing and networking</paper_heading>
	<authors>E Haug, J Dubois, J Clinckemaillie, S Vlachoutsis, G Lonsdale</authors>
	<abstract>This paper expresses the need for high performance computation for the ever-increasing demands in precision, speed of turn around and importance of numerical simulation of industrial processes and product performances. This need is illustrated by the examples of crashworthiness simulation, vehicle occupant safety simulation and sheet metal stamping simulation. It is concluded that the penetration of advanced simulation techniques into industry will be accelerated by massively parallel computing techniques.</abstract>
	<keywords>Industrial simulation; Crashworthiness; Occupant safety; Sheet metal stamping; High performance computing; Massive parallel programming</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=255>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Science and industry in HPCN</paper_heading>
	<authors>Pietro Rossi</authors>
	<abstract>The interplay of science and industry in the development and exploitation of a mutually beneficial HPCN policy is a non-issue. Everyone, I believe, is willing to subscribe to this view. Just a tad harder is to outline in detail the precise lines along which such a partnership should evolve and what the advantages are for both partners. CRS4 is a research institute based in Cagliari (Sardegna) with a vocation for mathematical modeling and numerical simulation. Research at CRS4 means applied research in close contact with industry. It would seem that we are occupying a privileged position to entertain thoughts and develop theories and models on the interplay of science and industry in HPCN. Nonetheless our experience is too time-limited to allow me to make any general conclusion on the subject. In my paper I will first speculate on what kind of development we will see in the coming years in the field of High Performance Computing and Networks. I will describe the interaction between CRS4 and industry, and extrapolate from this some of the needs of industry from HPCN. I will close by describing our view of the interaction between a HPCN and CRS4.</abstract>
	<keywords>Applied research; CRS4; Distributed computing; LAN; Mathematical modelling; Parallel computingWAN</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=256>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Direct and large-eddy simulations of turbulence in fluids</paper_heading>
	<authors>F.T.M Nieuwstadt, J.G.M Eggels, R.J.A Janssen, M.B.J.M Pourquié</authors>
	<abstract>As a result of the increasing power of supercomputers numerical simulation of turbulent flows has become feasible. In the present paper we give a short review of the requirements for such simulations. First we discuss so-called direct numerical simulation (DNS), where the equations of motion for a turbulent flow are solved in all detail. This application is illustrated with two examples, viz. the transition from laminar to turbulence in a differential heated cavity and a fully developed turbulent pipe flow. The second application of turbulence simulation is large-eddy modelling where only the large scales of turbulence are numerically resolved and the small scales are parameterized by a turbulence model. As example for this case we discuss the atmospheric boundary layer and in particular the dispersion of pollutants by atmospheric turbulence. We close our contribution with a discussion of the computer requirements necessary for turbulence simulation together with an outlook in the future in which we consider the pro's and contra's of massively parallel systems.</abstract>
	<keywords>Direct numerical simulation; Large-eddy simulation; Turbulence; Computation requirements; Massively parallel systems</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=257>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computation challenges in complex liquids: Entropy-driven phase transitions</paper_heading>
	<authors>Daan Frenkel</authors>
	<abstract>The numerical study of complex fluids offers a challenge to numerical simulation as the systems under consideration are too large to be treated by conventional atomistic models, yet too small to be treated macroscopically. This paper sketches some of the recent trends in the numerical study of phase behavior of complex liquids. The importance of the entropy for the phase-behavior is illustrated.</abstract>
	<keywords>Monte Carlo simulations; Colloids; Polymers; Phase transitions; Liquid crystals; Path-integrals</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=258>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>First-principles calculations on materials using massively parallel computing</paper_heading>
	<authors>M.J Gillan</authors>
	<abstract>Advances in high performance computing are making it possible to calculate the properties of materials accurately and reliably from the fundamental laws of quantum mechanics. A non-technical explanation is given of how the calculations are done, and of how massively parallel computing is already playing an important role in the field. The new possibilities that are opening up are illustrated by recent work on hydrogen in silicon, the simulation of catalytic processes, and the properties of materials under extreme conditions. The key importance of high performance computing for the future development of materials science is indicated.</abstract>
	<keywords>Parallel computing; Computer simulation; Materials; Catalysis; Molecular dynamics; Surfaces; Silicon</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=259>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The SuperJANET project</paper_heading>
	<authors>R Cooper</authors>
	<abstract>Planning for SuperJANET, a national high performance network to support UK research and higher education, commenced in 1989 and resulted in a four year project to deploy the network starting in March 1993. SuperJANET draws on the experience gained from its predecessor, the JANET network, but there are some major differences in the implementation strategy, technical aspects and applications. This paper describes the concepts behind the project, the plans for the network and the applications it will support.</abstract>
	<keywords>SuperJANET; High performance networking; Multi-service networks; Multi-media applications; ATMS; DHS; MDS</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=260>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High performance computing systems: Present and future</paper_heading>
	<authors>R Bisiani</authors>
	<abstract>This paper explores the factors that will influence the process of establishing High Performance Computing machines as widespread, general tools. The paper deals with systems rather than architectures because it is impossible, at least at this stage of knowledge, to ignore the computational model and the software used by an architecture. The paper focuses on general ideas rather than specific systems.</abstract>
	<keywords>Computer architecture; parallel processing</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=261>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Software development environments for massively parallel systems</paper_heading>
	<authors>Gianluigi Castelli</authors>
	<abstract>The commercial offer of massively parallel systems has now reached a wide variety, with well engineered products offered by European and American manufacturers. Furthermore, both the American government and the Commission of the European Communities have clearly identified the important role that High Performance Computing may play in science and industry in the near future. To be successful, HPC needs to improve greatly the quality attributes of its software. This paper investigates the status of the software environments for massively parallel systems. In fact, whilst the hardware technology improvements in the past few years have been very rapid, the technology has suffered because of a substantial misalignment between the potential user's expectations and the actual usability of software for massively parallel computer systems. Actions and serious industrialisation efforts are needed to bring the system software and the basic programming environments to a level of real usability, because it is time for these systems to get rid of the ancestral equation: massive parallel systems = number crunching. Many potential markets could be addressed by massively parallel systems if they were able to offer to the users the same ease of use, quality, completeness, and portability of conventional sequential systems.</abstract>
	<keywords>Compiler; Debugger; Operating system; Architecture Neutral Distribution Format (ANDF); Application Programming Interface (API)</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=262>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Two strategies in parallel computing: Porting existing software versus developing new parallel algorithms — two examples</paper_heading>
	<authors>U Gärtel, W Joppich, A Schüller, H Schwichtenberg, ... G Winter</authors>
	<abstract>Parallel systems have been used mainly in universities and large research institutions up to now. For permanent use in industry and in central service institutions — such as weather centers — the adequate mathematical methods and software are still missing. There is a need to parallelize the large existing production codes, on one hand, and to develop new parallel algorithms, on the other hand. This paper presents two highly efficient parallel codes that may be regarded as breakthroughs in these fields: the weather prediction code IFS, that has been parallelized in a portable way, and ASMG, a new, very fast low communication algorithm for Poisson-like equations that are typically solved in each time step of weather prediction and related models.</abstract>
	<keywords>Parallel weather prediction; IFS; Portability; Parallelization of production codes; Anisotropic sparse multigrid solver; ASMG; Multi-workstations</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=263>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Software developments for industrial research</paper_heading>
	<authors>Maurice Schlumberger</authors>
	<abstract>Industrial research is under the same pressures as most other organisations: it has to deliver results with fewer resources, and in particular in less time. Software based systems are often seen as one way to reach this goal. This paper shows a number of cases and directions where software based systems are currently required, and the approaches that are commonly taken to reach these results, as well as new approaches taken to enhance the productivity of industrial research. This field is fast becoming a computer intensive effort and will stay this way.
</abstract>
	<keywords>Information mining; Simulation; Knowledge engineering; Process support</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=264>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PEI: A simple unifying model to design parallel programs</paper_heading>
	<authors>E Violard, G.-R Perrin</authors>
	<abstract>Many ways have been explored nowadays about parallel programming. Derivation by refinement or transformations according to some computation models are main examples. This justifies to introduce a unifying theory which supposes an abstraction of the different concepts. Our theory is founded on the simple concept of multiset, represented as a data field [7–10].</abstract>
	<keywords>Parallel programming; Transformations; Multisets; Environment</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=265>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Manifold</paper_heading>
	<authors>F Arbab, I Herman</authors>
	<abstract>Manifold is a coordination language for orchestration of the communications among independent, cooperating processes in a massively parallel or distributed application. The fundamental principle underlying Manifold is the complete separation of computation from communication. This means that in Manifold (1) computation processes know noting about their own communication with other processes, and (2) coordinator processes manage the communications among a set of processes, but know noting about the computation they carry out. This principle leads to more flexible software made out of more re-usable components, and supports open systems. This paper is a brief introduction to Manifold.</abstract>
	<keywords>Parallel computing; Coordination languages; MIMD; Models of communication</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=266>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>LOCCS: Low overhead communication and computation subroutines</paper_heading>
	<authors>F Desprez, B Tourancheau</authors>
	<abstract>Our aim is to provide one set of efficient basic subroutines for scientific computing which include both communications and computations. The overlap of communications and computations is done using asynchronous pipelining to minimize the overhead due to communications. With this set of routines, we provide to the user of parallel machines an easy SPMD type and efficient way of programming. The main purpose of these routines is to be used in linear algebra applications but also in other fields like image processing or neural networks.</abstract>
	<keywords>Communication library; Overlap; Pipeline; Global communications</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=267>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GRIDS — A programming system for grid-based technical and scientific applications on parallel systems</paper_heading>
	<authors>U Geuder, M Härdtner, B Wörner, R Zink</authors>
	<abstract>The introduction of parallel computers is substantially slowed down by the difficulties of writing parallel programs. Fully automatic general parallelization is not feasible in the near future. The GRIDS programming system addresses this problem by offering an easy-to-use programming model for grid-based scientific computations. The code is parallelized automatically and executed efficiently by the GRIDS runtime system.</abstract>
	<keywords>Scientific computation; Automatic parallelization</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=268>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulation of parallel systems: PSEE (Parallel System Evaluation Environment)</paper_heading>
	<authors>Emilio Luque, Remo Suppi, Joan Sorribes</authors>
	<abstract>We have developed an interactive environment, PSEE (Parallel System Evaluation Environment) to study the behaviour of linked oriented parallel systems. It allows parallel programming simulation and performance evaluation of parallel algorithms in parallel architectures. PSEE permits to manage the main characteristic parameters involved in the system to show the tuning grade of the measurement, visualization and modification of parallel system parameters.</abstract>
	<keywords>Simulation of parallel computing; Performance evaluation; Visualization; Programing tools</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=269>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programming environment for a transputer based computer</paper_heading>
	<authors>E Luque, M.A Senar, D Franco, P Hernández, ... J.C Moure</authors>
	<abstract>Despite the availability of parallel computing from the last two decades, there is little use of these systems in production-level environments. One of the factors most commonly blamed for the slow transition to parallelism is the lack of software support. While in serial programming the performance depends basically on the algorithm designed by the user, in parallel programming there are many machine-dependent aspects that have a significant impact on the final performance. Therefore, the user must learn a great deal about machine-dependent aspects like process grain determination, task allocation, message routing, etc. The aim of this project is intended in the design and implementation of a user friendly environment for parallel programming in a Transputer-based system. The environment will free the user of all machine-dependent aspects by means of two system services running on a host computer and a distributed kernel running on the target computer. On one hand, the two system services consist of one tool that is responsible for obtaining clusters of program tasks, and another tool that is responsible for mapping those clusters onto physical processors. On the other hand, the environment has a distributed kernel running on the target machine that hides the physical architecture. It executes user tasks with location transparency, offering a simple and efficient interface for interprocess communication, and monitoring the execution of the user programme. As a consequence, user productivity will be increased because he does not need to be aware of those aspects that are carried out automatically by the environment.</abstract>
	<keywords>Parallel programming environment; Distributed kernel; Processor allocation problem; Clustering problem</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=270>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scheduling of parallel programs including dynamic loops</paper_heading>
	<authors>E Luque, A Ripoll, T Margalef, A Cortés</authors>
	<abstract>The classical static scheduling strategies do not consider the dynamic behavior of parallel programs. These strategies provide near-optimal solutions for the scheduling problem when the programs are well-known beforehand. However, the schedulings provided can be quite inefficient when the conditions of a real execution are not those assumed to obtain the scheduling. On the other hand, some of the structures with a dynamic behavior included in parallel programs include an intrinsic parallelism (e.g., Do-All loops), that cannot be exploited by the classical scheduling policies. To avoid these situations, the authors developed a methodology based on Task Replication that keeps the processors as busy as possible and takes advantage of the intrinsic parallelism of parallel programs. This methodology provides static schedulings, with some replicated tasks, that take into account the dynamic behavior of parallel programs including loops with a variable number of iterations and conditional structures.</abstract>
	<keywords>Parallel programming; Scheduling; Dynamic loops; Task replication</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=271>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The CAMAS workbench: Computer Aided Migration of Applications System</paper_heading>
	<authors>J.F de Ronde, P.M.A Sloot, M Beemster, L.O Hertzberger</authors>
	<abstract>A simulation methodology for predicting the performance of sequential programs as well as data parallel (SPMD) programs on parallel and distributed platforms is presented. The proposed methodology comprises a parameterised description of the applications as well as the target machines. An integrated simulation procedure estimates the time complexity of the (SPMD) application given a well-defined hardware platform and predicts the execution behaviour. The methodology has been actualised in terms of a toolset currently under development at the University of Amsterdam.</abstract>
	<keywords>Performance estimation; Massively parallel platforms; SPMD</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=272>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel multigrid code with a fast vectorized ILU-relaxation</paper_heading>
	<authors>Margreet Louter-Noo</authors>
	<abstract>In this paper we focus on MGD1M a multigrid method which uses Incomplete LU-relaxation (ILU) as a smoothing process. A straightforward ILU implementation leads to non-vectorizable recursive relations. By applying an alternative ordering of the grid points, the ILU vectorizes and the overall execution time is reduced considerably. Parallelism has been introduced by applying auto- and macrotasking.</abstract>
	<keywords>Elliptic PDEs;ILU-relaxation; Multigrid methods; Numerical software; Parallel computers; Sparse linear systems; Vector computers</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=273>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Boosting the performance of the linear algebra part in an ODE solver for shared memory systems</paper_heading>
	<authors>Kitty Potma, Walter Hoffmann</authors>
	<abstract>The purpose of this paper is to present an algorithm for solving stiff ordinary differential equations on a parallel system with shared memory. The algorithm we use is based on a parallel implicit Runge-Kutta method described by Van der Houwen and Sommeijer. To improve the performance, a matrix decomposition technique proposed by Enright, which is based on similarity transformations to Hessenberg form, is incorporated. A theoretical performance model of the algorithm is presented. Results are reported of our algorithm that was implemented in Fortran on a shared memory system.</abstract>
	<keywords>Hessenberg matrices; Ordinary differential equations; Parallelism; Runge-Kutta methods; Shared memory systems; Similarity transformations</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=274>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Solving dense linear systems by Gauss-Huard's method on a distributed memory system</paper_heading>
	<authors>Walter Hoffmann, Kitty Potma, Gera Pronk</authors>
	<abstract>A variant of Gaussian elimination that is known as Gauss-Huard's algorithm behaves like Gauss-Jordan's algorithm in the fact that it also reduces the matrix to diagonal form and it behaves like LU factorisation in the fact that it uses the same number of floating-point operations and has practically the same numerical stability. This contribution presents a block-variant of Gauss-Huard's algorithm with favourable data locality.</abstract>
	<keywords>Gaussian elimination; Gauss-Huard; Linear systems; Parallel algorithms</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=275>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Vectorization and parallelization of a multi-block Navier-Stokes flow solver on different computer architectures</paper_heading>
	<authors>P.A van Mourik</authors>
	<abstract>First, the vectorization of a (Computational Fluid Dynamics) flow solver is described which has been performed on the NEC SX-3/22. Performance results and a comparison of the CRAY Y-MP, the CRAY C-90 and the NEC SX-3 are given. Second, autotasking results are given for the computer architectures just mentioned. Finally the strategy for the parallelization of the flow solver from May 1993 onwards is outlined.</abstract>
	<keywords>Vectorization; Parallelization; Computational Fluid Dynamics; Performance results</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=276>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards first-principles theories of materials and biological systems — The need for massive parallelism</paper_heading>
	<authors>L.T Wille, J.L Rogers, C.P Burmester, R Gronsky</authors>
	<abstract>The first-principles paradigm, i.e. the calculation of a physical, chemical, or biological system's properties without including any fitted or adjustable parameters, is one of the most powerful in the scientist's quest to understand nature. Unfortunately most first-principles computations involve quite complicated governing equations and large system sizes, both of which have a detrimental effect on memory size and computer time requirements. Using two relatively simple yet realistic models, one in materials science and one in biophysics, it is discussed here how massive parallelism can make such studies feasible.</abstract>
	<keywords>Massively parallel computing; SIMD computers; Materials science; Biophysics; Parallel Monte Carlo simulation; Chaos</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=277>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Magnetohydrodynamic spectroscopy: Large scale computation of the spectrum of waves in plasmas</paper_heading>
	<authors>J.P Goedbloed, S Poedts, G.T.A Huysmans, G Halberstadt, ... A.J.C Beliën</authors>
	<abstract>A systematic study of the spectrum of magnetohydrodynamic (MHD) waves in thermonuclear and astrophysical plasmas is presently undertaken at the FOM Institute for Plasma Physics. This study is based on the fact that the macroscopic waves and instabilities in both thermonuclear confinement machines and in astrophysical magnetic flux loops are described by the MHD equations. The linearised interaction between plasma and magnetic field leads to large-scale non-symmetric eigenvalue problems for free oscillations and to temporal evolution problems for forced (externally driven) oscillations. These problems have been cast in the form of a set of two modular computer programs, called CASTOR for tokamaks and POLLUX for coronal flux tubes, exploiting identical solvers for the resulting very large systems. CASTOR is extensively used at JET (the Joint European Torus experiment at Culham) for the interpretation of measured MHD spectra, whereas POLLUX is presently used for the study of Alfvén wave heating of the solar corona and will be used in the near future for the interpretation of observed x-ray emissions from the SOHO satellite which will be launched by ESA and NASA in 1995. For the interpretation of actual plasma dynamics in tokamaks and solar coronal loops it is crucial to understand the non-linear phase of the evolution of MHD phenomena as well. This adds two complications: non-linear mode coupling of hundreds of small-scale modes and enormous discrepancies between the different dynamical time scales. We have developed numerical codes that simulate the non-linear temporal evolution of plasma columns under external excitation. These codes have been modelled after the linear ones, with a finite difference discretisation in the radial direction and a spectral discretisation in the two other directions. A relatively simple and flexible semi-implicit predictor-corrector scheme has been used for the time stepping. This allows time steps up to a factor 1000 larger than the largest time steps allowed for explicit methods. Because of the many modes involved, memory requirements and CPU time requirements soon become prohibitive. Hence, the new physics in the non-linear plasma dynamics regime requires parallel computing.</abstract>
	<keywords>Large eigenvalue systems; Non-linear dynamics; Parallel computing</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=278>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Direct numerical simulations of turbulent flow in a driven cavity</paper_heading>
	<authors>R Verstappen, J.G Wissink, W Cazemier, A.E.P Veldman</authors>
	<abstract>Direct numerical simulations (DNS) of 2 and 3D turbulent flows in a lid-driven cavity have been performed. DNS are numerical solutions of the unsteady (here: incompressible) Navier-Stokes equations that compute the evolution of all dynamically significant scales of motion. In view of the large computing resources needed for DNS cost-effective and accurate numerical methods are to be selected. Here, various-order accurate spatial discretization methods for DNS have been evaluated by applying them to the 2D driven cavity at Re = 22,000. To analyze the results of the DNS of the 2D flow in a driven cavity at Re = 22,000 the proper orthogonal decomposition (POD) technique has been applied. POD is an unbiased method to determine coherent structures. The Galerkin projection of the Navier-Stokes equations on the space spanned by the POD-basis-functions yields a relatively low-dimensional set of ordinary differential equations that mimics the dynamics of the Navier-Stokes equations. 3D DNS with no-slip conditions at all walls of the cavity have been performed at both Re = 3,200 and Re = 10,000. The results reproduce the experimentally observed Taylor-Görtler-like vortices.</abstract>
	<keywords>Direct Numerical Simulation; Turbulence; Navier-Stokes equations; Driven cavity; Numerical methods; Proper orthogonal decomposition</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=279>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Large scale natural vision simulations</paper_heading>
	<authors>T Lourens, N Petkov, P Kruizinga</authors>
	<abstract>A computationally intensive approach to pattern recognition in images is developed and applied to face recognition. Similarly to previous work, we compute functional inner products of a two-dimensional input signal (image) with a set of two-dimensional Gabor functions which fit the receptive fields of simple cells in the primary visual cortex of mammals. The proposed model includes nonlinearities, such as thresholding, orientation competition and lateral inhibition. The output of the model is a set of cortical images each of which contains only edge lines of a particular orientation in a particular light-to-dark transition direction. In this way the information of the original image is split into different channels. The cortical images are used to compute a lower-dimension space representation for object recognition. The method was implemented on the Connection Machine CM-51 and achieved a recognition rate of 97% when applied to a large database of face images.</abstract>
	<keywords>Gabor filters; Face recognition; Computer vision</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=280>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Crystallization on a sphere</paper_heading>
	<authors>J.M Voogd, P.M.A Sloot, R van Dantzig</authors>
	<abstract>We present a parallel simulated annealing algorithm for crystallization of N-body systems. The implementation uses systolic simulated annealing with additional parallel cost function evaluation. We have a hybrid topology which consists of a systolic ring and attached to each ring processor a tree of processors. Our research topic is crystallization of particles on a spherical surface. Some results of the crystallization experiments are presented.</abstract>
	<keywords>Parallel simulated annealing; Hybrid topology; Crystallization</keywords>
	<publication_month_year>1994-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 10, Issues 2–3</volumes_issues>
</paper>
<paper no=281>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Rarefied gas flow computational with a 3D unstructured mesh on a Connection Machine (CM2)</paper_heading>
	<authors>François Coron, Philippe Homsi</authors>
	<abstract>The implementation on a Connection Machine (CM2) of a Monte Carlo method for rarefied gas flow simulation is presented. The physical model and the algorithm are described. Because of the architecture of the CM2, the association between processors and data is very important. Two different mappings are used in order to obtain the best performance for the two different parts of the algorithm. Moreover, the displacement of a particle in a three-dimensional unstructured mesh requires a number of operations which depends on the particle and which is not a priori known. When the computation is finished for a great number of particles, it is better to work on a smaller subset in order to reduce the CPU cost even if it introduces some extra communication cost. A complex three-dimensional case is presented; the performances obtained on a 16 K configuration are comparable to those obtained on a CRAY XMP.</abstract>
	<keywords>Aerodynamic; Rarefied gas; Monte Carlo method; Massively parallel computers; Connection Machine</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=282>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>FEM/FVM calculations of compressible flows on a Meiko system</paper_heading>
	<authors>Mark Loriot, Loula Fezoui</authors>
	<abstract>We describe in this paper a strategy for solving the compressible Navier-Stokes system using unstructured meshes on parallel distributed memory machines.</abstract>
	<keywords>Compressible CFD; Finite-element/finite-volume formulation on unstructured meshes; MUSCL schemes; SPMD parallelisation with message-passing; Mesh decomposition; Parallel distributed memory systems</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=283>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Benchmarking the POMPC compiler on the Connection Machine CM-2</paper_heading>
	<authors>Thierry Porcher</authors>
	<abstract>In this paper we present the benchmarking of the POMPC compiler on the CM-2. Using a large subset of the BECAUSE benchmarks, we compare the performances of the POMPC compiler with results obtained with native CM-2 compilers: C∗, PARIS and slicewise versions of CM Fortran.</abstract>
	<keywords>POMPC compiler; Connection Machine CM-2</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=284>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Unstructured CFD computations on the KSR-1: Preliminary results</paper_heading>
	<authors>Stéphane Lanteri, Charbel Farhat</authors>
	<abstract>We report on our preliminary results for unstructured CFD computations on the KSR-1 parallel processor. While this machine features a virtual shared memory environment, it has been our experience that high performance results for unstructured computations are attainable only when using a message passing paradigm.</abstract>
	<keywords>CFD computations; KSR-1 parallel processor; Message passing paradigm</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=285>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Strategies for the implementation of the BBS communication tests on the supernode and their implications for efficiency</paper_heading>
	<authors>A. Peakall, G. Robertson</authors>
	<abstract>In this paper we describe our experience in pursuing two implementation strategies for the BBS communication benchmarks on the Parsys SN1000 SuperNode architecture. We detail the benchmarks themselves and explain the way in which they measure various aspects of communication performance. We give an account of the effort involved in porting the benchmarks to a model of parallelism noticeably different from that in which they were conceived. We present the results of the two strategies, derive some performance factors and discuss the issues and trade offs that arise in the comparison of the two strategies. Additionally, we describe how conducting the benchmarks lead to improvements to the SN1000 operating software through the evolution of a portable design of parallel application loader.</abstract>
	<keywords>Benchmarking; Parallelism; Programming environments; Communications</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=286>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>BBS results for the iPSC/2 and iPSC/860</paper_heading>
	<authors>R.F. Fowler, B.W. Henderson, C. Greenough</authors>
	<abstract>We present results for the BECAUSE Benchmark Set (BBS) on two machines, the Intel iPSC/2 and iPSC/860. These computers are of similar design and architecture, both being MIMD machines with processors connected together in a hypercube topology and supporting efficient through routing of messages. The software environment on the two machines is also very similar with support for a range of basic communication operations. The main difference between the machines lies in the processor used on each node, which on the iPSC/2 is an Intel 386 (plus FPU) while the iPSC/860 uses a state of the art i860 chip. It is shown that the basic floating point performance of the iPSC/2 is no longer competitive with current workstations such as the IBM RS6000/320. On the other hand, a single node of iPSC/860 is found to give comparable performance to such workstations. The communication performance of the machines for long messages is very similar (2.8 Mbytes/sec.), which is reasonable for the iPSC/2 but is found to present problems on the iPSC/860 in some of the benchmark tests. Results for other aspects of communication performance including overlap with computation and access to the concurrent file system are also presented. The scalability of the performance when using multiple nodes on both machines is examined for the Class 2 and 3 BBS tests and comparisons made with the reference machine. Some aspects of the implementations used for the parallel versions of the tests are discussed.</abstract>
	<keywords>Parallel benchmarking; Parallel processing; iPSC/2iPSC/860; BECAUSE benchmark set</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=287>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Porting a three-dimensional semiconductor device modelling program to the Intel iPSC/860 hypercube</paper_heading>
	<authors>R.F. Fowler, B.W. Henderson, C. Greenough</authors>
	<abstract>EVEREST is a three-dimensional device modelling package used to study the electrical behaviour of semiconductor devices. A set of partial differential equations describing the current flow within the device is solved using a mixed mesh of tetrahedral and hexahedral elements. The highly non-linear nature of the equations requires that a flexible solution strategy be used to make the software robust and efficient. Within the solution process large sparse non-symmetric linear systems are solved using iterative solvers such as CGS with preconditioning. Run times of many hours or even days are required for realistic devices on current workstations. EVEREST consists of over 100 000 lines of Fortran and was developed for scalar architectures. The main aim of this work was to investigate how such an application could be adapted to run on the iPSC/860. A review of some of the possible parallelisation techniques is made. Most techniques for the parallel implementation of such mesh based calculations on MIMD machines involve a partitioning of the mesh. An extension of an algorithm due to Farhat has been implemented in a pre-processor to partition the device meshes used by EVEREST. Aspects of load balancing and the need to minimise the number of interface nodes in the decomposition are discussed. Some initial results using the partitioned mesh to perform the system matrix assembly are presented. Methods that could be used to implement a parallel preconditioned CGS solver on the iPSC/860 are reviewed and some performance estimates made using the results of certain BBS tests on this machine.</abstract>
	<keywords>Parallel processing; iPSC/860; Semiconductor device simulation; CGS; ICCG; Mesh partitioning</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=288>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>First attempt to parallelise a CFD application software package: The CALIFE code</paper_heading>
	<authors>P. Steinfeld, L. Lequette, E. Znaty</authors>
	<abstract>In this paper, a first attempt to parallelise the monospecies, inviscid compressible option of a CFD application software package called CALIFE is presented. After having briefly described the equations solved and the numerical method used in CALIFE, a critical analysis to identify all the parts requiring communication is performed. For each identified Critical Part, some possible options are reviewed and a selection is done with justification of the choice. In particular, the special requirements put by the parallelisation on the Pre-Processing phase are detailed in depth. Initial obtained results using the Domain Decomposition Method (DDM) without overlapping are presented. A discussion of the work to be performed before having a parallel Code CALIFE ready for use on real industrial problems concludes the paper.</abstract>
	<keywords>BECAUSE project; Computational Fluid Dynamics; CALIFE; Finite element method</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=289>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Benchmark of application software kernels on the SUPERNODE SN1000 using the 3P PARLIB</paper_heading>
	<authors>R. Cornubert, G. Gruez, P. Steinfeld, E. Znaty</authors>
	<abstract>This article presents the benchmarking by BERTIN (F) of the SUPERNODE SN1000 parallel architecture from PARSYS within the framework of the BECAUSE Project. This evaluation of a Distributed Memory parallel architecture was realised by means of the BECAUSE Benchmark Set (BBS). The very strong idea was to specify parallelisation methodologies and to develop parallel software which are machine independent and as such portable. This approach was possible and realistic since the principle of parallelism which is involved is the Data Parallel Programming. As a consequence, the hardware features of the target architecture are transparent to the industrial user and are managed through a communication library called 3P PARLIB. In this paper, principles of parallelisation which were used are presented. Practical implementation of these parallelisation principles is illustrated with various significant Test Programs from the BBS. The corresponding results are presented. Specifications for the 3P PARLIB (Portable parallel programming library) are also given.</abstract>
	<keywords>BECAUSE Project; Benchmarking; Distributed Memory; MIMD Parallel Architectures; Data Parallel Programming; Data Mapping; Domain Decomposition Method (DDM)</keywords>
	<publication_month_year>1995-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 1</volumes_issues>
</paper>
<paper no=290>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel version of the phase space evolution model</paper_heading>
	<authors>Jack Janssen, Hai Xiang Lin</authors>
	<abstract>The feasibility of a parallel implementation of the Phase Space Evolution model for electron beam dose calculations is investigated. A parallel version of this model is obtained by means of domain decomposition in combination with the single program multiple data paradigm. It has been shown that load balancing is the most important issue in the parallelization of the model. The results show that a parallel version of the Phase Space Evolution model with a linear speedup of about 0.5 times the number of processors is feasible.</abstract>
	<keywords>Electron transport model; Parallel simulation; Domain decomposition</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=291>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Induced flocculation of casein micelles: A Brownian Dynamics simulation on the Parsytec GCel MPP</paper_heading>
	<authors>J.H.J. van Opheusden, M.T.A. Bos</authors>
	<abstract>We perform a Brownian Dynamics simulation of Lennard-Jones particles at low density and temperature. From an initially homogeneous but random configuration the system flocculates into a gel. We study the fractal properties of the gel formed and compare with gels of casein micelles as studied by confocal scanning laser microscopy. In this paper we focus on the aspects of porting the existing code for the simulation program to the Parsytec GCel Massive Parallel Processing computer.</abstract>
	<keywords>Molecular simulation; Fractal structure; Parallel processing; Distributed memory</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=292>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using the GCel for simulation of flow in the continental shelf region</paper_heading>
	<authors>Mark R.T. Roest, Edwin A.H. Vollebregt</authors>
	<abstract>Accurate predictions of flows and transport in the continental shelf region can be obtained using a numerical simulation of the three dimensional shallow water equations. As more powerful computers become available, the grids that are used in these simulations can be much finer, making the results more reliable. This article describes a first implementation of existing simulation software on the GCel computer of SARA in Amsterdam. The modification of the algorithms is described and issues related to load balancing, communication and data distribution will be discussed. The results of the first experiments with this implementation show that the expected performance can be reached.</abstract>
	<keywords>Parallelisation; Incompressible flow simulation</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=293>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation of a cell-vertex FV code for turbulent transonic flows on a Meiko computing surface</paper_heading>
	<authors>D. Golby, M.A. Leschziner</authors>
	<abstract>A parallel, domain-decomposed version of a finite-volume scheme for the calculation of turbulent transonic flow has been developed and implemented on a T-800 Meiko Computing Surface. The code combines a cell-vertex-storage arrangement of flow variables with a Lax-Wendroff time-marching scheme which advances the solution to the steady state. Approaches to dealing with the extensive data dependencies arising from the above discretisation method are considered, and related implementation issues are discussed. Parallel speed-up and efficiency levels achieved with a tile-decomposition strategy are reported for inviscid and turbulent transonic flow over a bump in a two-dimensional channel. Efficiency values are in the range 75–98%, depending on problem type, mesh size and granularity of the decomposed domain.</abstract>
	<keywords>Domain-decomposition; Turbulent-transonic flow; Shock-boundary-layer interaction; Cell-vertex scheme; Lax-Wendroff time-marching; Turbulence-transport models; Transputers;Meiko computing surface</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=294>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel nodal method of second order</paper_heading>
	<authors>Sebti Bousri</authors>
	<abstract>A nodal method of the so-called analytical physical type, using partial currents as variables, is applied to the multigroup diffusion equations of nuclear reactor theory in two-dimensional rectangular geometry. To solve the associated criticity problem, the power method is used for the outer iteration, and the Gauss-Seidel iteration for the inner iterations in the sense that each node uses the most recently estimated incoming partial currents from neighbouring nodes. As method of parallelization a 2-d grid of processors is chosen. The results obtained for the problem considered here show that for fine meshes, high speed-ups and efficiencies may be achieved with MIMD distributed memory architectures like the Parsytec GCEL machine.</abstract>
	<keywords>Multigroup diffusion equations; Nodal methods; Partial currents; MIMD architectures</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=295>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The conjugate gradient method on the Parsytec GCel-3/512</paper_heading>
	<authors>Lianne G.C. Crone</authors>
	<abstract>In this paper we present an implementation method for the conjugate gradient algorithm with geometric parallelization, also called domain decomposition. The results of some experiments on the Parsytec GCel are presented and we discuss further improvements for the implementation of CG on the Parsytec.</abstract>
	<keywords>Conjugate Gradient algorithm; Distributed systems; Parsytec GCel</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=296>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation of parallel block preconditionings on a transputer-based multiprocessor</paper_heading>
	<authors>Magolu monga-Made</authors>
	<abstract>In this paper we are concerned with the implementation on distributed memory MIMD computers of parallel block preconditionings for solving large and sparse symmetric positive definite linear systems. For this purpose, a recently proposed ordering strategy is used. Experimental tests performed on a transputer-based multiprocessor computer are reported, displaying that good performance can be achieved in exchange for some optimization effort and on to the assumption that the number of processors is smaller by orders of magnitude than the size of the problem to solve, which agrees with observations made elsewhere.</abstract>
	<keywords>Stieltjes matrices; Block incomplete factorizations; Preconditioned conjugate gradient; Distributed memory MIMD architectures</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=297>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel 2-opt algorithm for the Traveling Salesman Problem</paper_heading>
	<authors>M.G.A. Verhoeven, E.H.L. Aarts, P.C.J. Swinkels</authors>
	<abstract>We present a scalable parallel local search algorithm based on data parallelism. The concept of distributed neighborhood structures is introduced, and applied to the Traveling Salesman Problem (TSP). Our parallel local search algorithm finds the same quality solutions as the classical 2-opt algorithm and has a good speed-up. The algorithm is implemented on a Parsytec GCel, consisting of 512 transputers. Its performance is empirically analyzed for TSP instances with several thousands of cities.</abstract>
	<keywords>Local search; Traveling Salesman Problem; Data parallelism</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=298>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Embedding a ‘Treecode’ on a MIMD parallel computer using a domain decomposition paradigm</paper_heading>
	<authors>Gavin J. Pringle</authors>
	<abstract>‘Treecodes’ are hierarchical, recursive algorithms which reduce the computational time for the N-body problem from O(N2), by the direct summation method, to O(N log N) or O(N). This paper contains a description of Treecodes followed by an overview of one Treecode in particular, namely the Greengard-Rokhlin Fast Multipole Method in 2 dimensions. The parallel version of this method is then discussed with the presentation of two communication strategies; a systolic loop and a routine which enables neighbouring transputers to swap information in a predetermined sequence. The computer used is the Meiko Computing Surface, a MIMD distributed memory computer employing T800 transputers, running the CSTools communication harness using a SPMD (Single Program, Multiple Data), local domain decomposition paradigm. The 3D version of the parallel algorithm has also been implemented. In addition, Multigrid methods require the same parallel communication strategies presented here.</abstract>
	<keywords>N-body problem; Multipole method; SPMD; Meiko; CSTools</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=299>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Monitoring and modelling tools for high performance database systems</paper_heading>
	<authors>Paul Allen, Iain Cramb, Colin Upstill</authors>
	<abstract>We have produced a suite of software tools which has proved to be highly valuable in the porting of large, complex relational database applications from conventional to parallel systems. These tools are being continually developed to extend their functionality and genericity over a range of platforms, and provide a basis for the design, implementation, optimisation and management of any parallel database system. A considerable amount of the development has been performed using a Meiko Relational DataCache — a multi-spare, multi-transputer ORACLE system. In this paper we present a summary of our activities, beginning with a brief description of the Meiko system (as currently configured), followed by a more detailed account of some of the tools, and finishing with some general comments on future work.</abstract>
	<keywords>Parallel databases; ORACLE; Sun-Sparc performance monitoring; Database modelling; Database load synthesis; MeikoTeradata</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=300>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PDG: A process-level debugger for concurrent programs in the GRAPE parallel programming environment</paper_heading>
	<authors>Chris Caerts, Rudy Lauwereins, J.A. Peperstraete</authors>
	<abstract>In this paper, we describe the process-level debugger of GRAPE, our hierarchical graphical programming environment for concurrent programs. Its unique feature is that it clearly separates the identification of erroneous processes, which we call process-level debugging, from the exact localisation of the bug at the source-level. This divide-and-conquer approach is absolutely necessary for debugging complex parallel programs in a fast and systematic way. Our process-level debugging approach is based on an animation of the program's behaviour on its hierarchical graphical representations. Graphical views are used that reflect the programmer's mental picture of the actual application. Hierarchy allows us to employ a top-down debugging approach in which we successively refine the search-space by zooming in on suspect processes first-time-right. During animation a debugging kernel implementing a record-replay mechanism guarantees reproducible behaviour.</abstract>
	<keywords>Process-level debugging; Record-replay; Program animation; Data visualization</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=301>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>CHIMP and PUL: Support for portable parallel computing</paper_heading>
	<authors>R.A.A. Bruce, S. Chapple, N.B. MacDonald, A.S. Trew, S. Trewin</authors>
	<abstract>The lack of a portable programming interface for parallel computers has inhibited the development of applications for such systems, and thus restricted the exploitation of this technology. In this paper we discuss various efforts to provide a portable interface to parallel computers, and present the Edinburgh CHIMP (Common High-level Interface to Message-Passing), and PUL (Parallel Utilities Library) activities in this context. The concepts behind these projects are described, with details of the range of parallel systems on which they are available. An important consequence of the portability offered by the CHIMP message passing layer is the ability for applications programmers to reuse entire programs, as well as code modules. PUL assists this by providing a set of reusable components for implementing parallel applications. Developed on top of CHIMP, the PUL utilities are themselves portable across a range of platforms. Finally, this paper summarises experience using CHIMP and PUL to implement parallel applications in collaboration with both industrial and academic groups.</abstract>
	<keywords>Message passing; Parallel libraries; Portability; Reusability; Scalability</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=302>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance prediction of large MIMD systems for parallel neural network simulations</paper_heading>
	<authors>Louis Vuurpijl, Theo Schouten, Jan Vytopil</authors>
	<abstract>In this paper, we present a performance prediction model for indicating the performance range of MIMD parallel processor systems for neural network simulations. The model expresses the total execution time of a simulation as a function of the execution times of a small number of kernel functions, which have to be measured on only one processor and one physical communication link. The functions depend on the type of neural network, its geometry, decomposition and the connection structure of the MIMD machine. Using the model, the execution time, speedup, scalability and efficiency of large MIMD systems can be predicted. The model is validated quantitatively by applying it to two popular neural networks, backpropagation and the Kohonen self-organizing feature map, decomposed on a GCel-512, a 512 transputer system. Measurements are taken from network simulations decomposed via dataset and network decomposition techniques. Agreement of the model with the measurements is within 1–14%. Estimates are given for the performances that can be expected for the new T9000 transputer systems. The presented method can also be used for other application areas such as image processing.</abstract>
	<keywords>Performance prediction; Parallel neural network simulations; MIMD transputer systems</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=303>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Comparing distributed memory and virtual shared memory parallel programming models</paper_heading>
	<authors>J.A. Keane, A.J. Grant, M.Q. Xu</authors>
	<abstract>The virtues of the shared memory and distributed memory parallel programming models have been much debated. Conventionally the debate could be reduced to programming convenience on the one hand, and high scalability factors on the other. More recently the debate has become somewhat blurred with the provision of virtual shared memory models built on machines with physically distributed memory. The intension of such models/machines is to provide scalable shared memory, i.e. to provide both programmer convenience and high scalability. In this paper, the different models are considered from experiences gained with a number of systems ranging from applications in both commerce and science to languages and operating systems. Case studies are introduced as appropriate.</abstract>
	<keywords>Shared memory; Distributed memory; Programming convenience; Scalability factors; Virtual Shared Memory</keywords>
	<publication_month_year>1995-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 2</volumes_issues>
</paper>
<paper no=304>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation and evaluation of update-based cache protocols under relaxed memory consistency models</paper_heading>
	<authors>Håkan Grahn, Per Stenström, Michel Dubois</authors>
	<abstract>Invalidation-based cache coherence protocols have been extensively studied in the context of large-scale shared-memory multiprocessors. Under a relaxed memory consistency model, most of the write latency can be hidden whereas cache misses still incur a severe performance problem. By contrast, update-based protocols have a potential to reduce both write and read penalties under relaxed memory consistency models because coherence misses can be completely eliminated. The purpose of this paper is to compare update- and invalidation-based protocols for their ability to reduce or hide memory access latencies and for their ease of implementation under relaxed memory consistency models. Based on a detailed simulation study, we find that write-update protocols augmented with simple competitive mechanisms — we call such protocols competitive-update protocols — can hide all the write latency and cut the read penalty by as much as 46% at the cost of some increase in the memory traffic. However, as compared to write-invalidate, update-based protocols require more aggressive memory consistency models and more local buffering in the second-level cache to be effective. In addition, their increased number of global writes may cause increased synchronization overhead in applications with high contention for critical sections.</abstract>
	<keywords>Shared-memory multiprocessor; Write-update cache coherence protocols; Relaxed memory consistency models; Lockup-free cache designPerformance evaluation</keywords>
	<publication_month_year>1995-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 3</volumes_issues>
</paper>
<paper no=305>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>CTDNet III-An eager reduction model with laziness features</paper_heading>
	<authors>Padam Kumar, J.P. Gupta, S.C. Winter</authors>
	<abstract>A message passing multiprocessor model for computation based on functional languages has been suggested. The model follows the applicative (eager) order of reduction, giving it an edge in exploiting parallelism over those following normal order. However, the applicative strategy is prone to being unsafe and is incapable of handling recursion. A concept of Partial task has been introduced whose sluggishness in reduction is utilised to achieve a controlled version of recursion. The model has certain other features which make it selectively lazy so that the reduction tends to be more need-based and hence more safe.</abstract>
	<keywords>Functional programming; Lazy evaluation; Multiprocessing; Reduction machines; Supercombinators</keywords>
	<publication_month_year>1995-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 3</volumes_issues>
</paper>
<paper no=306>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Probabilistic parallel programming based on multiset transformation</paper_heading>
	<authors>V.K. Murthy, E.V. Krishnamurthy</authors>
	<abstract>The GAMMA parallel programming model is based on the multiset datastructure. Here, a succession of chemical reactions consume the elements of the multiset and produce new elements according to specific rules, This paper extends GAMMA model to its probabilistic version — called P-GAMMA model to realise evolutionary computations — namely probabilistic, classifier, bucket-brigade learning and genetic algorithms. We also explain how to support evolutionary computations through randomized choices and concurrent transformations on persistent globally accessible tuplespaces using query processing and transaction mechanisms.</abstract>
	<keywords>Parallel programming; Multiset transformation; GAMMA</keywords>
	<publication_month_year>1995-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 3</volumes_issues>
</paper>
<paper no=307>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A model for the parallel execution of subset-equational languages</paper_heading>
	<authors>Amos R. Omondi, David A. Plaisted</authors>
	<abstract>This paper describes a model for the parallel execution of subset-equational languages, a class of functional languages that allow constrained nondeterminism. The fundamentals of subset-equational languages are presented, followed by the formulation of a kernel language that is used as the basis of the model. The languages are based on nondeterministic term-rewriting systems; the model uses dataflow execution graphs and is suitable for implementation on both conventional and unconventional machines.</abstract>
	<keywords>Parallel processing; Subset-equation language; Nondeterminism; Dataflow graphs; Term-rewriting; Functional languages</keywords>
	<publication_month_year>1995-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 3</volumes_issues>
</paper>
<paper no=308>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Order statistics on a linear array with a reconfigurable bus</paper_heading>
	<authors>Yi Pan</authors>
	<abstract>The order statistics problem is considered in this paper. We present a parallel algorithm to find the smallest (or largest) kth element in a set of N totally ordered (but not sorted) data items. This algorithm runs in O(log2 N) expected time on a reconfigurable linear array with N processors and a constant amount of memory space in each processor. We also show that this algorithm can be generalized to process an oversized order statistics problem efficiently.</abstract>
	<keywords>Complexity; Linear array; Parallel algorithm; Reconfigurable bus; Selection</keywords>
	<publication_month_year>1995-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 3</volumes_issues>
</paper>
<paper no=309>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Contributions of FGCS technology to applications in legal reasoning</paper_heading>
	<authors>Marek Sergot</authors>
	<abstract>The Japanese Fifth Generation Computer Systems (FGCS) Project identified legal reasoning as one of the benchmark applications designed to demonstrate its newly developed technology. This paper reviews the FGCS work on legal reasoning, with the aim of assessing (1) its contribution to the field of automated legal reasoning, and (2) its contribution to the wider aims of the FGCS project, which may be summarised as the development of new technologies intended to open up important new classes of applications. A secondary aim of the paper is to provide a summary of some of the main strands of research that have been carried out in the field of Artificial Intelligence and law in recent years.</abstract>
	<keywords>FGCS project; Legal reasoning; Legal expert systems; Case-based reasoning; PIM machine</keywords>
	<publication_month_year>1995-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 3</volumes_issues>
</paper>
<paper no=310>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Trapper: A graphical programming environment for parallel systems</paper_heading>
	<authors>Lorenz Schäfers, Christian Scheidler, Ottmar Krämer-Fuhrmann</authors>
	<abstract>We present a graphical programming environment for parallel systems called TRAPPER™. TRAPPER supports the development of industrial applications which require high computing power. The programming environment is based on the programming model of communicating sequential processes. TRAPPER contains tools for the design, mapping, visualization and optimization of parallel systems. The Designtool supports a hybrid program development, where the parallel process structure is described using a graphical representation and the sequential behavior is described using textual representations. The configuration of the target hardware and the mapping of the application onto the configured hardware is supported by the Configtool. During run-time, the monitoring system records software events like interprocess communication and measures the computation and communication loads of the underlying hardware. The run-time behavior of the software is animated by the Vistool, the load behavior of the hardware is displayed by the Perftool. The first target systems are transputer-based systems as well as workstation clusters and parallel supercomputers with the standard interfaces PVM, PARMACS or MPI.</abstract>
	<keywords>Programming environments; Transputer; Cluster computing; Industrial applications; Monitoring; Performance optimization</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=311>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel application design: The simulation approach with HASTE</paper_heading>
	<authors>P. Pouzet, J. Paris, V. Jorrand</authors>
	<abstract>It is important for the development of an industrial parallel application to have estimates of the execution time and cycle time of the application from early stages of its conception. This paper presents HASTE, a tool built to simulate the behavior of an application in its design form, in terms of real time predictions for a particular target machine (transputers in the current version). HASTE (HAmlet Simulator Tool (E)) is a discrete event simulator implemented by a linkable library for programs written in parallel C. The modelling of the application design is based on functions for synchronization, communication and process creation. An event trace is generated for the analysis of the simulated execution. A version of HASTE has been provided to the parallel application developers of the HAMLET project. They have been using it for a few months and have provided some feedback which is used to develop the next version of the simulator.</abstract>
	<keywords>Parallel application; Execution time; Cycle time; Real time predictions</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=312>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On-line distributed debugging on scalable multiprocessor architectures</paper_heading>
	<authors>Thomas Bemmerl, Roland Wismüller</authors>
	<abstract>Debugging parallel programs is one of the most tedious jobs in programming scalable multiprocessor architectures. Due to the distributed resources of these machines, programming is often architecture dependent. Most development tools still reflect this dependency even during the analysis phase of parallel programs. This paper presents the distributed debugger DETOP, which offers a global name space and hides architectural features like the mapping of processes. DETOP is part of the integrated tool environment TOPSYS implemented on iPSC hypercubes, networks of SPARCstations and partly on Transputer and PowerPC based systems.</abstract>
	<keywords>Debugging; Parallel computers; Software monitoring; Hardware monitoring; Distributed event detection; Predicate transformation</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=313>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual shared memory programming on workstation clusters</paper_heading>
	<authors>Jörg-Thomas Pfenning, Achim Bachem, Ronald Minnich</authors>
	<abstract>Workstation clusters have recently attracted high interest as a technology providing supercomputer class performance at much lower price levels. Today the message passing programming model dominates the application development, despite the overhead and the complexity introduced by the explicitly coded data transfers. We give an introduction to the virtual shared memory programming model and report on the experiences with the MNFS system. We show that shared memory with weak coherency can perform competitive to message passing and provide an excellent tool for parallelizing programs using complex data structures with dynamic load balancing.</abstract>
	<keywords>Parallel computing; Workstation cluster; Virtual Shared Memory</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=314>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High Performance Fortran Languages: Advanced applications and their implementation</paper_heading>
	<authors>Barbara Chapman, Piyush Mehrotra, Hans Zima</authors>
	<abstract>High Performance Fortran Languages such as Vienna Fortran and High Performance Fortran (HPF) allow the programming of massively parallel machines at a relatively high level of abstraction, based on a user-specified distribution of data across the processors of the machine. In this paper we examine the parallelization of two advanced applications — which require irregular data and work distributions — in Vienna Fortran and identify the reasons why the current version of HPF does not provide adequate functionality for formulating efficient parallel versions of these codes.</abstract>
	<keywords>Massively parallel machines; Fortran language; Data parallel languages; Data distribution; Numerical computation</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=315>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A perspective of HPCN requirements in the European Aerospace Industry</paper_heading>
	<authors>J.A. Murphy</authors>
	<abstract>Product complexity is increasing rapidly to address the demanding requirements driven by customers, regulations and safety. More complex designs and an increasing number of design options must be evaluated whilst controlling the cost and length of the design cycle. The engineering requirements come from different disciplines, such as aerodynamics, structures and electromagnetics. The division of engineering into separate disciplines is not recognised by nature, with the result that addressing requirements within one discipline can compromise requirements in other disciplines. Product complexity is now such that requirements from multiple engineering disciplines need to be simultaneously addressed at an early stage of the design cycle to avoid costly re-design. These problems have placed strong demands on computational design, driving key trends in numerically intensive modelling, as well as the requirement for more computational resource. The requirement for more computational resource arises in many disciplines and is compounded by the need for multi-disciplinary design. This results from the increased product complexity which has to be matched by increasing the mathematical complexity of the model providing the underlying framework for the computational design tools. The availability of increased computational resource via parallel platforms at affordable prices enables these problems to be addressed facilitating. • computational analyses of more complex problems. • simultaneous computational analyses of multiple design and planning options. • multi-disciplinary analyses, i.e. simultaneously address requirements in different disciplines. The continual growth in computational design and computational resource provides the potential for the end-user engineer to carry out far more design more rapidly and accurately. However this potential can only be exploited if the end-user can efficiently define more complex problems, multiple problems, multiisciplinary problems and also analyse the large volume of results. Consequently considerable effort needs to be placed on the user environment, encompassing problem set-up and results analysis. This paper expands on these requirements within the aerospace and related sectors with examples of applications from specific disciplines. Some emphasis is attached to the growing supercomputer disciplines, computational electromagnetics in particular. Some key features of parallel processing are discussed including. • the high degree of parallelism required to get a modest percentage of peak performance. • exploitation of parallel platforms without Teraflop performance • the potential benefits of hybrid or heterogeneous platforms compared to homogeneous platforms. These are key issues affecting the exploitation of parallel platforms and the success of parallel processing in simulation and design in general. The presentation includes examples on colour viewfoils that are not included in the paper.</abstract>
	<keywords>Design cycles; Product complexity; Computational modelling; Numerical algorithms; Parallel processing; Grid generation</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=316>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Perspectives of collaborative supercomputing and networking in European Aerospace research and industry</paper_heading>
	<authors>Ulrich Lang, J.P. Peltier, Paul Christ, Stefan Rill, ... Peter Haas</authors>
	<abstract>For some years European aerospace industry and research pursue the idea of an ‘BASNET’ —a European Aeronautical Supercomputing Network. Within this context, the project RACE 2031 PAGEIN ‘Pilot Applications in a Gigabit European Integrated Network’ collected a major part of European aerospace research, some manufacturers and two universities. In the following paper on industry oriented activities the benefits of the new network based concepts and working modes are evaluated with new industrial partners. PAGEIN's application environment comprises elements such as supercomputers of different architectures, simulations in computational fluid dynamics (CFD), scientific visualization and cooperative working scenarios including voice and video —to be demonstrated in a high-speed network environment. In the following the content of multiple presentations given during the PAGEIN session of the HPCN'94 conference are combined (see [1–6]). Also an update is given on the state of the project.</abstract>
	<keywords>High performance computing; Aerospace; Networking; Collaborative working; Visualization; Simulation</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=317>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulation of metal forming processes with respect to MPP-systems</paper_heading>
	<authors>M. Hillmann, J. Weiher</authors>
	<abstract>This paper describes the activities of INPRO in the field of metal forming simulation. INPRO, as a subsidiary of Daimler-Benz AG, Volkswagen AG, Krupp Hoesch Stahl AG, Hoechst AG, Siemens AG and Voest-Alpine Stahl Ges. mbH. is concerned with advanced manufacturing technology innovations. A short description of different approaches to sheet metal simulation is given. At INPRO the special purpose program INDEED (INovative DEEp Drawing) has been developed. Although INDEED is highly optimized for well established hardware-platforms, industrial demands require a further reduction of CPU-time. The announcement of MPP-Systems rises the hope to dramatically speed up the numerical simulation. At present, INPRO investigates how much benefit could be obtained by using MPP-Systems for the simulation of forming processes. The question arises, how much redesign and development of new algorithms are necessary to obtain a reasonable gain of performance.</abstract>
	<keywords>Metal forming; Simulation; Automotive industry; Domain decomposition method</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=318>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Electrosmog & electromagnetic CAD</paper_heading>
	<authors>Wilfried Daehn</authors>
	<abstract>Microelectronics has penetrated all areas of public life. This penetration has resulted in an increased exposure of man and machines to electromagnetic fields that are produced by other electronic equipment and which are summarized under the term electrosmog. On the other hand the progress in microelectronics has also given birth to supercomputers with GFLOPS of computing performance and Megabytes of main memory. They are the hardware basis for finite difference time domains and other space grid solvers for Maxwell's equations. Government regulations concerning electromagnetic compatibility and the public's increased awareness of electrosmog have emerged a new interest in the computation of electromagnetic fields and their impact on human life.</abstract>
	<keywords>Electrosmog; Mobile telephones; Finite difference time domain simulation; Human interaction</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=319>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel computing in quantum chemistry — Message passing and beyond for a general ab initio program system</paper_heading>
	<authors>Hans Lischka, Holger Dachsel, Ron Shepard, Robert J. Harrison</authors>
	<abstract>The parallelization of the diagonalization step of the COLUMBUS MRSDCI program system is reported. A coarse grain algorithm has been developed by means of a segmentation of the trial and resulting update vectors of the iterative Davidson scheme. Message passing based on the TCGMSG toolkit and the global array (GA) tools are used. The latter program system allows an asynchronous access to data structures in the spirit of shared memory. The importance of portable facilities like GA going beyond message passing is stressed for quantum chemical methods and benchmark result for the Intel Touchstone Delta are given.</abstract>
	<keywords>Parallel computing; Quantum chemistry; Message passing; Global arrays</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=320>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Biologically motivated computationally intensive approaches to image pattern recognition</paper_heading>
	<authors>Nikolay Petkov</authors>
	<abstract>This paper presents some of the research activities of the research group in vision as a grand challenge problem whose solution is estimated to need the power of Tflop/s computers and for which computational methods have yet to be developed. The concerned approaches are biologically motivated, in that we try to mimic and use mechanisms employed by natural vision systems, more specifically the visual system of primates. Visual information representations which are motivated by the function of the primary visual cortex, more specifically by the function of so-called simple cells, are computed. Three different methods for using such representations to solve image pattern recognition problems are presented. These are: (i) extraction and comparison of lower-dimension representations, (ii) computing optimal mappings of an image onto other images by optic flow techniques and (iii) application of a self-organising neural network classifier. The problems of automatic recognition and classification of visual patterns, in particular the discrimination of human faces, are used to test the usefulness and feasibility of these approaches.</abstract>
	<keywords>Vision; Primary visual cortex; Simple cells; 2D Gabor representations; Lower-dimension representations; Optic flowMulti-scale matching; Artificial neural networks; Self-organisation; Kohonen networks; Parallel computing</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=321>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Comparison of vector and parallel implementations of the simulated annealing algorithm</paper_heading>
	<authors>J.M. Voogd, P.M.A. Sloot, R.v. Dantzig</authors>
	<abstract>In this paper we describe a vector and a parallel implementation of a stochastic simulation method to solve optimization problems in the field of many particle systems. We use a case-study where the (energetically) optimal distribution of particles on a closed surface is studied. Crystallization on a closed surface is an interesting sub-domain since such a topology causes lattice defects. To obtain the optimal distribution of particles on a sphere we use the simulated annealing algorithm. Simulated annealing is an application of the Markov chain simulation method which, in principle, guarantees that the minimum in energy of our system of particles is found. However, the time for the algorithm to converge increases rapidly with system size. In order to find the best performing implementation we have made vectorized and parallelized implementations. We parallelize the simulated annealing method in several ways. Here we use two types of parallelization in conjunction, a systolic decomposition of the Markov chains and a functional decomposition of the energy calculations. The sequential nature of the simulated annealing algorithm is hard to parallelize and is therefore an important research topic to study the functional differences between parallel and sequential implementations. Results show that the parallelization influences the accuracy of the iterative process. In this paper we give a comparison between the vectorized and the parallelized implementation. It is shown that the current parallel implementation on the Parsytec GC parallel transputer platform is not capable of outruning our vector implementation on the CRAY Y-MP.</abstract>
	<keywords>Parallel; Vector; Optimization</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=322>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Building a virtual machine-room — a focal point in metacomputing</paper_heading>
	<authors>Friedhelm Ramme</authors>
	<abstract>Computing resources which are transparently available to the user via networked environments are commonly called a metacomputer. In this sense, a metacomputer is a network of heterogeneous, computational resources linked by software in such a way that they can be used as easily as a single computational unit. During the last few years our work has been concentrated on developing methods and tools to provide a transparent and vendor independent hardware management system to the users. Solving this problem up to a high abstraction level will bring the idea of metacomputing a large step closer to its fruition. After reviewing the metacomputing approaches in Europe and the States, we will break down the task force into almost independent units. One of these, the resource access and allocation problem, the project Computing Center Software was focused on. This paper takes a closer look at CCS. Its underlying model which uses abstract views for specifying system components and the general purpose Resource Description Language will be sketched. We will explain how it is possible to support Wide-Area Network access and unstable connection lines. Afterwards, we will present the system and vendor independent batch processing facility usable for arbitrary programming environments. On-going activities and an enhancement of the CCS methodology to solve a core problem in wide-area metacomputing will conclude this paper.</abstract>
	<keywords>Distributed parallel computing; Resource and access management; Metacomputing; Heterogeneous system integration</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=323>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A step towards large scale parallelism: Building a parallel computing environment from heterogenous resources</paper_heading>
	<authors>Kimmo Koski</authors>
	<abstract>MPP industry has recently suffered from economical problems: various companies have disappeared from the market. Traditional vector system vendors, such as IBM, Cray and Convex, have entered the marketplace with their new systems based on standard RISC-technology. Competition and risks in massively parallel processing have increased. The real breakthrough of the parallel systems has not yet happened. Lack of software and tools makes the transition to MPP slow. In order to minimize the risks, a careful approach is to train the user base first with the existing computer resources, which generally include some RISC processors or clusters. A set of tools is available for the user, such as PVM. Pilot projects in order to promote MPP programming are necessary. In a heterogeneous environment, a special attention should be given in load balancing and efficient usage of the resources. The Center for Scientific Computing (CSC) in Finland has been running moderately parallel shared memory systems from the end of 1980s and additionally PVM clusters and IBM SP2 distributed memory system for the last few years in order to prepare for large scale parallelism. In the end of 1994 CSC made a decision to purchase a 192-processor Cray's future generation MPP system, which will be installed during 1996. The selection of the parallel tools is large and everyone of them cannot be supported. In order to concentrate the efforts, choice of technology has to be made. The next few years will demonstrate the abilities of the maturing compiler technology of the parallel systems. One choice of supported software tools and assumption of the compiler technology development have been presented in this paper.</abstract>
	<keywords>Metacomputing; MPP; Cluster; Load balancing; PVM;CSC</keywords>
	<publication_month_year>1995-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issues 4–5</volumes_issues>
</paper>
<paper no=324>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Thread prioritization: A thread scheduling mechanism for multiple-context parallel processors</paper_heading>
	<authors>Stuart Fiske, William J. Dally</authors>
	<abstract>Multiple-context processors provide register resources that allow rapid context switching between several threads as a means of tolerating long communication and synchronization latencies. When scheduling threads on such a processor, we must first decide which threads should have their state loaded into the multiple contexts, and second, which loaded thread is to execute instructions at any given time. In this paper we show that both decisions are important, and that incorrect choices can lead to serious performance degradation. We propose thread prioritization as a means of guiding both levels of scheduling. Each thread has a priority that can change dynamically, and that the scheduler uses to allocate as many computation resources as possible to critical threads. We briefly describe its implementation, and we show simulation performance results for a number of simple benchmarks in which synchronization performance is critical.</abstract>
	<keywords>Multiple-contexts; Multithreading; Priority scheduling; Scheduling; Synchronization</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=325>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Software assistance for data caches</paper_heading>
	<authors>O. Temam, N. Drach</authors>
	<abstract>Hardware and software cache optimizations are active fields of research, that have yielded powerful but occasionally complex designs and algorithms. The purpose of this paper is to investigate the performance of combined through simple software and hardware optimizations. Because current caches provide little flexibility for exploiting temporal and spatial locality, two hardware modifications are proposed to support these two kinds of locality. Spatial locality is exploited by using large virtual cache lines which do not exhibit the performance flaws of large physical cache lines. Temporal locality is exploited by minimizing cache pollution with a bypass mechanism that still allows to exploit spatial locality. Subsequently, it is shown that simple software informations on the spatial/temporal locality of array references, as provided by current data locality optimization algorithms, can be used to increase cache performance significantly. The performance and design tradeoffs of the proposed mechanisms are discussed, Software-assisted caches are also shown to provide a very convenient support for further enhancement of data locality optimizations.</abstract>
	<keywords>Software-assisted caches; Data locality; Numerical codes</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=326>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Two techniques for improving performance on bus-based multiprocessors</paper_heading>
	<authors>Craig Anderson, Jean-Loup Baer</authors>
	<abstract>In this paper, we explore two techniques for reducing memory latency in bus-based multiprocessors. The first one, designed for sector caches, is a snoopy cache coherence protocol that uses a large transfer block to take advantage of spatial locality, while using a small coherence block (called a subblock) to avoid false sharing. The second technique is read snarfing (or read broadcasting), in which all caches can acquire data transmitted in response to a read request to update invalid blocks in their own cache. We evaluated the two techniques by simulating 6 applications that exhibit a variety of reference patterns. We compared the performance of the new protocol against that of the Illinois protocol with both small and large block sizes and found that it was effective in reducing memory latency and providing more consistent, good results than the Illinois protocol with a given line size. Read snarfing also improved performance mostly for protocols that use large line sizes.</abstract>
	<keywords>Processor performance; Speculative and out-of-order execution; Memory latency; Prefetching; Relaxed models of memory consistency; Communication and synchronization</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=327>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An argument for simple COMA</paper_heading>
	<authors>Ashley Saulsbury, Tim Wilkinson, John Carter, Anders Landin</authors>
	<abstract>We present design details and some initial performance results of a novel scalable shared memory multiprocessor architecture. This architecture features the automatic data migration and replication capabilities of cache-only memory architecture (COMA) machines, without the accompanying hardware complexity. A software layer manages cache space allocation at a page-granularity — similarly to distributed virtual shared memory (DVSM) systems —leaving simpler hardware to maintain shared memory coherence at a cache line granularity. By reducing the hardware complexity, the machine cost and development time are reduced. We call the resulting hybrid hardware and software multiprocessor architecture Simple COMA. Preliminary results indicate that the performance of Simple COMA is comparable to that of more complex contemporary all-hardware designs.</abstract>
	<keywords>Multiprocessor architecture; Shared-memory; Cache architecture; Cache coherence protocols; Shared memory performance; Scalability</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=328>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An initial evaluation of the Convex SPP-1000 for earth and space science applications</paper_heading>
	<authors>Thomas Lawrence Sterling, Daniel F. Savarese, Phillip R. Merkey, Jeffrey P. Gardner</authors>
	<abstract>The Convex SPP-1000 is the most recent of the new generation of Scalable Parallel Computing systems being offered commercially. The SPP-1000 is distinguished by incorporating the first commercial version of directory based cache coherence mechanisms and the emerging Scalable Coherent Interface protocol to achieve a true global shared memory capability. Pairs of HP PA-RISC processors are combined in clusters of 8 processors using a cross-bar switch. Up to 16 clusters are interconnected using 4 ring networks in parallel with a distributed global cache. To evaluate this new system in a Beta test environment, the Goddard Space Flight Center conducted three classes of operational experiments with an emphasis on applications related to Earth and space science. A cluster was tested as a platform for executing a multiple program workload exploiting job-stream level parallelism. Synthetic programs were run to measure overhead costs of barrier, fork-join, and message passing synchronization primitives. A key problem for Earth and space science studies is gravitational N-body simulation of solar systems to galactic clusters. An efficient tree-code version of this problem was run to reveal scaling properties of the system and to measure the overall efficiency. This paper presents the experimental results and findings of this study and provides the earliest published evaluation of this new scalable architecture.</abstract>
	<keywords>Convex Exemplar; SPP-1000; Shared memory; Cache coherence; SCI; Performance evaluation</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=329>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fast barrier synchronization in wormhole k-ary n-cube networks with multidestination worms</paper_heading>
	<authors>Dhabaleswar K. Panda</authors>
	<abstract>This paper presents a new approach to implement fast barrier synchronization in wormhole k-ary n-cubes. The novelty lies in using multidestination messages instead of the traditional single destination messages. Two different multidestination worm types, gather and broadcasting, are introduced to implement the report and wake-up phases of barrier synchronization, respectively. Algorithms for complete and arbitrary set barrier synchronization are presented using these new worms. It is shown that complete barrier synchronization in a k-ary n-cube system with e-cube routing can be implemented with 2n communication start-ups as compared to 2nlog2k start-ups needed with unicast-based message passing. This leads to an asymptotic improvement by a factor of log2k. Simulation results for different system and architectural parameters indicate that the new framework can reduce barrier synchronization cost considerably compared to the unicast-based scheme. For arbitrary set barrier, an interesting trend is observed where the synchronization cost keeps on reducing beyond a certain number of participating nodes. The framework demonstrates potential for supporting fast barrier synchronization in large wormhole-routed systems.</abstract>
	<keywords>Barrier synchronization; Collective communication; Wormhole routing; Virtual cut-through; Broadcasting, Multicasting, Meshes, k-ary n-cubes; Path-based Routing</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=330>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Origin-based fault-tolerant routing in the mesh</paper_heading>
	<authors>Ran Libeskind-Hadas, Eli Brandt</authors>
	<abstract>The ability to tolerate faults is critical in multicomputer employing large numbers of processors. This paper describes a class of fault-tolerant routing algorithms for n-dimensional meshes that can tolerate large numbers of faults without using virtual channels. We show that these routing algorithms prevent livelock and deadlock while remaining highly adaptive.</abstract>
	<keywords>Multicomputer; Mesh topology; Wormhole routing; Adaptive routing; Fault-Tolerance</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=331>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Architectural support for inter-stream communication in an MSIMD system</paper_heading>
	<authors>Vivek Garg, David E. Schimmel</authors>
	<abstract>This paper considers hardware support for the exploitation of control parallelism on data parallel architectures. It is well known that data parallel algorithms may also possess control parallel structure. However, the splitting of control leads to data dependency and synchronization issues that were implicitly handled in conventional SIMD architectures. These include synchronization of access to scalar and parallel variables, and synchronization for parallel communication operations. We propose a sharing mechanism for scalar variables and identify a strategy which allows synchronization of scalar variables between multiple streams. The techniques considered are based on a bit-interleaved register file structure which allows fast copy between register sets. Hardware cost estimates and timing analyses are provided, and comparison with an alternate scheme is presented. The register file structure has been designed and simulated for the HP 0.8μm CMOS process, and circuit simulation indicates access times are less than six nanoseconds. In addition, the impact of this structure on system performance is also studied.</abstract>
	<keywords>Superscalar SIMD; MSIMD; Control parallelism; Synchronization; Interleaved Register File</keywords>
	<publication_month_year>1995-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 11, Issue 6</volumes_issues>
</paper>
<paper no=332>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The requirements of a high performance implementation of PVM</paper_heading>
	<authors>Massimo Bernaschi</authors>
	<abstract>PVM (Parallel Virtual Machine) is currently a de facto standard in the world of distributed computing based on message passing [10]. PV Me is a proprietary realization of the PVM programming model which maintains the full compliance with the original programming interface, but it is specially tuned to obtain higher performances on the SP2, the IBM parallel system based on the POWER 2 architecture and the AIX operating system. During the design and the development of PVMe we had a unique chance to gain clear indications about the aspects of the run-time system support more critical to achieve good results running applications by means of PVM. Hereafter we report about our experience and the (hopefully) useful lesson we learnt.</abstract>
	<keywords>Programming model; Communication protocols; System overhead; Memory management techniques</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=333>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Extending PVM to a massively parallel architecture</paper_heading>
	<authors>A. Ciampolini, C. Stefanelli</authors>
	<abstract>Heterogeneous computing is one of the key issues for reaching high performances with limited costs. In this area, PVM is a wide-spread programming environment supported by a collection of architectures, ranging from Unix workstations to various kinds of parallel machines. We have enriched this set of architectures with the Meiko Computing Surface, a multicomputer architecture based on the transputer technology. The paper describes how the porting has been implemented. The resulting environment allows a user to distribute any application partly on ‘traditional’ PVM nodes, and partly on transputer ones. The advantages deriving from the inclusion of the Meiko Computing Surface in the PVM environment are particularly evident with fine-grained parallel applications: as an example, we show the experimental results obtained for a computational vision application.</abstract>
	<keywords>PVM; Transputer; Porting; Heterogeneous computing</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=334>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel I/O for distributed systems: Issues and implementation</paper_heading>
	<authors>V.S. Sunderam, Steven A. Moyer</authors>
	<abstract>Parallel and distributed computing have matured sufficiently for their adoption in production environments, consequently necessitating effective, robust, and efficient frameworks for input and output. A number of concurrent I/O initiatives have evolved in response to these needs, some system specific, others proposing an abstract framework and portable interface. These parallel and distributed I/O efforts focus either on characterizing file-access patterns, language extensions, runtime libraries, or low level primitives. Important systems issues in each approach, technical details of various strategies, and their effect on performance are analyzed. The PIOUS system, a transport independent, scalable I/O framework for parallel and distributed systems is presented, and experiences with its use are reported.</abstract>
	<keywords>Parallel distributed input/output; Scalable transactions; PVM</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=335>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Job- and resource-management systems in heterogeneous clusters</paper_heading>
	<authors>F. Ferstl</authors>
	<abstract>This paper gives an overview about the state-of-the-art in job management systems and about future challenges for such load distribution facilities. A brief history of the evolution of job management systems is followed by a definition of typical features and application areas for such systems. The example of the system Codine is used to outline a suitable architectural approach. Current implementations providing support for serial and parallel batch jobs with and without checkpointing and for interactive jobs are compared. The paper then discusses future requirements for job management software such as application programmer's interfaces (APIs), cluster-wide resource control, interfaces to system management frameworks, complex interdependent job nets, interfaces to mixed shared and distributed memory parallel environments as well as usage in the wide area network (WAN) context.</abstract>
	<keywords>Scheduling; BatchBatch jobs; Parallel jobs; Job management; Resource management; Resource control; Load distribution; Load levelling; Workload management; Workload distribution; Parallel environment; Management framework; Resource management system; Codine</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=336>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A worldwide flock of Condors: Load sharing among workstation clusters</paper_heading>
	<authors>D.H.J. Epema, M. Livny, R. van Dantzig, X. Evers, J. Pruyne</authors>
	<abstract>Condor is a distributed batch system for sharing the workload of compute-intensive jobs in a pool of unix workstations connected by a network. In such a Condor pool, idle machines are spotted by Condor and allocated to queued jobs, thus putting otherwise unutilized capacity to efficient use. When institutions owning Condor pools cooperate, they may wish to exploit the joint capacity of their pools in a similar way. So the need arises to extend the Condor load-sharing and protection mechanisms beyond the boundaries of Condor pools, or in other words, to create a flock of Condors. Such a flock may include Condor pools connected by local-area networks as well as by wide-area networks. In this paper we describe the design and implementation of a distributed, layered Condor flocking mechanism. The main concept in this design is the Gateway Machine that represents in each pool idle machines from other pools in the flock and allows job transfers across pool boundaries. Our flocking design is transparent to the workstation owners, to the users, and to Condor itself. We also discuss our experiences with an intercontinental Condor flock.</abstract>
	<keywords>Distributed processing; Batch queueing system; Wide-area load sharing; Ownership rights; Flocking</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=337>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Interfacing Condor and PVM to harness the cycles of workstation clusters</paper_heading>
	<authors>Jim Pruyne, Miron Livny</authors>
	<abstract>A continuing challenge to the scientific research and engineering communities is how to fully utilize computational hardware. In particular, the proliferation of clusters of high performance workstations has become an increasingly attractive source of compute power. Developments to take advantage of this environment have previously focused primarily on managing the resources, or on providing interfaces so that a number of machines can be used in parallel to solve large problems. Both approaches are desirable, and indeed should be complementary. Unfortunately, the resource management and parallel processing systems are usually developed by independent groups, and they usually do not interact well together. To bridge this gap, we have developed a framework for interfacing these two sorts of systems. Using this framework, we have interfaced PVM, a popular system for parallel programming with Condor, a powerful resource management system. This combined system is operational, and we have made further developments to provide a single coherent environment.</abstract>
	<keywords>Resource management; Parallel programming; Distributed processing; PVM</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=338>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>MARS—A framework for minimizing the job execution time in a metacomputing environment</paper_heading>
	<authors>Jörn Gehring, Alexander Reinefeld</authors>
	<abstract>Utilizing a collection of workstations and supercomputers in a metacomputing environment does not only offer an enormous amount of computing power, but also raises new problems. The true potential of WAN-based distributed computing can only be exploited if the application-to-architecture mapping reflects the different processor speeds, network performances and the application's communication characteristics. In this paper, we present the Metacomputer Adaptive Runtime System (MARS), a framework for minimizing the execution time of distributed applications on a WAN metacomputer. Work-load balancing and task migration is based on dynamic information on the processor load and network performance. Moreover, MARS uses accumulated statistical data on previous execution runs of the same application to derive an improved task-to-process mapping. Migration decisions are based on: (1) the current system load; (2) the network load; and (3) previously obtained application-specific characteristics. Our current implementation supports C applications with MPI message passing calls, but the general framework is also applicable to other programing environments like PVM, PARMACS and Express.</abstract>
	<keywords>Metacomputing; Dynamic load balancing; Job migration; Networked computing platforms; Message passing; Portability; MPI</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=339>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A dynamic load balancing system for parallel cluster computing</paper_heading>
	<authors>B.J. Overeinder, P.M.A. Sloot, R.N. Heederik, L.O. Hertzberger</authors>
	<abstract>In this paper we discuss a new approach to dynamic load balancing of parallel jobs in clusters of workstations and describe the implementation into a Unix run-time environment. The efficiency of the proposed methodology is shown by means of a number of case studies.</abstract>
	<keywords>Dynamic load balancing; Task migration; Distributed computing systems; Clusters of workstations</keywords>
	<publication_month_year>1996-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 1</volumes_issues>
</paper>
<paper no=340>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards provably correct system synthesis and extension</paper_heading>
	<authors>Fausto Giunchiglia, Paolo Pecchiari, Alessandro Armando</authors>
	<abstract>Our ultimate goal is to define a framework and a methodology which will allow users to construct or extend complex reasoning systems in such a way that the correctness of the resulting system is guaranteed. Our approach is based on the following principles: (i) construct the prover according to certain general (but precise) criteria, in particular maintain a sharp distinction among the logical, control, and interaction components; (ii) use a uniform framework to specify these three levels; (iii) represent (selected parts of) the code in a classical first-order theory, use the inference capabilities of the system to reason deductively about this theory, and, as a result, synthesize new code which can be pushed back in the underlying implementation. This paper describes the approach, what we have done so far and how we intend to proceed to pursue our ultimate goal.</abstract>
	<keywords>Theorem proving; Metatheoretic reasoning; Open mechanized reasoning systems</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=341>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>METAFOL: Program tactics and logic tactics plus reflection</paper_heading>
	<authors>Massimo Benerecetti, Luca Spalazzi</authors>
	<abstract>The goal of this paper is to present a reasoning system, called METAFOL, embodying the features of a self-reflective system, i.e. a system which has a representation of its own code and is able to reason about and, possibly, extend or modify it. These features have been achieved by employing a logical metatheory, which represents the computation implementing deduction in its object theory. Computation is represented in the metatheory as logical manipulation of a set of terms, called logic tactics. They are the logical counterpart of a set of programs, called program tactics, implementing deduction in the object theory. The mapping existing between logic tactics and program tactics allows us to synthesize correct programs by reasoning in the metatheory. It also allows us to implement a reflection mechanism which can be exploited to execute logic tactics and/or compile them into the system code as program tactics.</abstract>
	<keywords>Self-reflective systems; Metatheoretic reasoning; Proof checkers; Tactics</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=342>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Descriptive dynamic logic and its application to reflective architectures</paper_heading>
	<authors>Carles Sierra, Lluis Godo, Ramon López de Màntaras, Mara Manzano</authors>
	<abstract>The aim of this paper is to propose dynamic logic as a common logical framework to describe and identify the most relevant formal characteristics of multi-language logical architectures (MLA) in order to investigate the expressive power of the knowledge bases that can be built upon them. In general, a MLA allows to build knowledge bases as a set of units with initial local theories written in possible different logical languages. Each unit is also usually allowed to have its own intra-unit deductive system. Moreover, the whole knowledge base is equipped with an additional set of deductive rules, called bridge rules, to control the information flow among the different units of the knowledge base. The set of bridge rules act as an inter-unit deductive system. The reasoning dynamics of a knowledge base on top of a MLA can therefore be described by how the local theories of the units evolve during execution.</abstract>
	<keywords>Dynamic logic; Multi languange architectures; Reflection</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=343>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Inference and reflection in the object-centered representation language NOOS</paper_heading>
	<authors>Josep Lluís Arcos, Enric Plaza</authors>
	<abstract>This paper explains the inference and reflection capabilities of NOOS, an object-centered representation language designed to integrate problem solving and learning. Problem solving and learning in NOOS are modelled by means of concepts, tasks, methods and metalevels. Metalevels allow NOOS to reason about own problem solving. Using metalevels, NOOS can reason about preferences in order to make decisions about sets of alternatives present in domain knowledge and problem solving knowledge. Reflection in NOOS is provided by inference processes that involve metalevels. Basic reflective capabilities include reasoning about alternative methods to solve a task, reasoning about what is known by the system itself, and reasoning about the existence of solutions. A formal model of NOOS inference using Descriptive Dynamic Logic is also presented.</abstract>
	<keywords>Reflection; Object-centered representation language; Knowledge representation; Machine learning; Case-based reasoning</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=344>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Meta-level selection techniques for the control of default reasoning</paper_heading>
	<authors>Victor Allis, Yao-Hua Tan, Jan Treur</authors>
	<abstract>The problem how to control default reasoning is addressed. In an earlier work it was described how selection functions (on default conclusions) added to default logic can be exploited to describe control. In this paper specific properties of selection functions are described. Moreover, we investigate under which conditions selection functions can be expressed in the form of declarative control knowledge at the meta-level. Furthermore, we show that normal default logic with additional control by selection functions is (in some sense) equivalent to default logic in general. Finally, an alternative approach to control is introduced based on inhibition relations between default rules.</abstract>
	<keywords>Non-monotonic reasoning; Knowledge representation; Implementations of non-classical reasoning; Meta-level architectures</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=345>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Declarative reflection tools for agent shells</paper_heading>
	<authors>Enn Tyugu, Mattin Addibpour</authors>
	<abstract>This paper introduces declarative reflection schemata and describes implementation tools for it. The keywords are planning and inference, i.e. a declarative (non-procedural) description of self must exist and be used for making inferences about the future activities of the reflective system and about consequences of these activities. The NUT system is used as a platform for intelligent agents with reflection. NUT processes running in a network are agent shells filled with knowledge in the form of NUT packages. A knowledge-based technique supported by automatic synthesis of programs is used for declarative reflection: a reflection schema with a model of self managed by daemons is implemented using run-time compilation/decompilation of classes and metaprogramming with productions.</abstract>
	<keywords>Declarative reflection; Intelligent agents; Metaprogramming</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=346>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using reflection techniques for flexible problem solving (with examples from diagnosis)</paper_heading>
	<authors>Annette ten Teije, Frank van Harmelen</authors>
	<abstract>Flexible problem solving consists of the dynamic selection and configuration of problem solving methods for a particular problem type, depending on the particular problem and the goal of problem solving. In this paper, we propose an architecture that supports such flexible problem solving automatically. For this purpose, problem solving methods are described in a uniform way, by an abstract model of components, which together define the functionality of the methods. Such an abstract model is used for dynamic selection and configuration of the problem solving methods. The proposed architecture for flexible problem solving consists of well-known reflection techniques: two object-meta relations, a definable naming mechanism and the axiomhood and theoremhood reflection rules. We have succeeded in using standard meta-architecture techniques to enable flexible problem solving.</abstract>
	<keywords>Applications of meta-reasoning; Architectures for meta-reasoning; Configuration of problem solving methods; Diagnosis</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=347>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation of a reflective system</paper_heading>
	<authors>Jacques Pitrat</authors>
	<abstract>MACISTE is a reflective system that uses metaknowledge to use this metaknowledge itself. In particular, metaknowledge compiles knowledge and metaknowledge. As all metaknowledge is given in a declarative way, we bootstrap the system so that one can give it metaknowledge in a more and more declarative form. There is another kind of reflectivity: MACISTE can observe its own behavior so that it will be able to improve itself if it detects some errors or inefficiencies. Although its main goal is to improve itself, it has been used to define metaknowledge for a general problem-solver and for a general theorem-prover.</abstract>
	<keywords>Reflectivity; Metaknowledge; Bootstrap</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=348>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A hierarchical and reflective framework for synchronization and scheduling controls</paper_heading>
	<authors>Tzilla Elrad, Ufuk Verün</authors>
	<abstract>Synchronization and scheduling of system components form a major part of the requirements of today's complex systems, such as soft-real-time and reactive/adaptive systems. We present a general framework for the reflective, distributed, and hierarchical prioritization of complex system components. Our framework differs from other approaches in a few ways: Synchronization and scheduling policies are expressible as first class constructs; they can be specified and programmed like any other language construct. The framework supports complete locality control: The system components are partitioned into a hierarchy of levels. Each hierarchy level has complete control over its functionality, behavior, actions, synchronization, and scheduling; each level prioritizes only the components within itself. The system dynamically computes and alters a prioritization policy expression at each level during run time. The abstraction treats the synchronization and scheduling controls as it has been the case for data and operations in object-oriented programming languages. The schema allows for the reuse and extension of such controls through specification parts.</abstract>
	<keywords>Object-oriented concurrent programming; Reactive/Adaptive systems; Reflection; Reuse; Synchronization; Scheduling</keywords>
	<publication_month_year>1996-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issues 2–3</volumes_issues>
</paper>
<paper no=349>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scalability, portability and predictability: The BSP approach to parallel programming</paper_heading>
	<authors>W.F. McColl</authors>
	<abstract>General purpose parallel computing systems come in a variety of forms. We have various kinds of distributed memory architectures, shared memory multiprocessors, and clusters of workstations. New technologies may increase this range still further. Can one hope to design portable and scalable parallel software in the face of such architectural diversity? In this paper we show that it is indeed possible to produce fully portable parallel software which will run with highly efficient, scalable and predictable performance on any general purpose parallel architecture. The approach we describe is based on the bulk synchronous parallel (BSP) model of computation. The BSP model provides a simple, unified framework for the design and programming of all kinds of general purpose parallel systems. Over the last few years, a number of important research activities in algorithms and architectures have been pursued as part of this new approach to scalable parallel computing. In this paper we give some simple BSP algorithms and show how they can be expressed as programs. We also briefly describe some of the BSP programming language developments which are now being pursued.</abstract>
	<keywords>Parallel computing; Bulk Synchronous Parallel model; Programming languages; Parallel algorithms</keywords>
	<publication_month_year>1996-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 4</volumes_issues>
</paper>
<paper no=350>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High-performance computing in chemistry: NW Chem</paper_heading>
	<authors>M.F. Guest, E. Apra, D.E. Bernholdt, H.A. Früchtl, ... J. Nieplocha</authors>
	<abstract>The impact of high-performance computing in computational chemistry is considered in the light of increasing demands for both the number and complexity of chemical systems amenable to theoretical treatment. Using self-consistent field Density Functional Theory (DFT) as a prototypical application, we describe the development, implementation and performance of the NWChem computational chemistry package that is targeting both present and future generations of massively parallel processors (MPP). The emphasis throughout this development is on scalability and the distribution, as opposed to the replication, of key data structures. To facilitate such capabilities, we describe a shared non-uniform memory access model which simplifies parallel programming while at the same time providing for portability across both distributed- and shared-memory machines. The impact of these developments is illustrated through a performance analysis of the DFT module of NWChem on a variety of MPP systems.</abstract>
	<keywords>High-performance computing; Computational chemistry; Density Functional Theory; Massively Parallel Processors</keywords>
	<publication_month_year>1996-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 4</volumes_issues>
</paper>
<paper no=351>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Matching user requirements in parallel programming</paper_heading>
	<authors>K.M. Decker, J.J. Dvorak, R.M. Rehmann, R. Rühl</authors>
	<abstract>While productive use of parallel systems for business applications is quickly growing, their usage for scientific applications is far below expectations. The main reason is that programming of parallel systems is still far too complex. Starting from an analysis of the user requirements, this paper explains our strategy towards next-generation parallel programming environments. We first discuss the problem areas of parallel programming from a user's point of view and give an overview on existing low-, high- and intermediate-level tools. We summarize our objectives and the status of our research. Then we discuss three enabling technologies to overcome current-days limitations of tool environments for parallel computing: formal specification languages, artificial intelligence, and compiler technology. The impact of these technologies on the components proposed for a next-generation programming environment is discussed in detail.</abstract>
	<keywords>Parallel computing; Development environment; Compiler technology; Specification language; Artificial intelligence; User interface; Distributed memory; High performance</keywords>
	<publication_month_year>1996-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 4</volumes_issues>
</paper>
<paper no=352>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel magnetohydrodynamics on the Cray T3D</paper_heading>
	<authors>P.M. Meijer, S. Poedts, J.P. Goedbloed</authors>
	<abstract>The equations of magnetohydrodynamics (MHD) are discussed in the framework of parallel computing. Both linear and nonlinear MHD models are addressed. Special attention is given to the parallellisation of the kernels of the existing sequential MHD codes. These kernels involve matrix-vector multiplications and dot products for the linear MHD calculations, and Fast Fourier Transforms and tri-diagonal systems solvers for the nonlinear MHD simulations. The parallel results, such as scaling with the number of processors and the performances, are given for a 256 node Cray T3D. Impressive results are obtained by combining the implicit shared memory shmem_get routine of Cray with single node BLAS routines.</abstract>
	<keywords>Parallel MHD: Cray T3D; Scalar products; Matrix-vector multiplications; Tridiagonal systems solver</keywords>
	<publication_month_year>1996-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 4</volumes_issues>
</paper>
<paper no=353>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Intelligent storage devices for scalable information management systems</paper_heading>
	<authors>Robert Kukla, Jon Kerridge</authors>
	<abstract>For most large commercial organisations the ability to store and manipulate massive volumes of data is a key operational requirement. Database systems are fundamental to the efficient provision of a range of business tasks from on-line transaction processing, decision support and enterprise information systems. High performance multiprocessor database servers open up new possibilities for the realisation of these commercial requirements. If these opportunities are to be realised intelligent storage devices will be required to provide a uniform interface regardless of implementation and which also directly supports the basic need of scalability. In this paper we explore some of the issues for such a device.</abstract>
	<keywords>Database management systems; Parallel processing; Storage device; ODBC; Transputer</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=354>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A new approach for load balancing in high-performance decision support systems</paper_heading>
	<authors>B. Schiemann, L. Borrmann</authors>
	<abstract>We outline a new approach for load balancing in high-performance parallel database systems. It is based on the architectural model of a distributed system where the communication between database servers is implemented by means of the technology of Interface Definition Languages. This model offers the potential of major breakthroughs in the load balancing issue on heterogeneous distributed systems.</abstract>
	<keywords>Load balancing; Decision support; CORBA; Middleware; Interoperability; Database Management System</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=355>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High performance scientific computing by a parallel cellular environment</paper_heading>
	<authors>S. Di Gregorio, R. Rongo, W. Spataro, G. Spezzano, D. Talia</authors>
	<abstract>This paper describes CAMEL, a parallel environment for designing scientific applications based on the cellular automata mathematical model. CAMEL is an interactive environment designed to support the development of high performance applications in science and engineering. It offers the computing power of a highly parallel computer, hiding the architecture issues from a user. The system can be used both as a tool to model dynamic complex phenomena and as a computational model for parallel processing. By CAMEL a user might write programs to describe the actions of thousands of simple active agents, then observe the global complex evolution that arises from all the local interactions. The paper presents the programming environment and a significant application in the area of soil decontamination.</abstract>
	<keywords>High performance computing; Parallel programming; Cellular automata; Simulation; Scientific visualization</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=356>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual particles and search for global minimum</paper_heading>
	<authors>W. Dzwinel</authors>
	<abstract>The models and computational techniques stemming from natural sciences — so-called natural solvers — become more and more popular as the methods of last resort in investigations of irreducible problems of universal character. In the paper, the author uses the concept of a virtual particle (VIP) model. It covers the broad range of natural solvers, which use VIPs as basic objects. The main features of particles paradigm proposed: simplicity, decomposition ability, and built-in message-passing way of communication, make it attractive as a universal approach for mapping different problems on parallel platform. In course of the paper, molecular dynamics (MD) is proposed as a pure VIP model. In dependence of the level of abstraction the particle can be defined as atom, cluster of molecules, piece of matter, object or UNIX process. MD applications as a method of search for global minimum in multi-dimensional function domain is discussed. The features extraction problem considered, shows the advantages of this technique. It is also shown that Hopfield nets can be mapped onto a particle model. The locality of the particles paradigm is discussed on the base of the spatial genetic algorithm.</abstract>
	<keywords>Natural solvers, Virtual particles; Global minimum search; Molecular dynamics; Predictive display; Vehicle navigation; Genetic algorithms; Simulated annealing; Hopfield nets</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=357>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Load balancing by redundant decomposition and mapping</paper_heading>
	<authors>J.F. de Ronde, A. Schoneveld, P.M.A. Sloot</authors>
	<abstract>In this paper a new methodology for load balancing parallel processes on parallel systems is proposed. The problem of load balancing is considered to be an NP-hard optimization task. Taking static parallel finite element applications as a case study, the benefits and losses that follow from applying the methodology are studied. It is found that the proposed methodology can be especially useful for load balancing in asymmetric processor topologies, and therefore is of importance for work load balancing in workstation clusters.</abstract>
	<keywords>Redundant domain decomposition; Mapping; Graph based parallel process/processor modelling</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=358>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>MPI-DDL: A distributed-data library for MPI</paper_heading>
	<authors>Niandong Fang, Helmar Burkhart</authors>
	<abstract>The Message-Passing Interface (MPI) defines a de facto standard for writing message-passing programs. However, MPI operates at a rather low level in the sense that a programmer regards a message as a programming unit. We present a new point of view. In our approach, a programmer regards distributed data as a programming unit. MPI-DDL is a programming environment which contains an application-oriented layer on top of MPI to facilitate the programming of distributed data, and a set of tools. We present initial performance comparisons of two matrix algorithms using MPI-DDL, HPF, and direct MPI implementations.</abstract>
	<keywords>Parallel programming; Message-passing library; MPISingle Program Distributed Data (SPDD)</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=359>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Iteration space partitioning</paper_heading>
	<authors>Aart J.C. Bik, Harry A.G. Wijshoff</authors>
	<abstract>In this paper, we present a general method to isolate the loop-body of a nested loop for all iterations in a particular polyhedral set. This isolation is done by successively partitioning execution sets of individual DO-loops in the loop, thereby partitioning the iteration space according to the linear inequalities that define the polyhedral set. In particular, this method is useful to perform program simplifications or to support data structure transformations.</abstract>
	<keywords>Iteration space partitioning; Loop transformation; Program restructuring; Program optimization</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=360>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Libraries to support distribution and processing of visualization data sets</paper_heading>
	<authors>Steve Larkin, Andrew J. Grant, W.T. Hewitt</authors>
	<abstract>The aims of “Visualization in Parallel” (Vipar) project is to produce a comprehensive environment for the development of parallel visualization modules in systems such as the Application Visualization System (AVS), Iris Explorer, IBM Data Explorer (DX) and Khoros. This paper presents an overview of the project and describes the libraries developed to support the first phase of the work which is a tool to describe parallel visualization modules. This work is funded as part of the EPSRC project (GR/K40390) Portable Software Tools for Parallel Architectures (PSTPA).</abstract>
	<keywords>AVS; Iris Explorer; IBM Data Explorer; Parallel visualization systems; MPI</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=361>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational steering</paper_heading>
	<authors>Robert van Liere, Jurriaan D. Mulder, Jarke J. van Wijk</authors>
	<abstract>The traditional cycle in simulation is to prepare input, execute a simulation, and to visualize the results as a post-processing step. However, more insight and a higher productivity can be achieved if these activities are done simultaneously. This is the underlying idea of computational steering: researchers change parameters of their simulation on the fly and immediately receive feedback on the effect. In this paper the computational steering environment, CSE, is described. We discuss the requirements of computational steering environment, its relation with high performance computing and networking, and show an application of its use.</abstract>
	<keywords>Scientific visualization; Computational steering; 3D graphics and interaction</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=362>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Perspectives on high performance network computing</paper_heading>
	<authors>V Strumpen, B Ramkumar, T.L Casavant, S.M Reddy</authors>
	<abstract>Networks of workstations have become increasingly popular for high performance computing. However, in order to become a real alternative for Massively Parallel Processors (MPPs), reliability and efficiency issues must be tackled. In this paper, we identify the key challenges for very large workstation networks, and describe implementation techniques at system software level to overcome these problems.</abstract>
	<keywords>High performance computing; MPP; Workstation networks</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=363>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Changing technologies of HPC</paper_heading>
	<authors>J.J. Dongarraa, H.W. Meuer, H.D. Simon, E. Strohmaier</authors>
	<abstract>In 1993 for the first time a list of the top 500 supercomputer sites world-wide was made available. The Top 500 list allows a much more detailed and well-founded analysis of the state of high performance computing. Previously data such as the number and geographical distribution of supercomputer installations were difficult to obtain, and only a few analysts undertook the effort to track the press releases by dozens of vendors. With the Top 500 report now generally and easily available it is possible to present an analysis of the state of High Performance Computing (HPC). This paper summarizes some of the most important observations about HPC as of late 1996, in particular the continued dominance of the world market in HPC by the US, the market penetration by commodity microprocessor based systems, and the growing industrial use of super-computers.</abstract>
	<keywords>High performance computing; HPC technology; Supercomputer market; Supercomputer technology</keywords>
	<publication_month_year>1997-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 5</volumes_issues>
</paper>
<paper no=364>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient shared-memory support for parallel graph reduction</paper_heading>
	<authors>Andrew J. Bennett, Paul H.J. Kelly</authors>
	<abstract>This paper presents the results of a simulation study of cache coherency issues in parallel implementations of functional programming languages. Parallel graph reduction uses a heap shared between processors for all synchronisation and communication. We show that a high degree of spatial locality is often present and that the rate of synchronisation is much greater than for imperative programs. We propose a modified coherency protocol with static cache line ownership and show that this allows locality to be exploited to at least the level of a conventional protocol, but without the unnecessary serialisation and network transactions this usually causes. The new protocol avoids false sharing, and makes it possible to reduce the number of messages exchanged, but relies on increasing the size of the cache lines exchanged to do so. It is, therefore, of most benefit with a high-bandwidth interconnection network with relatively high communication latencies or message handling overheads.</abstract>
	<keywords>Cache organisation; Simulation; Declarative languages; Shared-memory</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=365>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Resource estimation for heterogeneous computing</paper_heading>
	<authors>Mary M. Eshaghian, Ying-Chieh Wu</authors>
	<abstract>Code-profiling is the process of determining the types of codes found in a given heterogeneous task. Once this information is available, it is desirable to know how many processors are needed for each of the code types. In this paper, we propose two methods for estimating the minimum number of processors required for each of these code types. The first method involves making use of task compatibility graphs. We show that a task compatibility graph can be generated by analyzing certain compatible relations between task module pairs of a given task flow graph. We define the resource (processor) minimization problem therefore to be equivalent to finding the minimal number of cliques that cover the task compatibility graph, or to finding the minimal number of colors that color the vertices of its complement graph, called task conflict graph. We solve this problem using a greedy approach in O(¦V¦log¦V¦¦E¦) time, where ¦V¦ and ¦E¦ are the number of vertices and edges of the task compatibility graph. We further show that for three special types of task compatibility graphs, optimal solution can be obtained in polynomial time. The second method studied in this paper uses the Cluster-M methodology for estimating the minimum number of processors. Examples are shown to compare the estimated results obtained using different techniques.</abstract>
	<keywords>Resource estimation; Code-profiling; Heterogeneous computing</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=366>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A comparative workload-based methodology for performance evaluation of parallel computers</paper_heading>
	<authors>E. Onbasioglu, Y. Paker</authors>
	<abstract>A practical methodology for evaluating and comparing the performance of distributed memory Multiple Instruction Multiple Data (MIMD) systems is presented. The methodology determines machine parameters and program parameters separately, and predicts the performance of a given workload on the machines under consideration. Machine parameters are measured using benchmarks that consist of parallel algorithm structures. The methodology takes a workload-based approach in which a mix of application programs constitutes the workload. Performance of different systems are compared, under the given workload, using the ratio of their speeds. In order to validate the methodology, an example workload has been constructed and the time estimates have been compared with the actual runs, yielding good predicted values. Variations in the workload are analysed in terms of increase in problem sizes and changes in the frequency of particular algorithm groups. Utilization and scalability are used to compare the systems when the number of processors is increased. It has been shown that performance of parallel computers is sensitive to the changes in the workload and therefore any evaluation and comparison must consider a given user workload. Performance improvement that can be obtained by increasing the size of a distributed memory MIMD system depends on the characteristics of the workload as well as the parameters that characterize the communication speed of the parallel system.</abstract>
	<keywords>Code characterization; Execution time prediction; Machine characterization; MIMD performance; Parallel algorithms; Parallel computers; Performance evaluation; Performance prediction; Workload-based performance comparison</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=367>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A transformation strategy for implementing distributed, multilayer feedforward neural networks: Backpropagation transformation</paper_heading>
	<authors>George L. Rudolph, Tony R. Martinez</authors>
	<abstract>Most artificial neural networks (ANNs) have a fixed topology during learning, and often suffer from a number of shortcomings as a result. Variations of ANNs that use dynamic topologies have shown ability to overcome many of these problems. This paper introduces location-independent transformations (LITs) as a general strategy for implementing distributed feed forward networks that use dynamic topologies (dynamic ANNs) efficiently in parallel hardware. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents a LIT that supports both the standard (static) multilayer backpropagation network, and backpropagation with dynamic extensions. The complexity of both learning and execution algorithms is O(q(Nlog M)) for a single pattern, where q is the number of weight layers in the original network, N the number of nodes in the widest node layer in the original network, and M is the number of nodes in the transformed network (which is linear in the number hidden nodes in the original network). This paper extends previous work with 2-weight-layer backpropagation networks.</abstract>
	<keywords>Neural networks; Backpropagation (BP); Implementation design; Dynamic topologies; Reconfigurable architectures</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=368>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Upgrading the service capacity of video-on-demand servers with memory buffer</paper_heading>
	<authors>Fu-Ching Wang, Chun-Hung Wen, Chih-Yuan Cheng, Meng-Huang Lee, ... Yen-Jen Oyang</authors>
	<abstract>In this paper, we propose a new approach to exploit semiconductor memory to upgrade the service capacity of video-on-demand systems. The basic idea behind this so-called Gneralized Relay Mechanism is that if we can relay the data of frequently accessed programmes from one access request to another, then we can improve the service capacity of the system without requiring higher disk bandwidth. The design of the Generalized Relay Mechanism is aimed at optimizing allocation and trade-off of the memory resource and the disk bandwidth resource. A simulation-based study shows that the Generalized Relay Mechanism is a very competitive alternative in comparision with simply incorporating more hard disks. If compared with an intuitive approach of utilizing memory buffer that always fills up the memory with most frequently accessed programmes, the Generalized Relay Mechanism enjoys an advantage in terms of cost.</abstract>
	<keywords>Semiconductor memory; Video-on-demand; Service capacity</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=369>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The adolescence of smart cards</paper_heading>
	<authors>Jean-Jacques Quisquater</authors>
	<abstract>The story of the smart card is an interesting technological story because there was no research at all in the beginning. It means that there are many problems due to the fact that people wanted to solve very specific problems for few commercial applications. This paper sketches the various phases that the smart card has gone through.</abstract>
	<keywords>Smart card; Security; Cryptographic algorithms</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=370>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Remote auditing of software outputs using a trusted coprocessor</paper_heading>
	<authors>Bruce Schneier, John Kelsey</authors>
	<abstract>A cryptographic coprocessor is described for certifying outcomes of software programs. The system for certifying and authenticating outputs allows a third party who trusts the secure components of the system to verify that a specified program actually executed and produced a claimed output.</abstract>
	<keywords>Authentication; Digital signatures; Secure coprocessor; Smart cards</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=371>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Biometrics on smart cards: An approach to keyboard behavioral signature</paper_heading>
	<authors>Thomas J. Alexandre</authors>
	<abstract>To overcome the lack of security provided by passwords for authentication and access control, some researchers have investigated the field of biometrics for individual identification such as voice recognition, fingerprints or handwritten signature. Because of the significant amount of processing and memory space required by those approaches, implementing them in a physically secure environment such as a smart card remains difficult. Moreover, biometric systems based on physiological criteria suffer from the possibility for a potential intruder to imitate these features. In this paper we propose a new system of biometric identification based on the behavioral recognition of keyboard signature. Such a system provides the user with more security in the sense that a behavior is difficult to copy. The simplicity and reliability of our approach compared to the high power of discrimination it provides makes it suitable for built-in smart card applications. A neural network implementation through supervised and self-organizing techniques is also discussed in this paper and evaluated in terms of efficiency and performance. Because this new biometric system must be further evaluated with a large community of users, we finally discuss a proposal for testing purpose on the World-Wide Web using the emerging Java language.</abstract>
	<keywords>Biometrics; Smart cards; Neural networks; Keyboard signature</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=372>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lossless compression algorithms for smart cards: A progress report</paper_heading>
	<authors>J.F. Dhem, J.J. Quisquater</authors>
	<abstract>We first describe how data compression can be suitable for the smart card context. After looking at the different models of compressor, we choose the best suited for smart cards at the present time: the static lossless compressor, mainly the Huffman and the arithmetic. The static Huffman algorithm we implemented and the memory size needed for a smart card implementation are detailed. Thereafter, the first experimental results we obtain on some typical files permit to conclude that our compressor is implementable in smart cards. This paper opens up the way to a new approach of data compression algorithms for small files using very small resources. This work is still in progress.</abstract>
	<keywords>Static lossless compression; Smart cards; Huffman coding; Arithmetic coding</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=373>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using formal methods to cultivate trust in smart card operating systems</paper_heading>
	<authors>Marjan I. Alberda, Pieter H. Hartel, Eduard K. de Jong Frz</authors>
	<abstract>To be widely accepted, smart cards must contain completely trustworthy software. Because smart cards contain relatively simple computers, and are used only for a specific class of applications, it is feasible to make the language used to program the software components focused and tiny. Formal methods can be used to precisely specify this language and to reason about properties of the language, which results in more trustworthy software. We explore this process by specifying the core of a proprietary systems programming language for smart card operating systems. We show how the specification obtained is used in proofs, and in the development of tool support.</abstract>
	<keywords>Smart card operating systems; Structural operational semantics; Software components; Proofs; Tool support</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=374>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Protection of software algorithms executed on secure modules</paper_heading>
	<authors>H.D.L. Hollmann, J.P.M.G. Linnartz, J.H. van Lint, C.P.M.J. Baggen, L.M.G.</authors>
	<abstract>Loop structures in software code may reveal essential information about implemented algorithms and their parameters, even if the observer has no knowledge about which instructions are executed. Regular patterns can for instance be observed in power consumption, instruction fetches in external memory, or radiated EM energy. This paper addresses the use of dummy operations to obscure the details of the algorithm executed by the processor. We show that for a particular class of dummy insertion strategies, a Viterbi decoder can fairly reliably distinguish dummy fetches from real instruction fetches. In the second part of this paper, we study strategies to choose dummy fetches from a more general model. For certain situations, the optimum protection strategy appears to be deterministic (as opposed to random). Moreover, we show that in such a case, it is fundamentally not possible to enhance the security of the implementation by keeping the strategy for generating dummy fetches secret to the attacker.</abstract>
	<keywords>Software protection; Secure processor; Viterbi decoder; Dummy instructions</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=375>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multi-application smart cards and encrypted data, processing</paper_heading>
	<authors>Josep Domingo-Ferrer</authors>
	<abstract>Some existing approaches to multi-application smart card design rely on the card containing data and importing the code of functions (methods) to be performed on data. A complementary solution is proposed in this paper to relax the requirement-or rather the bottleneck-that all confidential data and processing be supported by the card. Our approach is based on running some applications outside the card using encrypted data processing, specifically privacy homomorphisms. Examples of privacy homomorphisms are given, one of which is very recent and allows full arithmetic on encrypted data while remaining secure against known-cleartext attacks.</abstract>
	<keywords>Cryptographic protocols for smart cards; Smart cards architecture and techniques; Multi-application smart cards</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=376>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>How smart cards can benefit from object-oriented technologies</paper_heading>
	<authors>Patrick Biget, Patrick George, Jean-Jacques Vandewalle</authors>
	<abstract>We submit that a key enabling technology for non-predefined and multi-purpose smart cards is object-oriented technology. Object-oriented concepts and skills have proved their efficiency to model, design, and implement information systems made of small components. Smart cards become more and more personal environments for multiplicity of services. Therefore, they need to allow downloading of unpredictable services and to be easily integrated into information systems. This paper presents the usage of object-oriented technologies to implement a generic smart card operating system and to provide a card object adapter to access smart card services from distributed object-oriented information systems based on CORBA architecture.</abstract>
	<keywords>Smart card operating system; Smart card integration; Object technologies; CORBA</keywords>
	<publication_month_year>1997-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 12, Issue 6</volumes_issues>
</paper>
<paper no=377>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Data mining and KDD: Promise and challenges</paper_heading>
	<authors>Usama Fayyad, Paul Stolorz</authors>
	<abstract>Databases are growing in size to a stage where traditional techniques for analysis and visualization of the data are breaking down. Data mining and knowledge discovery in databases (KDD) are concerned with extracting models and patterns of interest from large databases. Data mining techniques have their origins in methods from statistics, pattern recognition, databases, artificial intelligence, high performance and parallel computing, and visualization. In this article, we provide an overview of this growing multi-disciplinary research area, outline the basic techniques, and provide brief coverage of how they are used in some applications. We discuss the role of high performance and parallel computing in data mining problems, and we provide a brief overview of a few applications in science data analysis. We conclude by listing challenges and opportunites for future research.</abstract>
	<keywords>Data mining; Data analysis; Science data analysis; Overview article; Knowledge discovery in databases; Databases; Parallel data mining</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=378>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A statistical perspective on data mining</paper_heading>
	<authors>Jonathan R.M. Hosking, Edwin P.D. Pednault, Madhu Sudan</authors>
	<abstract>Data mining can be regarded as a collection of methods for drawing inferences from data. The aims of data mining, and some of its methods, overlap with those of classical statistics. However, there are some philosophical and methodological differences. We examine these differences, and we describe three approaches to machine learning that have developed largely independently: classical statistics, Vapnik's statistical learning theory, and computational learning theory. Comparing these approaches, we conclude that statisticians and data miners can profit by studying each other's methods and using a judiciously chosen combination of them.</abstract>
	<keywords>Classification; Frequentist inference; PAC learning; Statistical learning theory</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=379>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Clustering techniques</paper_heading>
	<authors>Pierre Michaud</authors>
	<abstract>Given a population of individuals described by a set of attribute variables, clustering them into “similar” groups has many applications. The clustering problem, also known as unsupervised learning, is the problem of partitioning a population into clusters (or classes). The population is a set of n elements that can be clients, products, shops, agencies, etc., described by m attributes. These attributes can be quantitative (salary), categorical (type of profession) or binary (owner of a credit card). The goal is to construct a partition in which elements of a cluster are “similar” and elements of different clusters are “dissimilar” in terms of the m attributes. Here we define the clustering problem and discuss the ideas behind some of the major approaches, including a relatively new method, called RDA/AREVOMS, that is based on the theory of voting.</abstract>
	<keywords>Unsupervised learning; Partitioning criteria; Neural network; New Condorcet criterion</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=380>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A comparative study of clustering methods</paper_heading>
	<authors>Mohamed Zaït, Hammou Messatfa</authors>
	<abstract>In this paper we propose a methodology for comparing clustering methods based on the quality of the result and the performance of the execution. We applied it to several known clustering methods: FastClust, Autoclass, Relational data analysis, and Kohonen nets. The quality of a clustering result depends on both the similarity measure used by the method and its implementation. An important feature of our methodology is a synthetic data generation program that allows producing data sets with specific (or desired) patterns using a combination of parameters, such as the number and the type of the attributes, the number of records, etc. We define a metric to measure the quality of a clustering method, i.e., its ability to discover some or all of the “hidden” patterns. The performance study is based on the resource consumption, i.e., CPU time and memory space.</abstract>
	<keywords>Clustering; Data mining; Data generation; Unsupervised classification</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=381>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Mining generalized association rules</paper_heading>
	<authors>Ramakrishnan Srikant, Rakesh Agrawal</authors>
	<abstract>We introduce the problem of mining generalized association rules. Given a large database of transactions, where each transaction consists of a set of items, and a taxonomy (is-a hierarchy) on the items, we find associations between items at any level of the taxonomy. For example, given a taxonomy that says that jackets is-a outerwear is-a clothes, we may infer a rule that “people who buy outerwear tend to buy shoes”. This rule may hold even if rules that “people who buy jackets tend to buy shoes”, and “people who buy clothes tend to buy shoes” do not hold. An obvious solution to the problem is to add all ancestors of each item in a transaction to the transaction, and then run any of the algorithms for mining association rules on these “extended transactions”. However, this “Basic” algorithm is not very fast; we present two algorithms, Cumulate and EstMerge, which run 2 to 5 times faster than Basic (and more than 100 times faster on one real-life dataset). Finally, we present a new interest-measure for rules which uses the information in the taxonomy. Given a user-specified “minimum-interest-level”, this measure prunes a large number of redundant rules; 40–60% of all the rules were pruned on two real-life datasets.</abstract>
	<keywords>Association rules; Taxonomy; Hierarchy; Data mining</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=382>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Attribute selection for modelling</paper_heading>
	<authors>Igor Kononenko, Se June Hong</authors>
	<abstract>Modelling a target attribute by other attributes in the data is perhaps the most traditional data mining task. When there are many attributes in the data, one needs to know which of the attribute(s) are relevant for modelling the target, either as a group or the one feature that is most appropriate to select within the model construction process in progress. There are many approaches for selecting the attribute(s) in machine learning. We examine various important concepts and approaches that are used for this purpose and contrast their strengths. Discretization of numeric attributes is also discussed for its use is prevalent in many modelling techniques.</abstract>
	<keywords>Attribute quality measures; Impurity function; Discretization; Classification; Regression</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=383>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Data mining with decision trees and decision rules</paper_heading>
	<authors>Chidanand Apté, Sholom Weiss</authors>
	<abstract>This paper describes the use of decision tree and rule induction in data-mining applications. Of methods for classification and regression that have been developed in the fields of pattern recognition, statistics, and machine learning, these are of particular interest for data mining since they utilize symbolic and interpretable representations. Symbolic solutions can provide a high degree of insight into the decision boundaries that exist in the data, and the logic underlying them. This aspect makes these predictive-mining techniques particularly attractive in commercial and industrial data-mining applications. We present here a synopsis of some major state-of-the-art tree and rule mining methodologies, as well as some recent advances.</abstract>
	<keywords>Decision tree; Rule induction; Data mining</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=384>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using neural networks for data mining</paper_heading>
	<authors>Mark W. Craven, Jude W. Shavlik</authors>
	<abstract>Neural networks have been successfully applied in a wide range of supervised and unsupervised learning applications. Neural-network methods are not commonly used for data-mining tasks, however, because they often produce incomprehensible models and require long training times. In this article, we describe neural-network learning algorithms that are able to produce comprehensible models, and that do not require excessive training times. Specifically, we discuss two classes of approaches for data mining with neural networks. The first type of approach, often called rule extraction, involves extracting symbolic models from trained neural networks. The second approach is to directly learn simple, easy-to-understand networks. We argue that, given the current state-of-the-art, neural-network methods deserve a place in the tool boxes of data-mining specialists.</abstract>
	<keywords>Machine learning; Neural networks; Rule extraction; Comprehensible models; Decision trees; Perceptrons</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=385>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Systems for KDD: From concepts to practice</paper_heading>
	<authors>Brian Dunkel, Nandit Soparkar, John Szaro, Ramasamy Uthurusamy</authors>
	<abstract>The considerable interest in knowledge discovery in databases (KDD) has led to several techniques and tools for the automated extraction of useful information from large data repositories. In order to use these developments in practical settings, there is need to consider the computing systems that would support the complete KDD process. In this regard, we identify and discuss important computing systems issues, and we compare and contrast some available research and commercial efforts. We suggest several enhancements to the underlying database systems that may significantly benefit the KDD process. We indicate why it is important that tools to handle different aspects of the KDD effort need to be integrated. Finally, we briefly describe our experience in implementing a prototype KDD system for a large corporate environment.</abstract>
	<keywords>Knowledge discovery; Data mining; Database systems; Systems integration</keywords>
	<publication_month_year>1997-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 2–3</volumes_issues>
</paper>
<paper no=386>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed parallel volume rendering on shared memory systems</paper_heading>
	<authors>D.J. Hancock, R.J. Hubbold</authors>
	<abstract>This paper reports on the remote use of MIMD parallel machines for compute-intensive visualisation tasks. As an alternative to dedicated rendering hardware, a system using conventional desktop machines connected to a visualisation server permits sharing of the resource between several users, and/or sites. The design of a parallel rendering system should consider not only how to achieve efficient parallel performance, but how to minimise response time, i.e. the time taken from the specification of the viewing parameters to the display of the completed image. Progressive refinement and latency hiding techniques are proposed as a method of reducing the latency of a distributed rendering system. In distributed rendering, integrating the transmission of the image with the rendering process allows the latency introduced by the network to be hidden. Timing data are presented demonstrating the scalability of the parallel algorithm and its interactive use over LAN networks.</abstract>
	<keywords>Visualisation; Distributed; Parallel; Rendering; Direct volume rendering</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=387>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel simulation of ion recombination in nonpolar liquids</paper_heading>
	<authors>Frank J. Seinstra, Henri E. Bal, Hans J.W. Spoelder</authors>
	<abstract>Ion recombination in nonpolar liquids is an important problem in radiation chemistry. We have designed and implemented a parallel Monte Carlo simulation for this computationally intensive task on a network of workstations. The main problem with parallelizing this application is that the amount of work performed by each process decreases during execution, resulting in high communication overhead and load imbalances. We address this problem by dynamically adjusting the number of processors that are used. We have evaluated the performance of the parallel program on two systems, one using Ethernet and the other using Myrinet. On Ethernet, the program suffers from a large communication overhead. Using the Myrinet high-speed network in combination with a programming system (Orca) that is optimized for fast networks, however, the program obtains a high efficiency.</abstract>
	<keywords>Parallel Monte Carlo simulation; N-body problem; MyrinetOrca</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=388>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Application of HPC to a portfolio choice problem</paper_heading>
	<authors>Marc Breitler, Stéphane Hegi, Jean-Daniel Reymond, Nils S. Tuchschmid</authors>
	<abstract>In this paper, we study a portfolio choice problem when the investment opportunity set is not constant over time. More precisely, the expected returns of financial assets are assumed to be time-varying and driven by four state variables. This allocation problem leads to a set of complex non-linear partial differential equations whose numerical solution requires powerful computing resources. The numerical method for solving this type of convection-diffusion equations in this framework is described. The algorithm is implemented on a vector computer and with another version using the single program multiple data (SPMD) approach on a dedicated massively parallel processing (MPP) system. The efficiency of the method is evaluated on both types of architecture. Numerical results for real market conditions are presented using a wide range of parameter values to explore the validity domain of the algorithm.</abstract>
	<keywords>Dynamic portfolio allocation; Optimal portfolio choice; MPP; MPI; Convection-diffusion equation; Hyperbolic and parabolic problems</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=389>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel irregular software for wave propagation simulation</paper_heading>
	<authors>Frédéric Guidec, Patrice Calégari, Pierre Kuonen</authors>
	<abstract>The objective of the European project STORMS (software tools for the optimization of resources in mobile systems) is to develop a software tool to be used for the design and the planning of the future universal mobile telecommunication system (UMTS). In this context the ParFlow method permits the simulation of outdoor radio wave propagation in urban environment, modelling the physical system in terms of the motion of fictitious microscopic particles over a lattice. This paper gives an overview of the ParFlow method, and reports the design and the implementation of ParFlow++, an object-oriented irregular parallel software for urban outdoor radio wave propagation prediction.</abstract>
	<keywords>Mobile telecommunications; Radio wave propagation simulation; Transmission line matrix; Irregular algorithm; Object-oriented programming; Parallel computation</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=390>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Designing parallel models of soil contamination by the CARPET language</paper_heading>
	<authors>Giandomenico Spezzano, Domenico Talia</authors>
	<abstract>This paper describes the main features of the CARPET language and its practical use for programming three-dimensional models of the contamination of soils developed in the CABOTO project. CARPET is a high-level language based on the cellular automata model, which supports rapid prototyping of a large number of applications in science and engineering. A CARPET implementation has been used for programming cellular algorithms in the CAMEL parallel system. The CAMEL (Cellular Automata environMent for systEms modeLing) system is a parallel implementation of a software environment for the simulation and modeling of complex systems based on cellular automata. CAMEL offers the computing power of a parallel computer although hiding, by the CARPET language, the architecture issues to a user. The CARPET language allows the design of parallel programs for describing the actions of thousands of simple active interacting agents that might simulate the behavior of very complex systems.</abstract>
	<keywords>Parallel programming; Cellular automata; High performance computing; Complex system modeling</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=391>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Non-overlapping preconditioners for a parallel implicit Navier-Stokes solver</paper_heading>
	<authors>E. Issman, G. Degrez</authors>
	<abstract>Parallel implicit iterative solution techniques are considered for application to a compressible hypersonic Navier-Stokes solver on unstructured meshes. The construction of parallel preconditioners with quasi-optimal convergence properties with respect to their serial counterpart is a key issue in the design of modern parallel implicit schemes. Two types of non-overlapping preconditioners are presented and compared. The first one is an additive Schwarz preconditioner requiring overlapping of the mesh and the second one is based on a Schur complement formulation. Both are using incomplete LU factorisation at the subdomain level but scale differently. Results are presented for computations on the Cray T3D under the message passing interface MPI.</abstract>
	<keywords>Parallel preconditioners; Implicit time-stepping; Compressible Navier-Stokes equations; Unstructured grid methods</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=392>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel iterative solvers for irregular sparse matrices in High Performance Fortran</paper_heading>
	<authors>E. de Sturler, D. Loher</authors>
	<abstract>Writing efficient iterative solvers for irregular sparse matrices in High Performance Fortran (HPF) is hard. The locality in the computations is unclear, and for efficiency we use storage schemes that obscure any structure in the matrix. Moreover, the limited capabilities of HPF to distribute and align data structures make it hard to implement the desired distributions, or to indicate these such that the compiler recognizes the efficient implementation. We propose techniques to handle these problems. We combine strategies that have become popular in message-passing parallel programming, like mesh partitioning and splitting the matrix in local submatrices, with the functionality of HPF and HPF compilers, like the implicit handling of communication and distribution. The implementation of these techniques in HPF is not trivial, and we describe in detail how we propose to solve the problems. Our results demonstrate that very efficient implementations are possible. We indicate how some of the ‘approved extensions’ of HPF-2.0 can be used, but they do not solve all problems. For comparison we show the results for regular sparse matrices.</abstract>
	<keywords>High Performance Fortran; Irregular sparse matrices; Iterative solvers</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=393>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lessons learned from implementing BSP</paper_heading>
	<authors>Jonathan M.D Hill, David B Skillicorn</authors>
	<abstract>We focus on two criticisms of bulk synchronous parallelism (BSP): that delaying communication until specific points in a program causes poor performance, and that frequent barrier synchronisations are too expensive for high-performance parallel computing. We show that these criticisms are misguided, not just about BSP but about parallel programming in general, because they are based on misconceptions about the origins of poor performance. The main implication for parallel programming is that higher levels of abstraction do not only make software construction easier — they also make high-performance implementation easier.</abstract>
	<keywords>BSP; Barrier synchronisation; Total exchange; Broadcast; Performance modelling</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=394>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A two-way BSP algorithm for tridiagonal systems</paper_heading>
	<authors>Yuguang Huang, W.F. McColl</authors>
	<abstract>A two-way parallel recursive method is presented for solving a tridiagonal linear system. The algorithm is based on the parallel segment recursive method proposed in Huang (1997). The computation and communication costs of the algorithm are analysed using the BSP (bulk synchronous parallel) model. Copyright © 1998 Elsevier Science B.V.</abstract>
	<keywords>Parallel computing; Parallel algorithms; Bulk synchronous parallel model; Tridiagonal system</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=395>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Ninf and PM: Communication libraries for global computing and high-performance cluster computing</paper_heading>
	<authors>Mitsuhisa Sato, Hiroshi Tezuka, Atsushi Hori, Yutaka Ishikawa, ... Umpei Nagashima</authors>
	<abstract>This paper presents two advanced communication libraries, Ninf and PM. Ninf is an ongoing global network-wide computing infrastructure project which allows users to access computational resources including hardware, software and scientific data distributed across a wide area network. Computational resources are shared as Ninf remote libraries executable at a remote Ninf server. Users can build an application by calling the libraries with the Ninf Remote Procedure Call. In order to facilitate location transparency and network-wide parallelism, Ninf metaserver maintains global resource information regarding computational server and databases, allocating and scheduling coarse-grained computation for global load balancing. PM is a high-performance communication library for workstation clusters connected with Myrinet gigabit LAN card, which has a dedicated processor and on-board memory to handle a communication protocol. In order to obtain high-performance communication and support a multi-user environment, we co-designed PM, an operating system realized by a daemon process, and the run-time routine for a programming language. Several unique features, e.g., network context switching and modified ACK/NACK flow control algorithm have been developed for PM. PM for the Suns has a speed of 20 μs round trip for a user-level 8 bytes message and 38.6 Mbytes/s bandwidth for an 8 Kbytes message.</abstract>
	<keywords>Global computing; Remote procedure call; Cluster computing; Gigabit LAN; User-level communication protocol; High-performance computing</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=396>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The MOSIX multicomputer operating system for high performance cluster computing</paper_heading>
	<authors>Amnon Barak, Oren La'adan</authors>
	<abstract>The scalable computing cluster at Hebrew University consists of 88 Pentium II and Pentium-Pro servers that are connected by fast Ethernet and the Myrinet LANs. It is running the MOSIX operating system, an enhancement of BSD/OS with algorithms for adaptive resource sharing, that are geared for performance scalability in a scalable computing cluster. These algorithms use a preemptive process migration for load-balancing and memory ushering, in order to create a convenient multiuser time-sharing execution environment for HPC, particularly for applications that are written in PVM or MPI. This paper begins with a brief overview of MOSIX and its resource sharing algorithms. Then the paper presents the performance of these algorithms as well as the performance of several large-scale, parallel applications.</abstract>
	<keywords>Cluster computing; Load-balancing; Preemptive process migration; PVM; Multicomputer systems</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=397>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the symbiosis of memory and communication in the programming of parallel applications</paper_heading>
	<authors>J. Cordsen, W. Schröder-Preikschat</authors>
	<abstract>Interoperability in non-sequential applications requires communication to exchange information using either the shared-memory or message-passing paradigm. In the past, the communication paradigm in use was determined through the architecture of the underlying computing platform. Shared-memory computing systems were programmed to use shared-memory communication, whereas distributed-memory architectures were running applications communicating via message-passing. Current trends in the architecture of parallel machines are based on shared-memory and distributed-memory. For scalable parallel applications, in order to maintain transparency and efficiency, both communication paradigms have to coexist. Users should not be obliged to know when to use which of the two paradigms. On the other hand, the user should be able to exploit either of the paradigms directly in order to achieve the best possible solution. The paper presents the VOTE communication support system. VOTE provides coexistent implementations of shared-memory and message-passing communication. Applications can change the communication paradigm dynamically at runtime, thus we are able to employ the underlying computing system in the most convenient and application-oriented way. The presented case study and detailed performance analysis underpins the applicability of the VOTE concepts for high-performance parallel computing. By having implemented a symbiosis of memory and communication at the level of virtual shared memory, the end-user message-passing performance was significantly improved.</abstract>
	<keywords>Parallel computing; Shared-memory; Message-passing communication; Virtual shared memory; Parallel operating systems</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=398>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Symbolic analysis techniques for program parallelization</paper_heading>
	<authors>Thomas Fahringer</authors>
	<abstract>Existing parallelizing compilers have been shown to detect and exploit only small portions of parallelism in real application programs. A core problem is the difficulty to collect sufficient information from the program to utilize the underlying multiprocessor architecture. Symbolic analysis is the key component to support compilers in parametrizing codes by the number of processors as well as array sizes and to enable performance analyzers examining programs with program unknowns. We present a variety of novel symbolic analysis techniques that are crucial for a new generation of parallelizing compilers and performance analysis tools to handle non-linear array index functions, complex loop bounds, and deal with unknown problem, array and machine sizes which is critical for programming languages such as High Performance Fortran. This includes counting solutions to a system of constraints, computing lower and upper bounds of symbolic expressions, and comparing symbolic expressions. We have implemented all of these techniques as part of a state-of-the-art parallelizing compiler and a performance estimator. Experimental results will be shown that demonstrate the effectiveness of our symbolic analysis.</abstract>
	<keywords>Program analysis; Program unknowns; Non-linear expressions; Compilers; Comparing symbolic expressions; Symbolic expression bounds; Solutions to systems of constraints; Simplifying symbolic expressions; Symbolic dependence analysis</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=399>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>First Fast Sink: A compiler algorithm for barrier placement optimisation</paper_heading>
	<authors>E.A. Stöhr, M.F.P. O'Boyle</authors>
	<abstract>This paper describes a new compiler algorithm to reduce the number of barrier synchronisations in parallelised programs. A preliminary technique to rapidly determine critical data dependences is developed. This forms the basis of the First Fast Sink (FFS) algorithm which places, provably, the minimal number of barriers in polynomial time for codes with a regular structure. This algorithm is implemented in a prototype compiler and applied to three well-known benchmarks. Preliminary results show that it outperforms an existing state-of-the-art commercial compiler.</abstract>
	<keywords>Barrier synchronisation; Optimising compilation; Graph algorithm</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=400>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Overcoming the limitations of the traditional loop parallelization</paper_heading>
	<authors>Ireneusz Karkowski, Henk Corporaal</authors>
	<abstract>Previous research has shown existence of a huge potential of the coarse-grain parallelism in programs. This parallelism is however not always easy to exploit, especially, when applying today's parallelizing compilers to typical applications from the “embedded” domain. This is mainly due to the deficiencies of the static data dependency analysis they relay on. This paper investigates the potentials of the loops parallelization techniques using dynamic loop analysis techniques. For a set of “embedded” benchmarks (including an MPEG-2 encoder) ~4 times more loops could be parallelized, in comparison with a state-of-the-art compiler (SUIF [S.P. Amarasinghe et al., Multiprocessors From a Software Perspective, IEEE micro, June 1996, pp. 52–61]), leading to average speed-ups of 2.85 (on a four-processor system). Dynamic analysis is however not “full-proof” — we intend to use it exclusively in cases when static analysis fails to give any answer, and only if the user asserts its applicability.</abstract>
	<keywords>Multiprocessing; Loop parallelization techniques; Data dependency analysis; High performance embedded system design</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 4–5</volumes_issues>
</paper>
<paper no=401>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel execution of Prolog with granularity control</paper_heading>
	<authors>Lourdes Araujo, Jose J. Ruz</authors>
	<abstract>This paper presents a system for parallel execution of Prolog supporting both independent conjunctive and disjunctive parallelism. The system is intended for distributed memory architecture and is composed of a set of workers with a hierarchical structure scheduler. The execution model has been designed in such a way that each worker's environment does not contain references to terms in other environments, thus reducing communication overhead. In order to guarantee the improvement of the performance by the parallelism exploitation, a granularity control has been introduced for each kind of parallelism. For conjunctive parallelism PDP applies a control based on the estimation provided by CASLOG. The features of the system allow to introduce this control without adding overhead. For disjunctive parallelism PDP controls granularity by applying a heuristic-based method, which can be adapted to other parallel Prolog systems. Different scheduling policies have also been tested. The system has been implemented on a transputer network and performance results show that it provides a high speedup for coarse grain parallel programs.</abstract>
	<keywords> Logic programming; Conjunctive parallelism; Disjunctive parallelism; Granularity; Distributed memory architecture</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 6</volumes_issues>
</paper>
<paper no=402>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient message passing on UNIX shared memory multiprocessors</paper_heading>
	<authors>Massimo Bernaschi</authors>
	<abstract>We describe a single-copy mechanism which enables an efficient message passing among UNIX processes on shared memory multiprocessors. A special version of PVMe, IBM's AIX implementation of the PVM message passing programming model, has been built based on this approach. Some preliminary results here reported show the clear advantage of the single-copy with respect to more conventional schemes.</abstract>
	<keywords>Shared memory; Message passing; PVM; PowerPC</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 6</volumes_issues>
</paper>
<paper no=403>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Associative random access machines and data-parallel multiway binary-search join</paper_heading>
	<authors>Ok-Hyeong Cho, Robert M. Colomb</authors>
	<abstract>Rapid advances in semiconductor technology have made it possible to build massively parallel processors. In addition, optical 3D storage and optical interconnections open a new opportunity due to inherent massive parallelism and non-interference of light beams. The approaches used in current parallel database research cannot take advantage of massive parallelism which can be provided by the emerging technologies, due to speedup and scaleup limitations. In this paper, we present a computational paradigm for database machines which takes advantage of the opening opportunity for massive parallelism and discuss the validity and feasibility of the paradigm. The approach we take is based on associative computing and fine grained data parallelism which allow unlimited speedup and scaleup. Additionally, an asymptotically fast data-parallel join algorithm, which can efficiently deal with the joins in which multiple relations share a common join field, is presented. The algorithm is based on parallel sorting and parallel binary search, and performs a multiway join in O(Σs + Σ log r) where s is the cost of sorting an intermediate relation and r is the size of an input relation. The cost s of sorting is kept minimum by the algorithm.</abstract>
	<keywords>Massive parallel database machine; Relational join; Associative computing; Data parallelism; Optical computing</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 6</volumes_issues>
</paper>
<paper no=404>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An evaluation of hardware-based and compiler-controlled optimizations of snooping cache protocols</paper_heading>
	<authors>Fredrik Dahlgren, Jonas Skeppstedt, Per Stenström</authors>
	<abstract>Coherence misses and invalidation traffic limit the performance of bus-based multiprocessors using write-invalidate snooping caches. This paper considers optimizations of a write-invalidate protocol that remove such overhead. While coherence misses are attacked by a hybrid update/invalidate protocol and another technique where update instructions are selectively inserted by a compiler, invalidation traffic is reduced by three optimizations that coalesce ownership acquisition with miss handling: migrate-on-dirty, an adaptive hardware-based scheme, and compiler-controlled insertion of load-exclusive instructions. The relative effectiveness of these optimizations are evaluated using detailed architectural simulations and a set of four parallel programs. We find that while both of the update-based schemes effectively remove most coherence misses, the hybrid update/invalidate scheme causes lower traffic. By contrast, the compiler-based approach to cut invalidation traffic is slightly more efficient than the adaptive hardware-based scheme. Moreover, the migrate-on-dirty heuristic is found to have devastating effects on the miss rate.</abstract>
	<keywords>Shared-memory multiprocessors; Snoopy cache protocols; Dataflow analysis techniques; Performance evaluation</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 6</volumes_issues>
</paper>
<paper no=405>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient scheduling of MPI applications on networks of workstations</paper_heading>
	<authors>M.A.R. Dantas, E.J. Zaluska</authors>
	<abstract>The availability of a large number of workstations connected through a network can represent an attractive option for high-performance computing for many applications. The message-passing interface (MPI) software environment is an effort from many organisations to define a de facto message-passing standard. In other words, the original specification was not designed as a comprehensive parallel programming environment and some researchers agree that the standard should be preserved as simple and clean as possible. Nevertheless, a software environment such as MPI should have somehow a scheduling mechanism for the effective submission of parallel applications on network of workstations. This paper presents an alternative lightweight approach called Selective-MPI (S-MPI), which was designed to enhance the efficiency of the scheduling of applications on an MPI implementation environment.</abstract>
	<keywords>Message passing; Parallel environments; MPI; Distributed processing; Load balancing; Network of workstations</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 6</volumes_issues>
</paper>
<paper no=406>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient and scalable quicksort on a linear array with a reconfigurable pipelined bus system</paper_heading>
	<authors>Yi Pan, Mounir Hamdi, Keqin Li</authors>
	<abstract>Based on the current fiber optic technology, a new computational model, called a linear array with a reconfigurable pipelined abus system (LARPBS), is proposed in this paper. A parallel quicksort algorithm is implemented on the model, and its time complexity is analyzed. For a set of N numbers, the quicksort algorithm reported in this paper runs in O(log2 N) average time on a linear array with a reconfigurable pipelined bus system of size N. If the number of processors available is reduced to P, where P < N, the algorithm runs in O((NP) log2N) average time and is still scalable. Besides proposing a new algorithm on the model, some basic data movement operations involved in the algorithm are discussed. We believe that these operations can be used to design other parallel algorithms on the same model. Future research in this area is also identified in this paper.</abstract>
	<keywords>Complexity; Optics; Parallel algorithm; Reconfigurable pipelined busSorting</keywords>
	<publication_month_year>1998-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 13, Issues 6</volumes_issues>
</paper>
<paper no=407>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The creation of a 21st century telemedical information society</paper_heading>
	<authors>Andy Marsh</authors>
	<abstract>Healthcare is a major candidate for improvement in any vision of the kinds of “information highways” and “information societies” that are now being visualised. The concept of telemedicine captures much of what is developing in terms of technology implementations, especially if it is combined with the growth of the Internet and World Wide Web (WWW). It is foreseen that the WWW will become the most important communication medium of any future information society. If the development of such a society is to be on a global scale, it should not be allowed to develop in an ad hoc manner. For this reason, the EC ISIS'95 supported Euromed project has identified 20 building blocks resulting in 39 steps requiring multi-disciplinary collaborations. Since, the organisation of information is critical especially when concerning healthcare the Euromed project has also introduced a new (global) standard called “Virtual Medical Worlds” which provides the potential to organize existing medical information and provide the foundations for its integration into future forms of medical information systems. Virtual Medical Worlds, based on 3D reconstructed medical models, utilises the WWW as a navigational medium to remotely access multi-media medical information systems. The visualisation and manipulation of hyper-graphical 3D “body/organ” templates and patient-specific 3D/4D/and VR models is an attempt to define an information infrastructure in an emerging WWW-based telemedical information society.</abstract>
	<keywords>Telemedicine; Information society; World Wide Web</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=408>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Progressive lossless coding of medical images</paper_heading>
	<authors>Paul Cristea, Jan Cornelis, Adrian Munteanu</authors>
	<abstract>The paper describes a procedure for lossless coding of medical images that uses a non-linear integer-to-integer wavelet transform generated by a lifting scheme and featuring progressive transmission capability. The quantization module implements a new modality for the coding of the wavelet coefficients that is more effective than the classical zerotree coding. Lossless image compression with progressive transmission capabilities plays a key role in medical image acquisition and handling. The contradictory requirements of exact data reconstruction possibility and of efficient and economic handling and transmission of large sets of images over large distances have to be met simultaneously. The experimental results, obtained on a set of 20 multimodal medical images, show that the algorithm outperforms other existing image lossless coding algorithms. Therefore it is a good candidate for telematics applications like teleradiology and PACS.</abstract>
	<keywords>Lossless coding; Integer wavelet transform; Progressive transmission</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=409>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visualization of 3D fields and medical data and using VRML</paper_heading>
	<authors>Branko Marovic, Zoran Jovanovic</authors>
	<abstract>This paper describes the visualization of 3D medical images and 3D fields on inexpensive workstations or personal computers. VRML 2.0 and Java language are used through the Web interface for manipulating 3D models. 3D data are typically stored and processed on powerful servers accessible by using TCP/IP. Surface rendering of CT and MRI images is done by applying the marching cube algorithm, and then simplifying the obtained triangular meshes and transforming them into VRML format. Various virtual tools may be used for field visualization and object examination and manipulation. The developed concepts and software may also be implemented for visualization of spatial fields in aerodynamics, fluid dynamics simulation, thermodynamics, etc.</abstract>
	<keywords>Medicine; Medical imaging; Field visualization; Surface reconstruction; Mesh simplification; Mesh decimation; Internet; WWW; Web; VMRL; Java</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=410>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Trusted third party services for health care in Europe</paper_heading>
	<authors>Despina Polemi</authors>
	<abstract>EUROMED-ETS was an one-year project supported by the European Commission (INFOSEC programme, DGXIII). Its goal was to secure telemedicine applications over the world wide web (WWW). The solution proposed and adopted during this project was based on the establishment of trusted third party services (TTPS) based on secure session layer (SSL). This solution was demonstrated and validated using an existing telemedicine platform. In this paper, the proposed solution will be presented, the specifications of the test bed will be provided and the pilot operation will be described.</abstract>
	<keywords>Telemedicine; Security; Trusted third party services; World Wide Web</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=411>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>TeleEEG: A telemedical software package for EEG</paper_heading>
	<authors>Luigi De Luca</authors>
	<abstract>This paper describes TeleEEG, a telemedical software application for EEG running on a Windows 95 platform; it has been developed under the Euromed project. EEG medical data are related to brain electrical activity; they consist in signals coming from electrodes typically placed on the scalp of the patient, and they are used to detect episodic brain disorders. TeleEEG can file, manage and visualise EEG data captured by digital EEG machines from different vendors. Moreover it incorporates telemedical facilities to allow remote data access and transfer among different medical centres by means of the Internet www as well as by common telephone lines.</abstract>
	<keywords>Telemedicine; Digital EEG; Telemedical software; Medical web application</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=412>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>From raw data to WWW compatible visualization and manipulation: Imaging human electric potentials</paper_heading>
	<authors>K. Delibasis, Ch. Michael, N. Mouravliansky, K. Papaodysseas</authors>
	<abstract>This paper describes a pilot system that acquires images of the electric potentials within the human body and stores the results in a format accessible and visualizable through the WWW. The proposed system uses a system of external electrodes to acquire measurements of electric potentials at the level of the skin, followed by an algorithm that reconstructs the value of the electric potential in the interior of the human body, thus producing a three-dimensional (3D) image. The main aspects of the reported work is the visualization of the results in such a way that the acquired information can be imaged through the WWW so that the diagnostically useful information can be maximized. For this reason, the voxels of the acquired electric image are labeled using a conventional CT image from the same patient whereas a triangulation algorithm (marching cubes — MC) is used to create the VRML output for information visualization.</abstract>
	<keywords>Segmentation; Registration; Triangulation; Visualization; Human electrical potentials</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=413>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A simplified simulation model and virtual reality visualization of tumour growth in vitro</paper_heading>
	<authors>Georgios S. Stamatakos, Nikolaos K. Uzunoglu, Konstantinos Delibasis, Mersini Makropoulou, ... Andy Marsh</authors>
	<abstract>The aim of this paper is to demonstrate the impact that modern visualization techniques can have on the simulation of biological processes such as tumour growth. Therefore, a simplified three-dimensional cytokinetic simulation model of tumour growth in vitro has been developed. Results concerning the development of a small cell lung cancer (SCLC) tumour spheroid in cell culture have been obtained and their behaviour has been compared with both published data and laboratory experience. A special visualization system allowing real time surface and volume rendering on inexpensive computer hardware has been proposed. Its application to the model has led to a spectacular three-dimensional visualization of both the external and the internal structure of a growing tumour spheroid.</abstract>
	<keywords>Tumour growth modelling; Cancer; Simulation; Virtual reality; Visualization</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=414>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual reality in medicine and biology</paper_heading>
	<authors>J.J. Camp, B.M. Cameron, D. Blezek, R.A. Robb</authors>
	<abstract>Medical and biological applications of virtual reality technology serve a wide range of basic research, educational, diagnostic and surgical planning purposes. All these applications rely on accurate, efficient, 3D models of human anatomy, extracted from sectional image data, although the level of required interactivity and “realism” varies widely between applications. In this paper, we describe our approach to creating tiled surface models from volumetric data, using examples of both normal anatomy as represented by the NLM Visible Human Project, and pathological anatomy from CT and MRI images of patients. We also describe how applications requiring varying levels of complexity and interactivity can be accommodated by making VR resources such as tracking and display devices available to multiple systems via local networking. Our experience with anesthesia simulation, virtual endoscopy, prostate and neurological surgery planning, as well as 3D histological analysis of tissues in the eye, prostate cancer and studies of single neurons demonstrates the broad scope and variety of applications of virtual reality technology in medicine and biology.</abstract>
	<keywords>Virtual reality; Virtual endoscopy; Surgery simulation; 3D Microscopic reconstruction; Anatomic modeling; Volume rendering</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=415>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A computational framework for telemedicine</paper_heading>
	<authors>Ian Foster, Gregor von Laszewski, George K Thiruvathukal, Brian Toonen</authors>
	<abstract>Emerging telemedicine applications require the ability to exploit diverse and geographically distributed resources. Highspeed networks are used to integrate advanced visualization devices, sophisticated instruments, large databases, archival storage devices, PCs, workstations, and supercomputers. This form of telemedical environment is similar to networked virtual supercomputers, also known as metacomputers. Metacomputers are already being used in many scientific application areas. In this article, we analyze requirements necessary for a telemedical computing infrastructure and compare them with requirements found in a typical metacomputing environment. We will show that metacomputing environments can be used to enable a more powerful and unified computational infrastructure for telemedicine. The Globus metacomputing toolkit can provide the necessary low level mechanisms to enable a large scale telemedical infrastructure. The Globus toolkit components are designed in a modular fashion and can be extended to support the specific requirements for telemedicine.</abstract>
	<keywords>Globus toolkit; Telemedicine; Metacomputing</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=416>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The virtual temporal bone, a tele-immersive educational environment</paper_heading>
	<authors>Mary Rasmussen, Theodore P. Mason, Alan Millman, Ray Evenhouse, Daniel Sandin</authors>
	<abstract>This paper describes the development of the virtual temporal bone, a new method of teaching the complex anatomy of the middle and inner ear utilizing virtual reality and current networking technology. Virtual reality technology allows a better visualization of these three-dimensional structures than conventional media because it supports stereo vision, viewer-centered perspective, large angles of view, and interactivity. Two or more ImmersaDeskTM systems, drafting table format virtual reality displays, are networked together providing an environment where teacher and student share a high quality three-dimensional anatomical model, and are able to converse and to point in three dimensions to indicate areas of interest. This project was realized by the teamwork of otologic surgeons, medical artists and sculptors, computer scientists and computer visualization experts.</abstract>
	<keywords>Virtual reality; Virtual temporal bone; Tele-immersive education</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=417>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual Medical Worlds Magazine — A Euromed on-line information building block for HPCN related telemedicine</paper_heading>
	<authors>Ad Emmen, Leslie Versweyveld</authors>
	<abstract>Telemedicine is a new keyword entering the medical vocabulary. The concept is dealing with the implementation of high performance computing and networking in the field of health care. The Euromed project is largely contributing to the structural organising of the 21st century telemedical information society in the creation of 20 building blocks resulting in 39 concrete steps. One of these initiatives is Virtual Medical Worlds Magazine, a monthly news site on the world wide web to disseminate telemedical information to a broad audience. It uses new Internet based technology. The magazine, its organisation and technical implementation are discussed.</abstract>
	<keywords>Telemedical information Society; Telemedicine building blocks; Virtual Medical Worlds Magazine; HPCN; Publishing</keywords>
	<publication_month_year>1998-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 1–2</volumes_issues>
</paper>
<paper no=418>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Making virtual reality useful: A report on immersive applications at Iowa State University</paper_heading>
	<authors>Carolina Cruz-Neira</authors>
	<abstract>This paper presents the research activities in virtual reality (VR) applications at the Iowa Center for Emerging Manufacturing Technology at Iowa State University. We are performing multidisciplinary applied research focused on investigating the potential of immersive environments for science and engineering applications. We present our most recent applications, currently under development, and provide an informal discussion about the features and research topics we are identifying.</abstract>
	<keywords>Virtual reality; Engineering applications; Interactive visualization</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=419>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual reality for large-scale industrial applications</paper_heading>
	<authors>Jon Cook, Roger Hubbold, Martin Keates</authors>
	<abstract>In this paper we present work undertaken by the Advanced Interfaces Group at the University of Manchester into the use of virtual reality (VR) for large-scale industrial applications, the benefits this can bring, and the problems of applying VR to large problems. We show how these problems are being addressed by the MAVERIK system under development within the group. MAVERIK is a VR system which can be tightly integrated into an existing application such that it can exploit that application's data structures, algorithms and semantics.</abstract>
	<keywords>Virtual reality system architecture; Industrial applications; Process plant; Engineering</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=420>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A highly flexible virtual reality system</paper_heading>
	<authors>Roland Blach, Jürgen Landauer, Angela Rösch, Andreas Simon</authors>
	<abstract>Virtual reality (VR) systems utilize more and more input and output media in order to make virtual environments more realistic. Furthermore, VR developers nowadays concentrate on rich interaction and behavior. This causes the need for a new type of VR development system architecture. In this paper such a new architecture is proposed and discussed. Its key features include support for new input and output media, device independence, and rapid behavioral prototyping. The Lightning VR system is introduced as a prototype for the presented architecture. It has already shown its usefulness in several industrial applications as design reviews, marketing events, assembly planing, etc.</abstract>
	<keywords>Virtual reality; Virtual environments; Virtual reality development system; Authoring system; Immersive environments</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=421>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A hierarchical virtual environment for a machine fault diagnostic application</paper_heading>
	<authors>Pekka Ala-Siuru, Juha Takalo, Jari Ensomaa, Johan Plomp</authors>
	<abstract>In this paper we describe a virtual environment (VE) which has been implemented for a machine fault diagnostic application which is running on-line in Rautaruukki Raahe Steel plant, Raahe Finland. This hierarchical virtual environment consists of three levels in which information is provided to the plant operators: (1) general guidance for navigating in the virtual plant environment, (2) more specific level for the important serviceable objects and (3) diagnostic and hypertext document application level. The system is implemented in a wireless wearable computer environment which is also discussed. Here we present the experiences of the first virtual prototype system.</abstract>
	<keywords>Virtual environments; Virtual reality; Fault diagnostics; Wearable computing Hierarchical multimedia interface</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=422>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Supporting 3D and VR applications in a metacomputing environment</paper_heading>
	<authors>Ad Emmen, Martijn Mulder, Ion Barosan</authors>
	<abstract>3D and VR applications require large amounts of computing time. In most applications it has to be available in a specific time span. In Cave or Immersive desk environments the power has to be available real-time. Much preparation can, however, be done on a longer time scale. Complex 3D applications in for instance the media industry require large computation power for rendering purposes in a timescale ranging from hours to a few days. Metacomputing environments, including a large number of machines, can be a useful tool for supporting these VR and 3D applications. Within the EROPPA project, a software environment for use by post production companies has been developed. Currently extensions to virtual surgery applications are investigated.</abstract>
	<keywords>Metacomputing; 3D Animation; Rendering; Codine</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=423>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The VEplatform system: A system for distributed virtual reality</paper_heading>
	<authors>Kris Demuynck, Jan Broeckhove, Frans Arickx</authors>
	<abstract>The VEplatform project aims at the development of a scaleable system which allows distributed virtual reality applications to be built. The system is decentralised, i.e. there is no central component which controls it and which can become a bottleneck. Entities of a virtual environment are represented by processes which are distributed over a network of computers. As a human participant (or “viewer”) is also regarded as an entity there is no need to treat it separatly and therefore the system retains a simple structure. In this paper the working principles of the VEplatform system and its performance are discussed.</abstract>
	<keywords>Distributed virtual reality; Communication protocols; Load-balancing</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=424>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational steering in the CAVE</paper_heading>
	<authors>Jurriaan D. Mulder, Robert van Liere, Jarke J. van Wijk</authors>
	<abstract>Scientists can gain much more insight from their simulations if they are enabled to change simulation parameters on the fly while observing the results immediately. A crucial aspect of such computational steering is an intuitive user interface. We have developed an environment that enables researchers to construct such interfaces efficiently and effectively for graphical workstations. In this paper we report on our next step towards more intuitive user-interfaces: We have modified our system for use in the CAVE. The CAVE is a projection-based virtual environment. Virtual environments are designed to provide the effect of immersion in an interactive three-dimensional computer-generated environment. We show that the use of virtual environments for computational steering interfaces can improve interaction with the simulation and immersion in the computational process. We present our system, the methods we have developed for improved 3D interaction, and describe three applications.</abstract>
	<keywords>Computational steering; Virtual environments; 3D graphics and interaction</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=425>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A virtual environment for the exploration of diffusion and flow phenomena in complex geometries</paper_heading>
	<authors>Robert G. Belleman, Jaap A. Kaandorp, Peter M.A. Sloot</authors>
	<abstract>With the development of high-performance computing techniques and the increase in availability of computer power, large and complex time dependent data sets are generated in simulations used in industrial and scientific applications. These data sets are not only large, but they also represent results of simulations of increasingly complex phenomena which often vary dynamically. In many cases, visual exploration of these complex data sets is one of the few options to analyze these data and to obtain further insight in the simulated phenomena. As a test case in the development of an immersive virtual exploration environment we have used data sets resulting from 3D simulations of diffusion and flow processes, and their impact on biological growth. We show that an immersive virtual environment such as the CAVE aids in the interactive exploration of the large scale and time dependent data sets that result from these simulations. In addition, the CAVE has found to be a suitable environment to compare shapes emerging in simulated biological growth processes with data sets obtained from CT scans of the actual objects.</abstract>
	<keywords>Interactive data exploration; Virtual environments; Simulation of flow and diffusion</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=426>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A scalable virtual environment for large scale scientific data analysis</paper_heading>
	<authors>Dirk Rantzau, Ulrich Lang</authors>
	<abstract>The growing complexity and sheer amount of data resulting from supercomputer simulations increases the need for new and intuitive analysis techniques such as VR. In this paper we present a scalable VR system integrated in a distributed and collaborative visualization and simulation environment which can be used for scientific data analysis and computational steering in several application domains. After discussing the requirements for such a system we describe the basic architecture of the VR environment. We then give some examples of currently investigated application domains in the areas of water turbine optimization and space vehicle re-entry and comet impact simulation involving metacomputing on a trans-atlantic scale.</abstract>
	<keywords>Collaborative virtual environments; Scientific visualization; Metacomputing; Computational steering</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=427>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Dynamic radiosity shadows for interactive virtual environments</paper_heading>
	<authors>Frank Schöffel</authors>
	<abstract>Current state-of-the-art virtual reality (VR) systems provide realistic illumination only for static scenes, since shadows are calculated off-line and cannot be updated at interactive rates. This paper presents a new method for incorporating realistic radiosity shadows also in interactive VR applications. A radiosity process and the VR rendering process are coupled. User interactions trigger the radiosity process, which updates the illumination by providing corrected vertex colors. To speed up radiosity updates, coherences are exploited. Thus, on-line updated radiosity shadows can be integrated into interactive, dynamic virtual environments, providing a high degree of visual realism.</abstract>
	<keywords>Virtual reality; Radiosity Soft shadows; Scene manipulation; Shadow update</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=428>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Navigating through a virtual city: Using virtual reality technology to study human action and perception</paper_heading>
	<authors>Hendrik A.H.C. van Veen, Hartwig K. Distler, Stephan J. Braun, Heinrich H. Bülthoff</authors>
	<abstract>The introduction of virtual reality technology in the field of human perception and behaviour research has spawned many new research initiatives. The paper outlines the motivations of researchers in this field to start using virtual environments for their studies by presenting two such studies conducted in our laboratory. First, we discuss how we are building a large virtual model of the city of Tübingen and how we are using it for our research on human navigation behaviour. Second, we present data on the phenomenon that observers tend to underestimate the perceived speed of their movement through a virtual environment, and we discuss what implications these results have for the design of virtual environments.</abstract>
	<keywords>Virtual reality; Human behaviour; Perception; Navigation; Biological cybernetics</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=429>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The development of an arthroscopic surgical simulator with haptic feedback</paper_heading>
	<authors>J.W. Ward, D.P.M. Wills, K.E Sherman, A.M.M.A. Mohsen</authors>
	<abstract>Interest in improved training methods for minimally invasive surgery (MIS) has led to the development of a number of computer graphics based surgical simulators. These provide a safe environment for training in surgical procedures and offer potential advantages over conventional techniques. This paper starts with a brief review of existing work in the area, then goes on to describe a surgical simulator for Knee arthroscopy developed at the University of Hull. This provides real time simulation of the arthroscopic view, collision detection between instruments and objects within the joint, interaction with deformable objects and limited haptic feedback through the instruments.</abstract>
	<keywords>Knee arthroscopy; Minimally invasive; Surgical simulation; Surgical training; Haptic feedback</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=430>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VR in medicine: Virtual colonoscopy</paper_heading>
	<authors>A. Marsh, F. Simistira, R. Robb</authors>
	<abstract>Colon/rectal cancer is the second most common cause of death, yet among the most preventable when detected in its early stages. The traditional diagnostic procedures cause tremendous discomfort and are deeply invasive. The motivation for this work is firstly to develop an alternative technique to visualise the inner mucosal surface of the colonic wall. This technique will be based on three-dimensional (3D) visualisation and virtual reality to perform virtual endoscopy. However, there is a requirement of vast computational support. Therefore, secondly, this paper will discuss the possibilities adopting high performance computing and networking to support virtual reality medical applications.</abstract>
	<keywords>Virtual reality; WWW; Telemedicine; Virtual colonoscopy</keywords>
	<publication_month_year>1998-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 3–4</volumes_issues>
</paper>
<paper no=431>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computing with proteins in a dynamic regime</paper_heading>
	<authors>J.L. Fernández-Villacañas, J.M. Fatah, S. Amin</authors>
	<abstract>Dynamic local search [1] has been applied to the evolution of interactions between protein-like structures. These are composed of a randomly selected sequence of amino acids that are linked together to form linear polymers in three dimensions. The objective function chosen for optimisation is the potential energy given by a Toy protein model. Proteins fold, move and interact with other chains to minimise their objective function at a given rate, Frate, depending on the sum of the rates for re-organisation of their structures. The interaction between different proteins gives a whole range of local attraction/repulsion regimes that result in new structures with new bonds, broken bonds and recursive loops.</abstract>
	<keywords>Optimisation; Evolution; Computation</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=432>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel evolutionary algorithm for the vehicle routing problem with heterogeneous fleet</paper_heading>
	<authors>Luiz S. Ochi, Dalessandro S. Vianna, Lúcia M.A. Drummond, AndréO. Victor</authors>
	<abstract>Nowadays genetic algorithms stand as a trend to solve NP-complete and NP-hard problems. In this paper, we present a new hybrid metaheuristic which uses parallel genetic algorithms and scatter search coupled with a decomposition-into-petals procedure for solving a class of vehicle routing and scheduling problems. The parallel genetic algorithm presented is based on the island model and its performance is evaluated for a heterogeneous fleet problem, which is considered a problem much harder to solve than the homogeneous vehicle routing problem.</abstract>
	<keywords>Parallel algorithms; Metaheuristics; Vehicle routing problem</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=433>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Arranging statements and data of program instances for locality</paper_heading>
	<authors>Claudia Leopold</authors>
	<abstract>In memory hierarchies, programs can be speeded up by increasing their degree of locality. One human approach to locality optimization considers several small program instances of a given program, optimizes the instances for locality, and generalizes the structure of the solutions to the program. The paper suggests a semi-automatic locality-optimization method based on this approach. The major contribution is a local search algorithm that automatically optimizes program instances via combined code and data transformations. The algorithm uses a novel objective function that quantifies the intuitive notion of locality. Experimental results indicate that our method compares well with current compiler optimizations.</abstract>
	<keywords>Data locality; Memory hierarchies; Locality optimization; Instance-based methods</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=434>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Feedforward backpropagation artificial neural networks on reconfigurable meshes</paper_heading>
	<authors>John Jingfu Jenq, Wingning Li</authors>
	<abstract>The artificial neural networks (ANNs) have been used successfully in applications such as pattern recognition, image processing, automation and control. Majority of today's applications use backpropagate feedforward ANN. In this paper, two methods of P pattern L layer ANN learning on n × n RMESH have been presented. One required memory space of O(nL) but conceptually is simpler to develop and the other uses pipelined approach which reduces the memory requirement to O(L). Both of these algorithms take O(PL) time and are optimal for RMESH architecture.</abstract>
	<keywords>Artificial neural networks; Reconfigurable mesh algorithms; Parallel algorithms</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=435>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A molecular quasi-random model of computations applied to evaluate collective intelligence</paper_heading>
	<authors>Tadeusz Szuba</authors>
	<abstract>This paper presents a bio-inspired model of computations, the random PROLOG processor (RPP), used for analysis of collective intelligence (CI). In the RPP, clause_molecules (CMs) of facts, rules, goals, or higher-level logical structures enclosed by membranes move quasi-randomly in structured computational_PROLOG_space (CS). When CMs rendezvous, an inference process can occur iff the logical conditions are fulfilled. CI can be evaluated as follows: (1) the mapping is done of a given social structure into the RPP; (2) the beings and their behavior are translated into PROLOG expressions, carried by CMs; (3) the goal(s) of the social structure are translated into N-element inference (NEI); (4) the efficiency of the NEI is evaluated and given as the intelligence quotient of a social structure (IQS) projected onto NEI.</abstract>
	<keywords>Collective intelligence; IQ measure; Social structure; PROLOG; Model of computations; Brownian movements</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=436>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Replicated shared object model for parallel edge detection algorithm based on spiral architecture</paper_heading>
	<authors>Xiangjian He, Tom Hintz, Ury Szewcow</authors>
	<abstract>Edge detection in computer vision and image processing is a process which detects one kind of significant feature in an image that appears as discontinuities in intensities. A parallel edge detection algorithm based on spiral architecture is designed in this paper using a replicated shared object model (RSOM). We use this approach to meet the requirements of replicated object management, update propagation, underlying communication and consistency maintenance among replicated objects. Multi-scale image theory plays an important role in this research.</abstract>
	<keywords>Parallel processing; Spiral architecture; Dynamic modelling; Edge detection; Multi-scale theory</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=437>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scheduling tasks of a parallel program in two-processor systems with use of cellular automata</paper_heading>
	<authors>F. Seredyński</authors>
	<abstract>In this paper, a cellular automaton (CA) is proposed as a tool for designing distributed scheduling algorithms for allocating parallel program tasks in multiprocessor systems. For this purpose, a program graph is considered as a CA containing elementary automata interacting locally according to some rules. In the first phase of the algorithm, effective rules for the CA are discovered by a genetic algorithm. In the second phase, the CA works as a distributed scheduler. In this phase, for any initial allocation of tasks in a multiprocessor system, the CA-based scheduler finds an allocation minimizing the total execution time of the program in a given system topology. The effectiveness of the proposed scheduling algorithm is shown for a number of program graphs scheduled in a two-processor system.</abstract>
	<keywords>Cellular automata; Scheduling tasks; Parallel program</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=438>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evolving processes and evolution schedulers for concurrent scheduling controls and parallel evolutionary computation</paper_heading>
	<authors>Tzilla Elrad, Jinlong Lin</authors>
	<abstract>In this paper we present concepts of evolution scheduling, which will provide us with a better scheduling mechanism for competing processes in the ready-queue, the blocked-queue, the pc (priority control) queue, and the cs (client-and-server) queue. The evolution scheduling is acceptable for the soft real-time system and useful for its performance to achieve fair and effective scheduling. On the other hand, for those who pursue evolutionary computation, this is good news. It allows them to carry out natural competition in an easy way consistent with their operating system and programming language.</abstract>
	<keywords>Evolving process; Evolution scheduling; Parallel evolutionary computation; Concurrent scheduling control</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=439>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An evolutionary approach to multiprocessor scheduling of dependent tasks</paper_heading>
	<authors>Roman Nossal</authors>
	<abstract>The scheduling of application tasks is a problem that occurs in all multiprocessor systems. This problem has been shown to be NP-hard if the tasks are not independent but are interrelated by mutual exclusion and precedence constraints. This paper presents an approach for pre-runtime scheduling of periodic tasks on multiple processors for a real-time system that must meet hard deadlines. The tasks can be related to each other by mutual exclusion and precedence forming an acyclic graph. The proposed scheduler is based on genetic algorithms, which relieves the user from knowing how to construct a solution. Consequently, the paper focuses on the problem encoding, i.e., the representation of the problem by genes and chromosomes, and the derivation of an appropriate fitness function. The main benefit of the approach is that it is scalable to any number of processors and can easily be extended to incorporate further requirements.</abstract>
	<keywords>Real-time systems; Multiprocessor schedulingGenetic algorithms</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=440>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multiprocessor scheduling using mean-field annealing</paper_heading>
	<authors>Shaharuddin Salleh, Albert Y. Zomaya</authors>
	<abstract>This paper presents our work on the static task scheduling model using the mean-field annealing (MFA) technique. Meanfield annealing is a technique of thermostatic annealing that takes the statistical properties of particles as its learning paradigm. It combines good features from the Hopfield neural network and simulated annealing, to overcome their weaknesses and improve on their performances. Our MFA model for task scheduling is derived from its prototype, namely, the graph partitioning problem. MFA is deterministic in nature and this has the advantage of faster convergence to the equilibrium temperature, compared to simulated annealing. Our experimental work verifies this finding, besides making comparison on the effectiveness of the model on various network and task graph sizes. Our work also includes the simulation of the MFA model on several network topologies using varying parameters. The MFA simulation model is targeted on nonpreemptive and precedence-related tasks with communication costs.</abstract>
	<keywords>Task scheduling; Mean-field annealing; Graph partitioning</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=441>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using the BSP cost model to optimise parallel neural network training</paper_heading>
	<authors>R.O Rogers, D.B Skillicorn</authors>
	<abstract>We derive cost formulae for three different parallelisation techniques for training both supervised and unsupervised networks. These formulae are parameterised by properties of the target computer architecture. It is therefore possible to decide both which technique is best for a given parallel computer, and which parallel computer best suits a given technique. One technique, exemplar parallelism, is far superior to almost all parallel computer architectures. Formulae also take into account optimal batch learning as the overall training approach. Cost predictions are made for several of today's popular parallel computers.</abstract>
	<keywords>Data mining; Neural networks; Parallelism; Bulk synchronous parallelism (BSP); Cost analysis; Supervised learning; Unsupervised learning; Batch learning; Deterministic learning; Stochastic learning</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=442>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A fault-tolerant parallel heuristic for assignment problems</paper_heading>
	<authors>E.-G. Talbi, Z. Hafidi, D. Kebbal, J.-M. Geib</authors>
	<abstract>This paper presents a new approach for parallel heuristic algorithms based on adaptive parallelism. Adaptive parallelism was used to dynamically adjust the parallelism degree of the application with respect to the system load. This approach demonstrates that high-performance computing using a hundred of heterogeneous workstations combined with massively parallel machines is feasible to solve large optimization problems with respect to the personal character of workstations. The fault-tolerant algorithm allows a minimal loss of computation in case of failures. The proposed algorithm exploits the properties of this class of applications in order to reduce the complexity of the algorithm in terms of the checkpoint files size and the control messages exchanged. The parallel heuristic algorithm combines different search strategies: simulated annealing and tabu search. Encouraging results have been obtained in solving the quadratic assignment problem. We have improved the best known solutions for some large real-world problems.</abstract>
	<keywords>Heuristic algorithms; Tabu search Simulated annealing; Adaptive parallelism; Fault-tolerance; Quadratic assignment problem</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=443>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Annealing-based heuristics and genetic algorithms for circuit partitioning in parallel test generation</paper_heading>
	<authors>C. Gil, J. Ortega, A.F. Díaz, M.D.G. Montoya</authors>
	<abstract>In this paper simulated annealing and genetic algorithms are applied to the graph partitioning problem. These techniques mimic processes in statistical mechanics and biology, respectively, and are the most popular meta-heuristics or general-purpose optimization strategies. A hybrid algorithm for circuit partitioning, which uses tabu search to improve the simulated annealing meta-heuristics, is also proposed and compared with pure tabu search and simulated annealing algorithms, and also with a genetic algorithm. The solutions obtained are compared and evaluated by including the hybrid partitioning algorithm in a parallel test generator which is used to determine the test patterns for the circuits of the frequently used ISCAS benchmark set.</abstract>
	<keywords>Circuit partitioning; Genetic algorithms; Parallel test-pattern generation; Simulated annealing; Tabu search</keywords>
	<publication_month_year>1998-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 14, Issues 5–6</volumes_issues>
</paper>
<paper no=444>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Architectures for real-time volume rendering</paper_heading>
	<authors>Hanspeter Pfister</authors>
	<abstract>Over the last decade, volume rendering has become an invaluable visualization technique for a wide variety of applications. This paper reviews three special-purpose architectures for interactive volume rendering: texture mapping, VIRIM, and VolumePro. Commercial implementations of these architectures are available or underway. The discussion of each architecture will focus on the algorithm, system architecture, memory system, and volume rendering performance.</abstract>
	<keywords>Computer graphics; Volume rendering; Hardware architecture; Graphics processors; Special-purpose and application-level systems</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=445>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Compression methods for visualization</paper_heading>
	<authors>Markus H. Gross, Lars Lippert, Oliver G. Staadt</authors>
	<abstract>Compression methods have become of fundamental importance in almost every subfield of scientific visualization. However, unlike image compression, advanced visualization applications impose manifold constraints on the design of appropriate algorithms, where progressiveness, multiresolution or topology preservation are some of the key issues. This paper demonstrates the importance of multiresolution compression methods for visualization using two examples: The first, compression domain volume rendering, enables one to visualize volume data progressively and instantaneously from its compressed data format and has been designed for WWW and networked applications. The second one is a multiresolution compression and reconstruction method that allows for progressive coding, transmission and geometric reconstruction of surfaces and volumes. Both of the presented methods are the so-called transform coding schemes and use wavelets for data representation.</abstract>
	<keywords>Wavelets; Multiresolution methods; Volume rendering; Mesh simplification; Compression</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=446>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multiresolution and hierarchical methods for the visualization of volume data</paper_heading>
	<authors>T. Ertl, R. Westermann, R. Grosso</authors>
	<abstract>As three-dimensional data sets resulting from simulations or measurements become available at ever growing sizes the need for visualization tools which allow the inspection and the analysis of these data sets at interactive rates is increasing. One way to deal with the complexity is the compression of the data in such a way that the number of cells which have to be processed by the visualization mapping is reduced. Since this compression will be lossy, it is up to the user to choose between quality or speed. The decision will usually be made interactively requiring fast access to a complete hierarchy of representations of the data set at various levels of resolution. Two different approaches and visualization algorithms based upon them are presented in this paper: wavelet analysis deriving a hierarchy of coarser representations from the original data set and multilevel finite elements generating successively refined tetrahedral grids from an initially coarse triangulation.</abstract>
	<keywords>Multiresolution; Visualization; Volume rendering</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=447>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Recent numerical methods – A challenge for efficient visualization</paper_heading>
	<authors>M. Rumpf</authors>
	<abstract>Together with a rapid development of computer hardware, sophisticated, efficient numerical algorithms allow simulation computations of complex physical phenomena. Methods, such as finite volume, multigrid finite element schemes, sparse grid, wavelet approaches, and particle methods or gridless discretizations, all carry data structures, which are tailored to the respective method. These data structures reflect the decomposition of the function space as well as the decomposition in physical space. In this paper an efficient multiresolutional visualization approach is described, which tries to reuse as much of the hierarchical structure in the numerical data as possible. The duality between grid and function space, both carrying an intrinsic hierarchical structure, is explained as the key issue of the approach. Furthermore, a general method of local error measurement is discussed, which allows a reliable representation of the desired multiresolutional data. Finally, the method of spatial, hierarchical subdivision combined with the procedural recovery of the local function spaces is presented to address fairly general numerical data. This leads to a visualization beyond prescribed data formats.Examples from various numerical methods and different data bases underline the applicability of the proposed concept.</abstract>
	<keywords>Visualization; Finite element; Finite volume; Sparse grid; Wavelet</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=448>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Interactive, knowledge-guided visualization of 3D medical imagery</paper_heading>
	<authors>N Ezquerra, L de Braal, E Garcia, C Cooke, E Krawczynska</authors>
	<abstract>One of the most important and difficult decision-making tasks is that of interpreting three-dimensional (3D) medical imagery. This information-intensive task typically requires expediency, accuracy and reliability in both the visual presentation as well as in the interpretation of complex information. We present an approach that facilitates this task by using domain knowledge to assist in the interpretation and visualization of 3D cardiac imagery. The objective is to provide the clinician with a more efficient, reliable, comprehensive, and clinically useful manner with which to accurately interpret and display large amounts of complex information. These objectives are met by capturing and representing the visual reasoning process in a computational model, and providing the user with an intuitive graphical and textual representation of the data, as well as explanations, justifications, and interactive data visualizations. The approach is illustrated with cardiovascular 3D SPECT tomographic perfusion imagery, a technique aimed at diagnosing heart disease.</abstract>
	<keywords>Visualization; 3D image visualization; Knowledge-based systems; Medical imaging</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=449>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visualization of dynamical systems</paper_heading>
	<authors>Eduard Gröller, Helwig Löffelmann, Rainer Wegenkittl</authors>
	<abstract>The visualization of analytically defined dynamical systems is important for a thorough understanding of the underlying system behavior. An introduction to analytically defined dynamical systems is given. Various visualization techniques for dynamical systems are discussed. Several current research directions concerning the visualization of dynamical systems are treated in more detail. These are: texture-based techniques, visualization of high-dimensional dynamical systems, advanced streamsurface representations, local analyses – Poincaré sections, visualizing econometric models.</abstract>
	<keywords>Dynamical system; Visualization; Texture; Multidimensional data; Parallel coordinates; Streamsurface; Econometric model; Poincaré section</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=450>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Global, geometric, and feature-based techniques for vector field visualization</paper_heading>
	<authors>Frits H. Post, Wim C. de Leeuw, I. Ari Sadarjoen, Freek Reinders, Theo van Walsum</authors>
	<abstract>ector field visualization techniques are subdivided into three categories: global, geometric, and feature-based techniques. We describe each category, and we present some related work and an example in each category from our own recent research. Spot noise is a texture synthesis technique for global visualization of vector fields on 2D surfaces. Deformable surfaces is a generic technique for extraction and visualization of geometric objects (surfaces or volumes) in 3D data fields. Selective and iconic visualization is an approach that extracts important regions or structures from large data sets, calculates high-level attributes, and visualizes the features using parameterized iconic objects. It is argued that for vector fields a range of visualization techniques are needed to fulfill the needs of the application.</abstract>
	<keywords>Vector field visualization; Global techniques; Spot noise; Geometric techniques; Deformable surfaces; Feature extraction techniques</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=451>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visualizing evolving scalar phenomena</paper_heading>
	<authors>Deborah Silver, Xin Wang</authors>
	<abstract>Visualizing 3D continuum or time-varying simulation (scalar) datasets is difficult because of the immense amount of data to be processed and understood. Furthermore, these datasets may contain many evolving amorphous regions, making it difficult to visually follow features of interest. In this paper, we present a process for analyzing continuum datasets and review some of our previous work on feature tracking. We show how the tracking information can be used to enhance standard rendering and enable new visualizations. The tracking is demonstrated on different application domains including turbulence, weather and inlet design.</abstract>
	<keywords>Scientific visualization; Multi-dimensional visualization; Feature tracking; Computer vision; CFD; Isosurfaces; Volume rendering</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=452>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visualization within environments supporting human communication</paper_heading>
	<authors>H.-G. Pagendarm</authors>
	<abstract>An overview about recent research in distributed, co-operative or WWW-based software involving visualization components reveals a variety of approaches ranging from remote simulation over computer supported co-operative engineering to service engines on the web. Visualization plays a key role in improving the communication of concepts or data contents. Tight coupling of visualization software with communication systems lowers the access barrier for communicating the hidden features within data or information spaces in greater detail.</abstract>
	<keywords>Visualization; Collaborative working; Virtual reality; WWW</keywords>
	<publication_month_year>1999-02-12 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 1</volumes_issues>
</paper>
<paper no=453>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A survey of computational steering environments</paper_heading>
	<authors>Jurriaan D. Mulder, Jarke J. van Wijk, Robert van Liere</authors>
	<abstract>Computational steering is a powerful concept that allows scientists to interactively control a computational process during its execution. In this paper, a survey of computational steering environments for the on-line steering of ongoing scientific and engineering simulations is presented. These environments can be used to create steerable applications for model exploration, algorithm experimentation, or performance optimization. For each environment the scope is identified, the architecture is summarized, and the concepts of the user interface are described. The environments are compared and conclusions and future research issues are given.</abstract>
	<keywords>Computational steering</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=454>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The establishment of a pilot telemedical information society</paper_heading>
	<authors>Andy Marsh</authors>
	<abstract>National and international telecommunication infrastructures have been set up through Europe to facilitate the movement of information. One major benefactor of the improved communication infrastructures is the health care community. The accessibility and interoperability of medical information systems is one of the grand challenges for the 21st century. Within Europe current developments in the application of telemedicine are being defined in separate initiatives. There are a number of pilot actions concentrating on various aspects of telemedicine. These actions involving the introduction of new technology or working practices rarely fail for technology related problems. However, in order to fully assess the likely take-up of telemedical technologies it is vital that all the aspects including non-technical are also addressed. This paper describes how a complete pilot telemedical information society will be set up which facilitates to support secure and standardised remote diagnosis, teleconsultations and advanced medical facilities in a number of sectors covering a crucial spectrum of those required to support a complete telemedical information society. This pilot testbed will then be assessed in the context of a European environment identifying a business plan for its extension to other member states therefore promoting a truly international telemedical information society for the 21st century.</abstract>
	<keywords>Telemedicine; Information society; World Wide Web</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=455>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The ITIS’98 forums</paper_heading>
	<authors>Ad Emmen, Leslie Versweyveld</authors>
	<abstract>On 22nd April 1998, the second day of the ITIS’98 Conference in Amsterdam, three discussion forums were organized in order to get people talking and plans laid down on how a telemedical information society for the future should be implemented. The forum members covered a variety and diverse spectrum of technologies, and expressed their opinions on national projects. The overall aim of the forums consisted in tackling and identifying some of the major obstacles that could prevent an advanced society for health care from being realized. The ITIS’98 forums form a serious attempt to promote a consensus towards useful answers.</abstract>
	<keywords>Telemedicine; Standardization; World Wide Web; Security; Cyber-doctor</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=456>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Wavelet-based compression of medical images: Protocols to improve resolution and quality scalability and region-of-interest coding</paper_heading>
	<authors>Peter Schelkens, Adrian Munteanu, Jan Cornelis</authors>
	<abstract>The paper describes a methodology to improve the scalability support of an embedded bit stream, generated with a wavelet-based compression algorithm, and a generic protocol to handle multiple regions-of-interest (ROIs). The generic scheme, illustrated for embedded zero-tree wavelet (EZW) coding, exploits the inherent graceful degradation capabilities of wavelet-based compression methods and ensures an optimal trade-off between the image reconstruction quality and the compression ratio. Additionally, an efficient protocol is proposed to handle multiple ROIs in an interactive client-server set-up for telemedicine applications. The processing of the ROIs takes place in the wavelet domain.</abstract>
	<keywords>Wavelet-based image compression; Region-of-interest coding; Telemedicine; Resolution and quality scalability</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=457>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>MR functional cardiac imaging: Segmentation, measurement and WWW based visualisation of 4D data</paper_heading>
	<authors>K.K. Delibasis, N. Mouravliansky, G.K. Matsopoulos, K.S. Nikita, A. Marsh</authors>
	<abstract>This paper considers the problem of ventricular segmentation and visualisation from dynamic (4D) MR cardiac data covering an entire patient cardiac cycle, in a format that is compatible with the web. Four different methods are evaluated for the process of segmentation of the objects of interest: The K-means clustering algorithm, the fuzzy K-means (FKM) algorithm, self-organizing maps (SOMs) and seeded region growing algorithm. The technique of active surface is then subsequently applied to refine the segmentation results, employing a deformable generalised cylinder as geometric primitive. The final ventricular models are presented in VRML 2.0 format. The same process is repeated for all the 3D volumes of the cardiac cycle. The radial displacement between end systole and end diastole is calculated for each point of the active surface and is encoded in colour on the VRML vertex, using the RGB colour model. Using the VRML 2.0 specifications, morphing is performed showing all cardiac phases in real time. The expert has the ability to view the objects and interact with them using a simple internet browser. Preliminary results of normal and abnormal cases indicate that very important pathological situations (such as infarction) can be visualised and thus easily diagnosed and localised with the assistance of the proposed technique.</abstract>
	<keywords>4D MR cardiac images; K-means clustering algorithm; Fuzzy K-means (FKM) algorithm; Self-organizing maps (SOMs); Seeded region growing algorithm; Active surface; Surface triangulation; VRML 2.0 format</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=458>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multiresolutional distributed filtering: A novel technique that reduces the amount of data required in high resolution electrocardiography</paper_heading>
	<authors>Mihai Popescu, Paul Cristea, Anastasios Bezerianos</authors>
	<abstract>High resolution ECG analysis is widely accepted as the best non-invasive technique for the assessment of ventricular tachycardia risk in post-myocardial infarction patients. However, the standard analysis approaches involve an extensive averaging procedure which requires long data records, accompanied by the consequent efforts for storage and transmission. This paper outlines an algorithm for multiresolutional distributed filtering, that can significantly reduce the necessary amount of data. The proposed filtering method comprises three basic steps: the dyadic wavelet transform computation, the shrinkage of the wavelet coefficients using adaptive Bayesian rules, and the reconstruction of the denoised signal through the inverse wavelet transform. The performance evaluation using controlled simulation experiments revealed that the present technique could accelerate the noise reduction, preserving the diagnostic value of the signals.</abstract>
	<keywords>High resolution ECG; Wavelet transform; Bayesian wavelet shrinkage</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=459>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Nursing informatics education for the next millenium</paper_heading>
	<authors>Peter Kokol, Damjan Zazula, Viljem Brumec, Ljudmila Kolenc</authors>
	<abstract>Better health care for all and an overall health care restructuring are two among most important processes which should be introduced in the new millenium. These two processes will expose many new tasks and responsibilities for the nursing personnel, entailing information as input and also generating information as the output. It is assumed that in the near future it is to be expected that most of the information processing including data gathering decision making support, data mining and information generation will be performed by computers. To cope with this new situation the nurses have to be armed with proper knowledge and thereafter the proper education will play a very important role. In this manner we started a new European project called NICE , whose main objective is to develop a new nursing informatics curricula and the aim of this paper is to present it and its design.</abstract>
	<keywords>Nursing informatics; Curriculum development; Education; SSM</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=460>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A physically based model to simulate maxillo-facial surgery from 3D CT images</paper_heading>
	<authors>Alessandro Sarti, Roberto Gori, Claudio Lamberti</authors>
	<abstract>Computer-based surgery simulation represents a rapidly emerging and increasingly important area of research that combines a number of disciplines for the common purpose of improving health care. Generally, the goal of computer-based surgery simulation is to enable a surgeon to experiment with different surgical procedures in an artificial environment. This paper describes an approach for elastic modelling of human tissue based on the use of embedded boundary condition techniques. Embedded boundary condition models allow to simulate the cranio-facial surgery directly on the grid of the 3D CT image of the patient. Previously simulated operations have been performed using surface models or by using a low detailed model of the tissue volume. The approach proposed here involves complete 3D modelling of the solid highly detailed structure of the object, starting from the information present in the 3D diagnostic images. Due to the huge amount of data and the computational complexity of the problem, a parallel version of the software has been implemented on the supercomputer CRAY T3E. The application of this approach for modelling the elastic deformation of human tissue in response to movement of bones is demonstrated both on the visible human data set of the National Library of Medicine and on the CT data set of real patients.</abstract>
	<keywords>Virtual surgery; Surgery planning; Tissue displacement simulation</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=461>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Wavelet-based medical image compression</paper_heading>
	<authors>Eleftherios Kofidis, Nicholas Kolokotronis, Aliki Vassilarakou, Sergios Theodoridis, Dionisis Cavouras</authors>
	<abstract>In view of the increasingly important role played by digital medical imaging in modern health care and the consequent blow up in the amount of image data that have to be economically stored and/or transmitted, the need for the development of image compression systems that combine high compression performance and preservation of critical information is ever growing. A powerful compression scheme that is based on the state-of-the-art in wavelet-based compression is presented in this paper. Compression is achieved via efficient encoding of wavelet zerotrees (with the embedded zerotree wavelet (EZW) algorithm) and subsequent entropy coding. The performance of the basic version of EZW is improved upon by a simple, yet effective, way of a more accurate estimation of the centroids of the quantization intervals, at a negligible cost in side information. Regarding the entropy coding stage, a novel RLE-based coder is proposed that proves to be much simpler and faster yet only slightly worse than context-dependent adaptive arithmetic coding. A useful and flexible compromise between the need for high compression and the requirement for preservation of selected regions of interest is provided through two intelligent, yet simple, ways of achieving the so-called selective compression. The use of the lifting scheme in achieving compression that is guaranteed to be lossless in the presence of numerical inaccuracies is being investigated with interesting preliminary results. Experimental results are presented that verify the superiority of our scheme over conventional block transform coding techniques (JPEG) with respect to both objective and subjective criteria. The high potential of our scheme for progressive transmission, where the regions of interest are given the highest priority, is also demonstrated.</abstract>
	<keywords>Medical imaging; Image compression; Wavelet transform; Lifting; Reversible transforms; Wavelet zerotrees; Entropy coding; Selective compression; Progressive transmission</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=462>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The evolution of telemedicine</paper_heading>
	<authors>Mary Moore</authors>
	<abstract>Telemedicine is not new. It began in the 1920s. The current period of rapid growth began almost a decade ago. Although comprehensive telemedicine research and evaluation still is lacking, examining the history of telemedicine reveals a number of lessons. Applications seem to have fluctuated between complex technologies and applications and extremely simple ones. Fully integrated telemedicine will require high bandwidth and robust technologies. Store-and-forward applications transmitted through the Internet appear elegantly basic. The same societal challenges to implementation remain, regardless of technology.</abstract>
	<keywords>Telemedicine; Telehealth; Telecare; Information society</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=463>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>EEG analysis in a telemedical virtual world</paper_heading>
	<authors>Emil Jovanov, Dus̆an Starc̆ević, Aleksandar Samardz̆ić, Andy Marsh, Z̆eljko Obrenović</authors>
	<abstract>Telemedicine creates virtual medical collaborative environments. We propose here a novel concept of virtual medical devices (VMD) for telemedical applications. VMDs provide different views on biomedical recordings and efficient signal analysis. In this paper we present a telemedical EEG analysis environment based on virtual reality technologies. The same EEG signal/recording could be viewed either as a waveform or as animated topographic maps on a 3D head model. In addition to visualization, sonification is used as a secondary presentation modality. The environment is based on a VRML head model animated with Java applets, and allows the use of standard web browser.</abstract>
	<keywords>Telemedicine; Virtual reality; EEG; Multimodal presentation; Sonification</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=464>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>TTPs and biometrics for securing the payment of telemedical services</paper_heading>
	<authors>Despina Polemi</authors>
	<abstract>The use of biometric smart cards may provide strong coordinated authentication across multi-vendor telemedical applications, health care enterprises and payers. The establishment of trusted third party services (TTPs) for securing the commerce component of health care in a web-based telemedical information society is necessary for the operation of certificate-based payment systems such as SET.</abstract>
	<keywords>TTPs; Telemedicine; EUROMED</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=465>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Intranet health clinic – A web-based interactive communication environment for the continuation in health care</paper_heading>
	<authors>A Prentza, S Palamas, A Anagnostaki, D Koutsouris</authors>
	<abstract>Intranet health clinic is a two-year project supported by the European Commission (Telematics Applications Programme of DGXIII – Health Care Sector). Its goal is to cover the gaps experienced in the continuation and coordination of care all the way from the initial delivery of primary health care up to the delivery of highly specialized, tertiary health services and vice versa, where transfer of any type of relevant health data is needed, namely, among health care professionals and between them and the patients. Different health-related application domains are addressed covering the needs of treating chronic patients with frequent acute periods of exacerbation, chronic and even terminal patients with less “acute” aspects, and in principle healthy persons with an acute need of intensive information during pregnancy. In this paper, the proposed solution concerning the specification of a common shell and the access procedures to the service will be presented.</abstract>
	<keywords>Telemedicine; Internet technology; Health care; Services integration</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=466>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computer implementation of a medical diagnosis problem by pattern classification</paper_heading>
	<authors>Domenico Conforti, Luigi De Luca</authors>
	<abstract>In this paper we present a software system which can aid the medical diagnostician for the diagnosis of breast cancers. The system has been developed on a “Windows 95” platform and provides a user friendly interface, made up of windows and visualization tools. An interesting and innovative feature is represented by the telemedicine configuration of the software system, which can be run in a remote fashion, exploiting, from some remote regions, the expertize and the clinical database available in advanced medical centers. A prototype version of the software system, named CAMD (computer aided medical diagnosis) is currently being tested and validated with the collaboration of the Cytopathology Department of the Cosenza General Hospital (Calabria, Italy).</abstract>
	<keywords>Medical diagnosis; Pattern classification; Mathematical programming; Telemedicine; Web application</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=467>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An image processing and management system for radiology with telemedicine services</paper_heading>
	<authors>S Pavlopoulos, D Koutsouris</authors>
	<abstract>The morphology of Greece has a significant effect on the structure and operational characteristics of the Greek health care system. The remote location of many rural health care centers and the concentration of major hospitals in the few big cities have an effect on both the quality and availability of health care that is provided. We are developing a strategic plan that would allow hospitals and health care centers across Greece to exchange medical data in digital form and have access to telemedicine and teleconsulting facilities. Two pilot networks have been implemented. The first network is designed to allow for telemedicine and teleconsulting services in the island of Evia. The second pilot network is an Image Management and Communications Systems (IMAC) and was implemented in the Onassio Hospital in Athens. Results of the pilot demonstrators were evaluated and have been very promising for a scaled-up implementation of this pilot project.</abstract>
	<keywords>Image processing; Management system; Telemedicine services</keywords>
	<publication_month_year>1999-03-11 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 2</volumes_issues>
</paper>
<paper no=468>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Technology transfer within the ProHPC TTN at ENS Lyon</paper_heading>
	<authors>Christophe Barberet, Lionel Brunie, Frédéric Desprez, Gilles Lebourgeois, ... Karine Van Heumen</authors>
	<abstract>This article is devoted to the description of our activities related to transferring the HPCN technology to SMEs. This work is performed in the framework of the French TTN ProHPC which we briefly describe in the beginning of the paper. Then we move to a more technical description of our activities, which include seven projects within the awareness campaign PHPC-ACRA and the participation to a Best Practice prototype.</abstract>
	<keywords>High-performance computing; Parallel algorithms; Parallel applicationsIndustrial transfer; Research and development; SME</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=469>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High-performance road-vehicle optimised aerodynamic design: Application of parallel computing to car design</paper_heading>
	<authors>Matteo Beccaria, Guido Buresti, Alberto Ciampa, Giovanni Lombardi, ... Andrea Viceré</authors>
	<abstract>The HIPEROAD project has developed a software system capable of performing a semi-automatic optimisation of the shape of sport cars with respect to their aerodynamical properties. The system utilises an aerodynamic solver implemented on parallel MIMD systems, and features advanced tools for the evolution and meshing of car surfaces. The system allows one to include aerodynamic optimisation in the early stages of car design. It has been tested on a Ferrari auto model and full agreement between computed and measured aerodynamical properties was found. The code has been used for designing an optimised model with improved car safety.</abstract>
	<keywords>Car design; Aerodynamic optimisation; Parallel computing; MIMD computer systems</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=470>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A high performance simulator of the immune response</paper_heading>
	<authors>M. Bernaschi, F. Castiglione, S. Succi</authors>
	<abstract>The application of concepts and methods of statistical mechanics to biological problems is one of the most promising frontiers of computational physics. For instance Cellular Automata (CA), i.e. fully discrete dynamical systems evolving according to Boolean laws, appear to be extremely well suited to the simulation of the immune system dynamics. A prominent example of immunological CA is represented by the Celada–Seiden automaton that has proven capable of providing several new insights into the dynamics of the immune system response. In the present paper we describe a parallel version of the Celada–Seiden automaton. Details on the parallel implementation as well as performance data on the IBM SP2 parallel platform are presented and commented on.</abstract>
	<keywords>Immune response; Cellular automata (CA); Parallel virtual machine (PVM); Memory management</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=471>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High Performance Fortran for practical scientific algorithms: An up-to-date evaluation</paper_heading>
	<authors>Chris H.Q Ding</authors>
	<abstract>A suite of High Performance Fortran (HPF) coding examples of practical scientific algorithms are examined in detail, with the idea that on these simple but non-trivial examples, we can fairly well understand issues related to different data distributions, different parallel constructs, and different programming styles (static versus dynamic allocations). Coding examples include 2D stencils solution of PDEs, N-body problem, LU factorization, several vector/matrix library routines, 2D and 3D array redistribution. Performances of HPF codes are compared to hand-written Fortran codes with message passing libraries. From 1997 to 1998, HPF compilers are improved significantly such that HPF codes perform as well as Fortran+MPI codes for all the examples investigated here. However, many important peculiarities of HPF coding still exist.</abstract>
	<keywords>HPF; Distributed-memory architecture; Parallel programming language; MPI; Stencils computation; N-body problem; LU factorization; Array redistribution; FFT</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=472>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallelization of NAS benchmarks for shared memory multiprocessors</paper_heading>
	<authors>Abdul Waheed, Jerry Yan</authors>
	<abstract>This paper presents our experiences of parallelizing the sequential implementation of NAS benchmarks using compiler directives on SGI Origin2000 distributed shared memory (DSM) system. Porting existing applications to new high performance parallel and distributed computing platforms is a challenging task. Ideally, a user develops a sequential version of the application, leaving the task of porting the code to parallelization tools and compilers. Due to the simplicity of programming shared-memory multiprocessors, compiler developers have provided various facilities to allow the users to exploit parallelism. Native compilers on SGI Origin2000 support multiprocessing directives to allow users to exploit loop-level parallelism in their programs. Additionally, supporting tools can accomplish this process automatically. We experimented with these compiler directives and supporting tools by parallelizing sequential implementation of NAS benchmarks. Results reported in this paper indicate that with minimal effort, the performance gain is comparable with the hand-parallelized, carefully optimized, message-passing implementations of the same benchmarks.</abstract>
	<keywords>Parallelization; Shared memory multiprocessing; Compiler directives; Benchmarks; Performance evaluation</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=473>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Method of particles in visual clustering of multi-dimensional and large data sets</paper_heading>
	<authors>Witold Dzwinel, Jan Błasiak</authors>
	<abstract>A method dedicated for visual clustering of N-dimensional data sets is presented. It is based on the classical feature extraction technique – the Sammon’s mapping. This technique empowered by a particle approach used in the Sammon’s criterion minimization makes the method more reliable, general and efficient. To show its reliability, the results of tests are presented, which were made to exemplify the algorithm ‘immunity’ from data errors. The general character of the method is emphasized and its role in multicriterial analysis discussed. Due to inherent parallelism of the methods, which are based on the particle approach, the visual clustering technique can be implemented easily in parallel environment. It is shown that parallel realization of the mapping algorithm enables the visualization of data sets consisting of more than 104 multi-dimensional data points. The method was tested in the PVM, MPI and data parallel environments on an HP/Convex SPP/1600. In this paper, the authors compare the parallel algorithm performance for these three interfaces. The approach to visual clustering, presented in the paper, can be used in visualization and analysis of large multi-dimensional data sets.</abstract>
	<keywords>Visual clustering; Multi-dimensional data sets; Feature extraction; Method of particles; Parallel implementation</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=474>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>HPF+: High Performance Fortran for advanced scientific and engineering applications</paper_heading>
	<authors>Siegfried Benkner</authors>
	<abstract>High Performance Fortran (HPF) offers an attractive high-level interface for programming scalable parallel architectures by extending Fortran with directives for specifying data distributions and for indicating parallel execution. The current version of HPF is very well suited for regular codes but lacks important features required for an efficient parallelization of applications based on irregular grids or unstructured meshes. In this paper, we present an optimized version of high performance Fortran, called HPF+, that addresses these requirements. HPF+ provides data distribution features applicable in the context of irregular, dynamically changing data structures and access patterns. Additional mechanisms are introduced to influence the mapping of computations to processors and to reduce the runtime preprocessing and communication overheads of irregular loops. Without sacrificing required functionality, HPF+ adopts a data distribution model based on a one-level mapping of data to processors, reducing the complexity of the language, while alleviating the compiler’s task of producing an efficient parallel program.</abstract>
	<keywords>High performance Fortran; Language extensions; Data parallelism; Irregular codes; Communication schedule reuse</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=475>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multiple data parallelism with HPF and KeLP</paper_heading>
	<authors>John Merlin, Scott Baden, Stephen Fink, Barbara Chapman</authors>
	<abstract>High Performance Fortran (HPF) is an effective language for implementing regular data parallel applications on distributed memory architectures, but it is not well suited to irregular, block-structured applications such as multiblock and adaptive mesh methods. A solution to this problem is to use an SPMD program to coordinate multiple concurrent HPF tasks, each operating on a regular subgrid of the multiblock domain. This paper presents such a system, in which the coordination layer is provided by the C++ class library KeLP. We describe the KeLP–HPF implementation and programming model, and show an example KeLP–HPF multiblock solver together with performance results.</abstract>
	<keywords>C++; Data parallelism; Distributed memory; High performance fortran; Multiblock applications</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=476>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A high performance distributed database system for enhanced Internet services</paper_heading>
	<authors>A.J.H Peddemors, L.O Hertzberger</authors>
	<abstract>Using a distributed database system as a part of the distributed web sever architecture has obvious advantages. It is shown that a first phase distributed database system can be build by extending an existing object oriented database system with application-specific additions. A web database is implemented, as a part of the traditional HTTP-based distributed web server, using this distributed database system.</abstract>
	<keywords>Distributed database; High performance web server; Internet application</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=477>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficiency of standard software architectures for Java-based access to remote databases</paper_heading>
	<authors>N Zingirian, M Maresca, S Nalin</authors>
	<abstract>Novel client–server architectures for remote database access are increasingly taking advantage of Web technology, adopting Web browsers as graphic user interfaces in the clients and traditional SQL database management systems (DBMSs) in the servers. The inter-operation between standard browsers and specific DBMSs is today supported by a number of software architectures based on the Java virtual machine embedded in last generation browsers. Unfortunately such software architectures, which appear excellent from the points of view of openness and flexibility, introduce conspicuous latencies in database access. The objective of this paper is to identify such latencies through the analysis of a number of experimental results. The paper describes four different software architectures supporting Java-based SQL database access, reports their performance measurements on different hardware platforms and compares the results obtained.</abstract>
	<keywords>CORBA; Java; JDBC; Performance evaluation; Visual database access</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=478>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High-performance computer management based on Java</paper_heading>
	<authors>Volker Sander, Dietmar Erwin, Valentina Huber</authors>
	<abstract>Coupling of distributed computer resources connected by a high speed network to one virtual computer is the basic idea of a metacomputer. Access to the metacomputer should be provided by an intuitive graphical user interface (GUI), ideally world wide web (WWW) based. This paper presents a metacomputer architecture using a Java based GUI. The concept will be discussed with regard to security, communication, scalability, and the integration into existing frameworks.</abstract>
	<keywords>Metacomputing; Resource management; DCE; Single sign-on; Java</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=479>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Mining multi-dimensional data for decision support</paper_heading>
	<authors>June M. Donato, Jack C. Schryver, Gregory C. Hinkel, Richard L. Schmoyer, ... Nancy W. Grandy</authors>
	<abstract>Personal bankruptcy is an increasingly common yet little understood phenomenon. Attempts to predict bankruptcy have involved the application of data mining techniques to credit card data. This is a difficult problem, since credit card data is multi-dimensional, consisting of monthly account records and daily transaction records. In this paper, we describe a two-stage approach that combines decision trees and neural networks to predict personal bankruptcy using credit card data.</abstract>
	<keywords>Data mining; Personal bankruptcy; Decision trees; Partially recurrent neural networks</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=480>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The GRED graphical editor for the GRADE parallel program development environment</paper_heading>
	<authors>Péter Kacsuk, Gábor Dózsa, Tibor Fadgyas, Róbert Lovas</authors>
	<abstract>In this paper, we describe a graphical editor GRED as a part of the integrated programing environment GRADE that is intended to support designing, debugging and performance tuning of message-passing programs running on a heterogeneous network of computers. The GRED editor hides the cumbersome details of the underlying low-level message-passing system (which can be either PVM or MPI) by providing visual abstractions but allows the programer to define local computations of the individual processes in C (or in Fortran in the future) independently from the visually supported process management and inter-process communication activities. Visual representation of the critical (i.e. message-passing related) parts of the code can help the user in grasping the complex structure and run-time behavior of the whole parallel application, even if he or she is not an expert in the field of concurrent programing.</abstract>
	<keywords>Visual programing; Parallel programing environments; Message-passing; PVM; MPI</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=481>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Communication performance optimisation requires minimising variance</paper_heading>
	<authors>Stephen R. Donaldson, Jonathan M.D. Hill, David B. Skillicorn</authors>
	<abstract>The cost of communication in message-passing systems can only be computed based on a large number of low-level details. Consequently, the only architectural measure they naturally suggest is a first-order one, latency. We show that a second-order property, the standard deviation of the delivery times is also of interest. Most importantly, the average performance of a large communication system depends not only on the average performance of its components, but also on the standard deviation of these performances. In other words, building a high-performance system requires components that are themselves performing high-performance, but their performance must also have small variance. We illustrate this effect using distributions of the BSP g parameter. Lower bounds in the time per unit transfer of communication in large systems can be derived from data measured over single links.</abstract>
	<keywords>Parallel programming; High-performance computing; Communication performance; Machine architecture; BSP</keywords>
	<publication_month_year>1999-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 3</volumes_issues>
</paper>
<paper no=482>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Three-dimensional visualization of fluid dynamics on the Responsive Workbench</paper_heading>
	<authors>Gerold Wesche</authors>
	<abstract>Computational fluid dynamics is used by the automotive industry to efficiently reduce development cycles for car components. We present a system, that supports the exploration of such huge simulation data sets in a highly interactive way. Our system has the following two-key features: First, the visualization of the simulation data and the interaction occur through a virtual environment, called the Responsive Workbench. It is a high resolution stereoscopic table-top display, which supports head and hand tracking. Secondly, we tightly coupled a graphics workstation, that drives the workbench, with a parallel computer via a HIPPI connection, to offload computational expensive tasks.</abstract>
	<keywords>Three-dimensional visualization; Virtual environment; Responsive Workbench; Computational fluid dynamics; Structural mechanics</keywords>
	<publication_month_year>1999-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 4</volumes_issues>
</paper>
<paper no=483>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Interactive simulation of contaminant evolution through porous media</paper_heading>
	<authors>Louis F. Rossi, George Sohos</authors>
	<abstract>We describe an interactive porous media simulation designed and implemented at the Global Information Infrastructure testbed for Supercomputing ‘95. This application was designed as a tool for environmental scientists and engineers to explore and test remediation strategies on contaminant leaks into the ground. The application is controlled by a CAVE (TM) virtual environment and uses an implicit finite difference discretization for multiphase flow through porous media to simulate contaminant transport through a porous cube. The mathematical model was scaled across a remote Silicon Graphics Power Challenge Array, and the data was transmitted to the conference site via vBNS and ATM links, achieving reasonable speed for animation and interaction.</abstract>
	<keywords>Supercomputing; Virtual environment; Flow through porous medium; Animation; GII testbed</keywords>
	<publication_month_year>1999-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 4</volumes_issues>
</paper>
<paper no=484>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A virtual environment for steered molecular dynamics</paper_heading>
	<authors>Jan F. Prins, Jan Hermans, Geoffrey Mann, Lars S. Nyland, Martin Simons</authors>
	<abstract>A molecular dynamics simulation approximates the motion of atoms in a system of molecules over short intervals of simulated time, typically in the order of picoseconds to nanoseconds. Such simulations may run for days or weeks on a computer when used to investigate the dynamic behavior of small proteins in biological systems. By adding additional restraints, a simulation may be steered to observe the possibility of particular behaviors or to eliminate others over shorter timescales. We have developed the steered molecular dynamics (SMD) system to interactively place and observe the effects of restraints in a running dynamics simulation. In this article, we describe an application of SMD to the extraction of small ligands from proteins, and an immersive virtual 3D environment through which the SMD system can be operated. The virtual environment is constructed using the protein interactive theater (PIT) system, a head-tracked stereo workspace for two users.</abstract>
	<keywords>Computational steering; Virtual environments; Molecular dynamics</keywords>
	<publication_month_year>1999-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 4</volumes_issues>
</paper>
<paper no=485>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An optimized task-farm model to integrate reduced dimensionality Schrödinger equations on distributed memory architectures</paper_heading>
	<authors>R. Baraglia, R. Ferrini, D. Laforenza, A. Laganà</authors>
	<abstract>This paper investigates the parallelization of a quantum reactive scattering code that tackles the problem of integrating a two-dimensional Schrödinger equation. Computational codes integrating differential equations associated with quantum chemistry problems are known to be difficult to parallelize. A detailed examination of the algorithms and a careful analysis of possible sources for inefficiency is performed to devise a suitable parallelization strategy on MIMD-DM machines. As a result of the study, a task-farm model was adopted. The efficiency of the adopted parallel model is discussed with the help of several test calculations to pave the way for future work on full dimensional Schrödinger equations.</abstract>
	<keywords>Load balancing; MIMD-DM architecture; Task-farm model; Performance evaluation; Differential equations; Quantum reactive scattering</keywords>
	<publication_month_year>1999-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 4</volumes_issues>
</paper>
<paper no=486>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visualizing parallel simulations that execute in network computing environments</paper_heading>
	<authors>C.D. Carothers, Brad Topol, R.M. Fujimoto, J.T. Stasko, Vaidy Sunderam</authors>
	<abstract>Parallel discrete-event simulation systems (PDES) are used to simulate large-scale applications such as modeling telecommunication networks, transportation grids, and battlefield scenarios. While a large amount of PDES research has focused on employing multiprocessors and multicomputers, the use of networks of workstations interconnected through Ethernet or ATM has evolved into a popular effective platform for PDES. Nonetheless, the development of efficient PDES systems in network computing environments is not without obstacles that severely degrade simulator performance. To better understand how these factors degrade performance as well as develop new algorithms to mitigate them, we investigate the use of graphical visualization to provide insight into performance evaluation and simulator execution. We began with a general-purpose network computing visualization system, PVaniM, and used it to investigate the execution of an advanced version of Time Warp, called Georgia Tech Time Warp (GTW), which executes in network computing environments. Because PDES systems such as GTW are essentially middleware that support their own applications, we soon realized these systems require their own middleware-specific visualization support. To this end we have extended PVaniM into a new system, called PVaniM–GTW by adding middleware-specific views. Our experiences with PVaniM–GTW indicates that these enhancements enable one to better satisfy the needs of PDES middleware than general-purpose visualization systems while also not requiring the development of application-specific visualizations by the end-user.</abstract>
	<keywords>Parallel discrete-event simulation; Visualization; Network computing; Time warp</keywords>
	<publication_month_year>1999-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 4</volumes_issues>
</paper>
<paper no=487>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>UNICORE: uniform access to supercomputing as an element of electronic commerce</paper_heading>
	<authors>Jim Almond, Dave Snelling</authors>
	<abstract>The battle for the desktop has been won by workstations and PCs. Offering computational capacity adequate for most applications, and superior user interfaces, they also incorporate the user’s link to a global information base via the World Wide Web. By contrast, High Performance Computing facilities tend to be increasingly isolated by such deterrents as geographical remoteness, architectural individuality, and the non-uniform operational policies of autonomous centres. The future of such centralised Supercomputing facilities and large scale data resources may depend to a large extent on the development of interfaces for accessing their resources from the user’s desktop in a uniform and user-friendly manner; otherwise, High Performance Computing may fall short of its full potential, becoming increasingly specialised and less competitive. In the most pessimistic scenario, the volume of the HPC market could fall below the threshold required for its economic survival in the free marketplace. The Uniform Interface to Computing Resources (UNICORE) project addresses these issues using the mechanisms of the World Wide Web (WWW).</abstract>
	<keywords>Seamless computing; High performance; Electronic commerce; World Wide Web; Network security; SSL</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=488>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SPINEware – a framework for user-oriented and tailorable metacomputers</paper_heading>
	<authors>E.H. Baalbergen, H. van der Ven</authors>
	<abstract>Computing networks in enterprises are rapidly growing not only in capacity but also in complexity. The users usually get faced with networking details and with a variety of heterogeneous computing systems. In the present paper, we present SPINEware as a facility for reducing the complexity of computer network usage. SPINEware is a facility that supports the development of working environments on top of existing computer networks. Such working environment reveals itself to the user as a powerful and easy-to-use single application environment – a metacomputer – providing uniform and network-transparent access to the resources and applications available from the computer network. To further reduce the complexity, SPINEware-based metacomputers may be tailored for particular end users and application areas.</abstract>
	<keywords>Metacomputing; Globally distributed computing; Network computing</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=489>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Charlotte: Metacomputing on the Web</paper_heading>
	<authors>A. Baratloo, M. Karaul, Z.M. Kedem, P. Wijckoff</authors>
	<abstract>Parallel computing on local area networks is generally based on mechanisms that specifically target the properties of the local area network environment. However, these mechanisms do not effectively extend to wide area networks due to issues such as heterogeneity, security, and administrative boundaries. We present a system which enables application programmers to write parallel programs in Java and allows Java-capable browsers to execute parallel tasks. It comprises a virtual machine model which isolates the program from the execution environment, and a runtime system realizing this virtual machine on the Web. Load balancing and fault masking are transparently provided by the runtime system.</abstract>
	<keywords>Metacomputing; Parallel computing Fault tolerance; Load balancing; World Wide Web</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=490>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>HARNESS: a next generation distributed virtual machine</paper_heading>
	<authors>Micah Beck, Jack J. Dongarra, Graham E. Fagg, G.Al Geist, ... Vaidy Sunderam</authors>
	<abstract>Heterogeneous Adaptable Reconfigurable Networked SystemS (HARNESS) is an experimental metacomputing system [L. Smarr, C.E. Catlett, Communications of the ACM 35 (6) (1992) 45–52] built around the services of a highly customizable and reconfigurable Distributed Virtual Machine (DVM). The successful experience of the HARNESS design team with the Parallel Virtual Machine (PVM) project has taught us both the features which make the DVM model so valuable to parallel programmers and the limitations imposed by the PVM design. HARNESS seeks to remove some of those limitations by taking a totally different approach to creating and modifying a DVM.</abstract>
	<keywords>Metacomputing; Message-passing library; Distributed application; Distributed virtual machine; PVM</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=491>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Resource management in Legion</paper_heading>
	<authors>Steve J. Chapin, Dimitrios Katramatos, John Karpovich, Andrew Grimshaw</authors>
	<abstract>The recent development of gigabit networking technology, combined with the proliferation of low-cost, high-performance microprocessors, has given rise to metacomputing environments. These environments can combine many thousands of hosts, from hundreds of administrative domains, connected by transnational and world-wide networks. Managing the resources in such a system is a complex task, but is necessary to efficiently and economically execute user programs. In this paper, we describe the resource management portions of the Legion metacomputing system, including the basic model and its implementation. These mechanisms are flexible both in their support for system-level resource management but also in their adaptability for user-level scheduling policies. We show this by implementing a simple scheduling policy and demonstrating how it can be adapted to more complex algorithms.</abstract>
	<keywords>Parallel and distributed systems; Task scheduling; Resource management; Autonomy</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=492>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scalable networked information processing environment (SNIPE)</paper_heading>
	<authors>Graham E Fagg, Keith Moore, Jack J Dongarra</authors>
	<abstract>Scalable Networked Information Processing Environment (SNIPE) is a metacomputing system that aims to provide a reliable, secure, fault-tolerant environment for long-term distributed computing applications and data stores across the global Internet. This system combines global naming and replication of both processing and data to support large-scale information processing applications leading to better availability and reliability than currently available with typical cluster computing and/or distributed computer environments. To facilitate this the system supports: distributed data collection, distributed computation, distributed control and resource management, distributed output and process migration. The underlying system supports multiple communication paths, media and routing methods to aid performance and robustness across both local and global networks. This paper details the goals, design and an initial implementation of SNIPE, and then demonstrates its usefulness in supporting a middleware project. Initial communications performance is also presented.</abstract>
	<keywords>Metacomputing; Message-passing library; Distributed application; MPI; PVM; RCD; SMulti-path communication</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=493>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Globus project: a status report</paper_heading>
	<authors>Ian Foster, Carl Kesselman</authors>
	<abstract>The Globus project is a multi-institutional research effort that seeks to enable the construction of computational grids providing pervasive, dependable, and consistent access to high-performance computational resources, despite geographical distribution of both resources and users. Computational grid technology is being viewed as a critical element of future high-performance computing environments that will enable entirely new classes of computation-oriented applications, much as the World Wide Web fostered the development of new classes of information-oriented applications. In this paper, we report on the status of the Globus project as of early 1998. We describe the progress that has been achieved to date in the development of the Globus toolkit, a set of core services for constructing grid tools and applications. We also discuss the Globus Ubiquitous Supercomputing Testbed Organization (GUSTO) that we have constructed to enable large-scale evaluation of Globus technologies, and we review early experiences with the development of large-scale grid applications on the GUSTO testbed.</abstract>
	<keywords>Globus project; Computational grid; Metacomputing; High performance distributed computing</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=494>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DISCWorld: an environment for service-based matacomputing</paper_heading>
	<authors>K.A Hawick, H.A James, A.J Silis, D.A Grove, ... F.A Vaughan</authors>
	<abstract>We describe our DISCWorld system for wide-area, high-performance metacomputing in which we adopt a high-level, service-based approach. Users’ client programs request combinations of services from a set of server nodes which communicate at a peer-based level. DISCWorld is a constrained metacomputing system, running only the service operations its participating resource administrators have chosen to provide and advertise, and provides a common integration environment for clients to access these services and developers to make them available. We discuss our software architecture and experiences building DISCWorld using Java and CORBA components, and the associated research issues for metacomputing that we are addressing.</abstract>
	<keywords>Metacomputing; High-performance computing; Wide-area network; Applications service</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=495>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PARDIS: Programmer-level abstractions for metacomputing</paper_heading>
	<authors>Katarzyna Keahey</authors>
	<abstract>The potential offered by metacomputing is hard to realize due to the complexity of programming geographically distributed applications spanning different software systems. This paper describes PARDIS, a system designed to address this challenge, based on ideas underlying the Common Object Request Broker Architecture (CORBA), a successful industry standard. PARDIS is a distributed environment in which objects representing data-parallel computations, called Single Program Multiple Data (SPMD) objects, as well as non-parallel objects present in parallel programs, can interact with each other across platforms and software systems. Each of these objects represents a small encapsulated application and can be used as a building block in the construction of powerful distributed metaapplications. The objects interact through interfaces specified in the Interface Definition Language (IDL), which allows the programmer to integrate within one metaapplication component implemented using different software systems. Further, support for non-blocking interactions between objects allows PARDIS to build concurrent distributed scenarios.</abstract>
	<keywords>CORBA; Parallel; Metacomputing; Interoperability</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=496>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and implementations of Ninf: towards a global computing infrastructure</paper_heading>
	<authors>Hidemoto Nakada, Mitsuhisa Sato, Satoshi Sekiguchi</authors>
	<abstract>The world-wide computing infrastructure on the growing computer network technology is a leading technology to make a variety of information services accessible through the Internet for every user from the high-performance computing users through many of personal computing users. The important feature of such services is location transparency; information can be obtained irrespective of time or location in virtually shared manner. In this article, we overview Ninf, an ongoing global network-wide computing infrastructure project which allows users to access computational resources including hardware, software and scientific data distributed across a wide area network. Preliminary performance result on measuring software and network overhead is shown, and that promises the future reality of world-wide network computing.</abstract>
	<keywords>Distributed computing; Global computing; Scheduling</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=497>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Javelin: Parallel computing on the internet</paper_heading>
	<authors>Michael O. Neary, Bernd O. Christiansen, Peter Cappello, Klaus E. Schauser</authors>
	<abstract>Java offers the basic infrastructure needed to integrate computers connected to the Internet into a seamless distributed computational resource: an infrastructure for running coarse-grained parallel applications on numerous, anonymous machines. First, we sketch such a resource’s essential technical properties. Then, we present a prototype of Javelin, an infrastructure for global computing. The system is based on Internet software that is interoperable, increasingly secure, and ubiquitous: Java-enabled Web technology. Ease of participation is seen as a key property for such a resource to realize the vision of a multiprocessing environment comprising thousands of computers. Javelin’s architecture and implementation require participants to have access to only a Java-enabled Web browser. Experimental results are given in the form of a Mersenne Prime application and a ray-tracing application that run on a heterogeneous network of several parallel machines, workstations, and PCs. Two key areas of current research, fault-tolerance and scalability, are subsequently explored briefly.</abstract>
	<keywords>Distributed computing; High performance computing; Java; Internet; World Wide Web</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=498>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Bayanihan: building and studying web-based volunteer computing systems using Java</paper_heading>
	<authors>Luis F.G. Sarmenta, Satoshi Hirano</authors>
	<abstract>Project Bayanihan is developing the idea of volunteer computing, which seeks to enable people to form very large parallel computing networks very quickly by using ubiquitous and easy-to-use technologies such as web browsers and Java. By utilizing Java’s object-oriented features, we have built a flexible software framework that makes it easy for programmers to write different volunteer computing applications, while allowing researchers to study and develop the underlying mechanisms behind them. In this paper, we show how we have used this framework to write master-worker style applications, and to develop approaches to the problems of programming interface, adaptive parallelism, fault-tolerance, computational security, scalability, and user interface design.</abstract>
	<keywords>Metacomputing; Parallel and distributed computing; Network of workstations; Heterogeneous computing; Java</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=499>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>WAMM in the framework of graphical user interfaces for metacomputing management</paper_heading>
	<authors>R. Baraglia, R. Ferrini, D. Laforenza</authors>
	<abstract>Current environments for metacomputing generally have tools for managing the resources of a metacomputer but often lack adequate tools for designing, writing, and executing programs. Building an application for a metacomputer typically involves writing source codes on a local node, transferring and compiling codes on every node, and starting their execution. Without such tools, the application development phases can come up against considerable difficulties. In order to alleviate these problems, some graphical user interfaces (GUIs) based on PVM, such as XPVM, Parallel Application Development Environment (PADE) and Wide Area Metacomputing Manager (WAMM) have been implemented. These GUIs integrate a programming environment which facilitates the user in performing the application development phases and the application execution. This paper outlines the general requirements for designing GUIs for metacomputing management, and compares WAMM, a graphical user interface, with some related works.</abstract>
	<keywords>Metacomputing; Graphical user interface; XPVM; PADE; WAMM</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=500>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Message-passing environments for metacomputing</paper_heading>
	<authors>Matthias A. Brune, Graham E. Fagg, Michael M. Resch</authors>
	<abstract>In this paper, we present the three libraries PACX-MPI, PLUS, and PVMPI that provide message-passing between different high-performance computers in metacomputing environments. Each library supports the development and execution of distributed metacomputer applications. The PACX-MPI approach offers a transparent interface for the communication between two or more MPI environments. PVAMPI allows the user spawning parallel processes under the MPI environment. The PLUS protocol bridges the gap between vendor-specific (e.g., MPL, NX, and PARIX) and vendor-independent message-passing environments (e.g., PVM and MPI). Moreover, it offers the ability to create and control processes at application runtime.</abstract>
	<keywords>Metacomputing; Message-passing library; Distributed application; MPI; PVM</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=501>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A new model of security for metasystems</paper_heading>
	<authors>Steve J. Chapin, Chenxi Wang, William A. Wulf, Frederick Knabe, Andrew Grimshaw</authors>
	<abstract>With the rapid growth of high-speed networking and microprocessing power, metasystems have become increasingly popular. The need for protection and security in such environments has never been greater. However, the conventional approach to security, that of enforcing a single system-wide policy, will not work for the large-scale distributed systems we envision. Our new model shifts the emphasis from ‘system as enforcer’ to user-definable policies, making users responsible for the security of their objects. This security model has been implemented as part of the Legion project. Legion is an object-oriented metacomputing system, with strong support for autonomy. This includes support for per-object, user-defined policies in many areas, including resource management and security. This paper briefly describes the Legion system, presents our security model, and discusses the realization of that model in Legion.</abstract>
	<keywords>Security; Metasystem</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=502>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Predictive resource management for meta-applications</paper_heading>
	<authors>N. Floros, A.J.G. Hey, K.E. Meacham, J. Papay, M. Surridge</authors>
	<abstract>This paper defines meta-applications as large, related collections of computational tasks, designed to achieve a specific overall result, running on a (possibly geographically) distributed, non-dedicated meta-computing platform. To carry out such applications in an industrial context, one requires resource management and job scheduling facilities (including capacity planning), to ensure that the application is feasible using the available resources, that each component job will be sent to an appropriate resource, and that everything will finish before the computing resources are needed for other purposes. This requirement has been addressed by the PAC in three major European collaborative projects: PROMENVIR, TOOLSHED and HPC-VAO, leading to the creation of job scheduling software, in which scheduling is brought together with performance modelling of applications and systems, to provide meta-applications management facilities. This software is described, focusing on the performance modelling approach which was needed to support it. Early results from this approach are discussed, raising some new issues in performance modelling and software deployment for meta-applications. An indication is given about ongoing work at the PAC designed to overcome current limitations and address these outstanding issues.</abstract>
	<keywords>Resource management; Meta-application; Performance modeling</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=503>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Web based metacomputing</paper_heading>
	<authors>Tomasz Haupt, Erol Akarsu, Geoffrey Fox, Wojtek Furmanski</authors>
	<abstract>Programming tools that are simultaneously sustainable, highly functional, robust and easy to use have been hard to come by in the HPCC arena. This is partially due to the difficulty in developing sophisticated customized systems for what is a relatively small part of the worldwide computing enterprise. Thus, we have developed a new strategy – termed High Performance Commodity Computing (HPCC) [G. Fox, W. Furmanski, HPCC as high performance commodity computing, in: I. Foster, C. Kesselman (Eds.), Building National Grid, http://www.npac.syr.edu/users/gcf/HPcc/HPcc.html] – which builds HPCC programming tools on top of the remarkable new software infrastructure being built for the commercial web and distributed object areas. We add high performance to commodity systems using multi-tier architecture with Globus metacomputing toolkit as the backend of a middle-tier of commodity web and object servers. We have demonstrated the fully functional prototype of WebFlow during Alliance’98 meeting.</abstract>
	<keywords>WebFlow; Metacomputing; Globus; Visual authoring tool; Distributed objectJava; Web server; Three-tier architecture; HPCC</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=504>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Deploying fault tolerance and taks migration with NetSolve</paper_heading>
	<authors>James S. Plank, Henri Casanova, Micah Beck, Jack J. Dongarra</authors>
	<abstract>Computational power grids are computing environments with massive resources for processing and storage. While these resources may be pervasive, harnessing them is a major challenge for the average user. NetSolve is a software environment that addresses this concern. A fundamental feature of NetSolve is its integration of fault-tolerance and task migration in a way that is transparent to the end user. In this paper, we discuss how NetSolve’s structure allows for the seamless integration of fault-tolerance and migration in grid applications, and present the specific approaches that have been and are currently being implemented within NetSolve.</abstract>
	<keywords>Fault-tolerance; Scientific computing; Computational server; Checkpointing; Migration</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=505>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The network weather service: a distributed resource performance forecasting service for metacomputing</paper_heading>
	<authors>Rich Wolski, Neil T Spring, Jim Hayes</authors>
	<abstract>The goal of the Network Weather Service is to provide accurate forecasts of dynamically changing performance characteristics from a distributed set of metacomputing resources. Providing a ubiquitous service that can both track dynamic performance changes and remain stable in spite of them requires adaptive programming techniques, an architectural design that supports extensibility, and internal abstractions that can be implemented efficiently and portably. In this paper, we describe the current implementation of the NWS for Unix and TCP/IP sockets and provide examples of its performance monitoring and forecasting capabilities.</abstract>
	<keywords>Network weather; Network monitoring; Performance prediction; Metacomputing; Network-aware; Distributed computing</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=506>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Metacomputing in practice: a distributed compute server for pharmaceutical industry</paper_heading>
	<authors>Robert Bywater, Jörn Gehring, Alexander Reinefeld, Friedrich Rippmann, Anke Weber</authors>
	<abstract>We describe a distributed high-performance compute server that has been implemented for running compute-intensive applications on a mixture of HPC systems interconnected by Inter-and Intranet. With a practical industrial background, our work focusses on high availability, efficient job load balancing, security, and the easy integration of HPC computing into the daily work-flow at pharmaceutical companies. The work was done in the course of the ESPRIT project Phase A Distributed Pharmaceutical Application Server The client software is implemented in Java. All results are displayed in a web browser and can be forwarded to the next stage of applications used in the drug design cycle. The server software handles the job load balancing between the participating HPC nodes and is capable of managing multi-site applications. Our environment currently supports four key applications that are used in rational drug design and drug target identification. They range from the automatic functional annotation of protein sequences to three-dimensional protein structure prediction tools and protein comparison applications.</abstract>
	<keywords>Metacomputing; Resource management; Bio-informatics; Job load balancing; Multisite application; Java; Web technology</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=507>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A metacomputing environment for demanding applications: design, implementation, experiments and business benefit</paper_heading>
	<authors>A. Meliones, T. Varvarigou, P. Tsagronis, A. Emmen, I. Barosan</authors>
	<abstract>The paper describes the design, implementation, and use of a commercial metacomputing environment for computationally intensive loosely-coupled parallel applications. Much weight has been laid on practical and commercialisation aspects, and on business benefit. This distinguishes this work from many other metacomputing activities in a positive way. It demonstrates how a metacomputing environment can be used to improve a company’s position in the market. A cluster of networked geographically dispersed computing nodes is considered as physical layer. The proposed distribution of work over the nodes of the execution network is proven optimal, in terms of minimizing the execution time, with respect to the availability of resources. We also present our experience on testing the environment for computing-intensive 3D-rendering jobs derived from the ESPRIT project EROPPA and demonstrate that the new environment can change dramatically the character of the post production business.</abstract>
	<keywords>Distributed high-performance computing; Metacomputing; Cluster computing; Post production; Remote rendering; Distributed rendering; CODINE; Job management; Turnaround time; Loosely-coupled parallelism; EROPPA</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=508>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Metacomputing experience in a transatlantic wide area application test-bed</paper_heading>
	<authors>Michael M. Resch, Dirk Rantzau, Robert Stoy</authors>
	<abstract>In the frame of a G7 initiative the High Performance Computing Center Stuttgart (HLRS) together with the Pittsburgh Supercomputing Center (PSC) and Sandia National Laboratories (SNL) has set up a transatlantic wide area application test-bed in 1997. A dedicated ATM-Link was installed that connected German research networks to vBNS and ESnet. During 1 year this test-bed was extensively used for metacomputing and collaborative working. Two applications – one from computational fluid dynamics and one from molecular dynamics – were adapted and run on the test-bed. For message-passing an MPI library was implemented that supports metacomputing. An already existing software for collaborative visualization was adapted for that scenario. This article describes the technical background of the cooperation, results that have been achieved for the two applications so far and lessons that have been learned. Special emphasis will be given to future work planned.</abstract>
	<keywords>Global Information; Society Initiative; Global Interoperability; Broadband Networks</keywords>
	<publication_month_year>1999-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 15, Issues 5-6</volumes_issues>
</paper>
<paper no=509>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An extensible information model for shared scientific data collections</paper_heading>
	<authors>Amarnath Gupta, Chaitanya Baru</authors>
	<abstract>An information model is defined to support sharing of composite-media scientific data. The model consists of data objects and links. Data objects are associated with descriptors which contain all the metadata related to the object. A novel aspect of the information model is that both the data and metadata associated with a data object can be in structured or semistructured form. The links in the model are typed, and contain several built-in types such as resolution and derivation link types, to adequately model object relationships in scientific data. The model is extensible in the sense that users are allowed to define new object types and new link types. At the San Diego Supercomputer Center, we are currently investigating issues in providing the necessary infrastructure to implement this model.</abstract>
	<keywords>Scientific information management; Extensible information model; Semistructured data model; XML</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=510>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A method for interoperable digital libraries and data repositories</paper_heading>
	<authors>John J. Helly, T.Todd Elvins, Don Sutton, David Martinez</authors>
	<abstract>Aside from the basic importance of metadata in documenting, the characteristics of data for reuse is the fundamental role it plays in the functioning of digital libraries and data repositories. Metadata provides both the content of the search catalogs and provides part of the basis for performing quality control on the source data. In this paper, we describe a method for a scalable and decentralized system of interoperable digital libraries and data repositories. The description includes transportable metadata format, a persistent naming convention for arbitrary digital objects and a protocol for the asynchronous distribution of metadata. We also include a description of an operational data repository based on these methods.</abstract>
	<keywords>metadata; digital libraries; decentralized system</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=511>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Development of the astronomical image archive and catalog database for production of GSC-II</paper_heading>
	<authors>Gretchen Greene, Brian McLean, Barry Lasker</authors>
	<abstract>The Catalogs and Surveys Branch (CASB) of the Space Telescope Science Institute (STScI), in collaboration with a number of international astronomical institutions, is continuing with the development and creation of an archive of digitized images, and an associated catalog of stars and galaxies which cover the entire sky. These data are being made available to the astronomical community to support telescope operations and research projects.</abstract>
	<keywords>Astronomical catalogsImage archives; Object oriented databases</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=512>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The VizieR system, a unified interface to astronomical catalogs</paper_heading>
	<authors>F. Ochsenbein, P. Fernique, P. Ortiz, D. Egret, F. Genova</authors>
	<abstract>We present in this paper some aspects of the VizieR service, as an example of a database connecting to the largest set of astronomical catalogs. We especially emphasize here role of the metadata: how these are acquired from the catalog descriptions, and how they are stored in the system. The specification of relations or links between data are an important part of the metadata, and we use GLU (Générateur de Liens Uniformes=uniform link generator) expressions to describe the links to external databases or services. The basic principles of the GLU system initiated at CDS, as well as some tools based on the GLU protocol, are briefly described. Finally, the on-going developments, especially related to data mining purposes, are presented.</abstract>
	<keywords>Astronomical catalogs; Astronomical metadata; Data mining</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=513>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The NCSA astronomy digital image library: from data archiving to data publishing</paper_heading>
	<authors>Raymond L. Plante, Richard M. Crutcher, Robert E. McGrath</authors>
	<abstract>The NCSA Astronomy Digital Image Library (ADIL)2, a repository providing astronomers with research-quality images over the Web, represents the final stage of the data-research pipeline that begins at the telescope and ends with the publication of scientific results. By examining the ADIL’s mission and architecture, we will highlight some of the challenges arising from the development of a scientific data library. After an overview of the motivation and context surrounding the Library, we will present the two faces of its user interface: one for those looking for data, and one for those wishing to add to the Library’s collection. We will then outline the design of the Library’s back-end system, describing some of the technical choices made during its development. Finally, we will examine the role of the ADIL in the evolving life cycle of scientific data, highlighting two important issues: interoperability and data publishing. Today, the data life cycle is almost entirely electronic; thus, the scientific data library becomes a critical part of the solution to the ‘Data Crisis’ in astronomy: keeping up with the exploding growth of new data.</abstract>
	<keywords>Data archives; Digital libraries; Images; Visualization</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=514>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Astronomical archives of the future: a Virtual Observatory</paper_heading>
	<authors>A.S. Szalay, R.J. Brunner</authors>
	<abstract>Astronomy is entering a new era as multiple, large area, digital sky surveys are in production. The resulting datasets are truly remarkable in their own right; however, a revolutionary step arises in the aggregation of complimentary multi-wavelength surveys (i.e. the cross-identification of a billion sources). Federating these different datasets, however, is an extremely challenging task. With this task in mind, we have identified several areas where community standardization can provide enormous benefits in order to develop the techniques and technologies necessary to solve the problems inherent in federating these large databases, as well as the mining of the resultant aggregate data. Several of these areas are domain specific, however, the majority of them are not. We feel that the inclusion of non-astronomical partnerships can provide tremendous insights.</abstract>
	<keywords>Astronomy; multi-wavelength; cross-identification</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=515>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Interfacing to distributed active data archives</paper_heading>
	<authors>K.A. Hawick, P.D. Coddington</authors>
	<abstract>The general problem of managing large data archives or libraries of digital data is particularly challenging when the system must cope with active data which is processed on-demand. Conventional data archives are designed so that data is ingested into the system, and retrieved by users to satisfy particular requests. An active data archive can be defined as one where much of the data is generated on-demand, as value-added data products or services derived from existing data holdings. Interfacing users and applications to such an archive system requires a more complex infrastructure than conventional or passive archive systems. We describe two archive systems we have built for providing web-based access to satellite and geospatial imagery as well as to medical imagery such as that from the Visible Human datasets. We contrast the requirements and features of these archives and discuss the Java and CORBA software infrastructure that we have developed to interface to them. We also describe our integration of commercial products, such as StudioCentral and Informix with our software and some of the geospatial standard interface definitions, such as GIAS and OpenGIS.</abstract>
	<keywords>Digital library; Active dataOn-line archive; Data services; Geospatial imagery; Visible human data</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=516>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An XML architecture for high-performance web-based analysis of remote-sensing archives</paper_heading>
	<authors>G. Aloisio, G. Milillo, R.D. Williams</authors>
	<abstract>We introduce XML (eXtensible Markup Language) as a simple, flexible, and powerful way for computers to exchange metadata and control information, not only with humans, but also with each other. We describe an existing system that delivers customized remote-sensing data products to web-connected clients, and what more is required to support supervised, on-demand processing of the data. We discuss how such an architecture can be used for these purposes, and how it may interoperate with existing parallel SAR processing architectures, or with a GIS application server. Applications of such a system include agriculture, ecology, geophysics, glaciology and hazard-monitoring.</abstract>
	<keywords>eXtensible Markup; exchange metadata; remote-sensing; XML; GIS</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=517>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Large scale distributed data repository: design of a molecular dynamics trajectory database</paper_heading>
	<authors>Michael Feig, Matin Abdullah, Lennart Johnsson, B.Montgomery Pettitt</authors>
	<abstract>The design of a molecular dynamics trajectory database is presented as an example of the organization of large-scale dynamic distributed repositories for scientific data. Large scientific datasets are usually interpreted through reduced data calculated by analysis functions. This allows a database architecture in which the analyzed datasets, that are kept in addition to the raw datasets, are transferred to a database user. A flexible user interface with a well defined Application Program Interface (API) allows for a wide array of analysis functions and the incorporation of user defined functions is a critical part of the database design. An analysis function is executed only when the requested analysis result is not available from an earlier request. A prototype implementation used to gain initial practical experiences with performance and scalability is presented.</abstract>
	<keywords>Distributed database; Scientific data archive; Data analysis</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=518>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Modeler’s Workspace: a distributed digital library for neuroscience</paper_heading>
	<authors>Jenny Forss, David Beeman, James M. Bower, Rogene M.Eichler West</authors>
	<abstract>The amount of available data in the neuroscience community is growing rapidly, and is approaching the point where researchers can no longer keep up. This paper describes the development of the Modeler’s Workspace, a digital library system we are building to improve the researcher’s means of accessing, managing and sharing neuroscience data. The Workspace consists of a distributed system of heterogeneous databases, and a Java front-end for data mining and visualization. A three-tier architecture with CORBA as the middleware facilitates database federation. We discuss our design choices, and some sociological issues, such as how to encourage data submission and ensure security and quality control.</abstract>
	<keywords>Digital library; Distributed databases; Object-oriented databases; CORBA; Neuroscience; Neural modeling</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=519>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The laser interferometer gravitational-wave observatory scientific data archive</paper_heading>
	<authors>Lee Samuel Finn</authors>
	<abstract>LIGO – The Laser Interferometer Gravitational-Wave Observatory – is one of several large projects being undertaken in the United States, Europe and Japan to detect gravitational radiation. The novelty and precision of these instruments is such that large volumes of data will be generated in an attempt to find a small number of weak signals, which can be identified only as subtle changes in the instrument output over time. In this paper, I discuss how the nature of the LIGO experiment determines the size of the data archive that will be produced, how the nature of the analyses that must be used to search the LIGO data for signals determines the anticipated access patterns on the archive, and how the LIGO data analysis system is designed to cope with the problems of LIGO data analysis.</abstract>
	<keywords>Gravitational-Wave; Observatory; LIGO; gravitational radiation</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=520>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>‘Split personalities’ for scientific databases: targeting database middleware and interfaces to specific audiences</paper_heading>
	<authors>Cherri M. Pancake, Mark Newsome, F.Joe Hanus</authors>
	<abstract>Scientific researchers are anxious to discover new insights into the relationships and interactions among the exceedingly diverse components of our physical, biological and ecological environment. To do this, individual scientists must be able to synthesize conclusions from data drawn from disciplines outside their domains of expertise. While key datasets have already been brought online, they are housed in diverse agencies, using different database software on a variety of platforms. This paper explores the problems in providing middleware and interface support for these autonomous research databases (ARDs). The development of interfaces to ARDs is complicated by the fact that potential end-users range from highly-specialized research scientists to the general public. To adequately support such diverse users, the interface must assume different ‘personalities’. We describe how interfaces can be targeted to three categories of end-users: domain specialists, non-domain specialists, and students. By automating the activities that are most frustrating, time-consuming, or error-prone, target-specific interfaces can significantly improve database usability. There is also heterogeneity among the people who must develop ARD interfaces. We target three different classes of implementers – database-technology familiar, database-content familiar, and database unfamiliar. By developing different middleware personalities that respond to the specific skills and interests of each audience, we have been able to keep our software simple and usable without sacrificing flexibility.</abstract>
	<keywords>Ecology; Databases Query languages; Human interfaces; Lichen</keywords>
	<publication_month_year>1999-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 1</volumes_issues>
</paper>
<paper no=521>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulation of cellular automata</paper_heading>
	<authors>Thomas Worsch</authors>
	<abstract>There exists quite a number of software packages for the simulation of cellular automata (CA). After a short review of the standard definition some modifications and extensions are discussed which have in particular proven to be useful or important for the use of CA as models of real phenomena. A survey of the features of several software packages for the simulation of CA is given. For several applications reasonably fast simulations require the use of parallel computers. For message passing systems connected by a reasonably fast network a domain decomposition approach and possible optimizations are described.</abstract>
	<keywords>Cellular automata; Simulation; Parallelization</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=522>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lattice gases and cellular automata</paper_heading>
	<authors>Bruce M. Boghosian</authors>
	<abstract>We review the class of cellular automata known as lattice gases, and their applications to problems in physics and materials science. The presentation is self-contained, and assumes very little prior knowledge of the subject. Hydrodynamic lattice gases are emphasized, and non-lattice-gas cellular automata – even those with physical applications – are not treated at all. We begin with a review of lattice gases as the term is understood in equilibrium statistical physics. We then discuss the various methods that have been proposed to simulate hydrodynamics with a lattice gas, leading up to the 1986 discovery of a lattice gas for the isotropic Navier–Stokes equations. Finally, we discuss variants of lattice-gas models that have been used for the simulation of complex fluids.</abstract>
	<keywords>Cellular automata; Lattice gases; Hydrodynamics; Discrete kinetic theoryIsing model; Complex fluidsmicroemulsions</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=523>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programmable matter methods</paper_heading>
	<authors>Tommaso Toffoli</authors>
	<abstract>Fine-grained, indefinitely-extended mesh architectures, which can aptly be termed ‘Programmable Matter’, play a complementary rather than competitive role vis-à-vis traditional architectures. Ideal areas of applicability are physical modeling, materials science, interactive modeling of complex objects, image processing, and pattern recognition. Moreover, these architectures are well suited to furnish a viable blueprint for computers built out of atomic-scale components, towards which technology is inexorably leading. In spite of a potentially huge performance gain over conventional computers, programmable matter is currently hard to exploit. In most cases, its raw computational resources do not directly match the structure of problems one might want to apply it to. Since much ad hoc programming is needed to attain a reasonable fraction of the theoretical performance, so far only niche applications have been explored. We are developing a modeling methodology that will give programmable matter a much broader scope of application. This methodology makes extensive use of synthetic dynamics inspired by physics (kinematic transformations, cellular automata and lattice gases, statistical–mechanical ensembles, simulated annealing, simulated staining, texture-locked loops, etc.), but harnesses these dynamics to data-processing tasks of a more general nature as are encountered in a variety of mundane applications. In particular, by means of suitable feedback, the massive pattern-generation resources of cellular automata machines are used to construct flexible pattern recognizers.</abstract>
	<keywords>Fine-grain computation; Parallel computation; Cellular automata; Emergent Coputation</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=524>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programming cellular automata algorithms on parallel computers</paper_heading>
	<authors>Giandomenico Spezzano, Domenico Talia</authors>
	<abstract>A high-level programming support is an essential component for the practical development of computational science applications using the cellular automata model. This paper, after introducing the CARPET language, shows its practical use for programming cellular automata simulations on parallel computers. CARPET is a high-level language designed for supporting rapid prototyping and full implementation of a large number of science and engineering applications on high-performance computers. The language provides a user with a programming layer that offers constructs for the direct definition of the cellular automata features such as lattice dimension, cell state, neighborhood, and transition function. The CARPET parallel run-time system maps CA programs on a parallel computer hiding the architecture issues to a user and it provides advanced visualization of program’s output. The paper describes how practical cellular automata algorithms for lattice gas, gas diffusion simulation, and traffic flow modeling can be designed using the CARPET programming language.</abstract>
	<keywords>Cellular automata; Parallel programming; Computational science; High-performance computing.</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=525>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>P-CAM: a framework for parallel complex systems simulations</paper_heading>
	<authors>A. Schoneveld, J.F. de Ronde</authors>
	<abstract>History has taught that the design and implementation of an efficient parallel simulation program is a tedious and error prone process. Methods that can circumvent the parallelization steps in this process are usually warmly welcomed by parallel simulation architects. In this paper we introduce a Parallel Cellular Automata Modeling environment (P-CAM) for doing spatial load balancing on arbitrary connected grids or task graphs. This environment adopts an object oriented application framework in which we can instantiate a variety of simulation problems. We have implemented a kernel, based on this framework, which facilitates dynamic load balancing and supports process migration and irregular interprocess communication patterns. The design of the kernel enables a transparent implementation of complex systems models onto arbitrary parallel computer systems. We show that the design of a parallel simulation program can be assisted by using the P-CAM kernel.</abstract>
	<keywords>P-CAM; Parallel Cellular Automata; parallel simulation; P-CAM kernel</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=526>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>System development for parallel cellular automata and its applications</paper_heading>
	<authors>C. Hecker, D. Roytenberg, J.-R. Sack, Z. Wang</authors>
	<abstract>Cellular automata (CA) are a powerful technique for modeling otherwise intractably complex systems. However, the amount of computation required can be large. This may mean that the use of CA is not feasible or that models must be simplified, resulting in loss of accuracy. To address this, we have implemented a system called the Cell Driver to facilitate the development of CA in parallel. The Cell Driver is designed to support the rapid development of parallel cellular automata applications, making parallel computing easily accessible for CA developers. We present the results of applications in fire modeling, earthquake modeling, and crystal growth.</abstract>
	<keywords>Cellular automata (CA); Cell Driver; CA developers; earthquake model</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=527>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Cellular automata and lattice Boltzmann methods: a new approach to computational fluid dynamics and particle transport</paper_heading>
	<authors>Bastien Chopard, Alexandre Masselot</authors>
	<abstract>Cellular automata (CA) and lattice Boltzmann (LB) approaches are computational methods that offer flexibility, efficiency and outstanding amenability to parallelism when modeling complex phenomena. In this paper, the CA and LB approach are combined in the same model, in order to describe a system where point-particles are transported in a fluid flow. This model is used to simulate the snow transport, erosion and deposition by the wind.</abstract>
	<keywords>Simulation and modeling; Cellular automata and lattice Boltzmann methods; Fluid flows; Particle erosion and deposition; Snow transport; Massively parallel computing</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=528>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An empirical method for modelling and simulating some complex macroscopic phenomena by cellular automata</paper_heading>
	<authors>Salvatore Di Gregorio, Roberto Serra</authors>
	<abstract>Novel parallel computing models sometime represent a valid alternative to standard differential equation methods in modelling complex phenomena. In particular, Cellular Automata (CA) provide such an alternative approach for some complex natural systems, whose behaviour can be described in terms of local interactions of their constituent parts. This paper illustrates an empirical method applied with interesting results in modelling and simulating some complex macroscopic phenomena. While classical CA are based upon elementary automata, with few states and a simple transition function, in order to deal with macroscopic phenomena it is often necessary to allow a large number of different states a more complicated transition. The notion of substate is introduced in the macroscopic case for decomposing the state of the cell. The values associated to substates can change in time either due to interactions among substates inside the cell (internal transformations) or to local interactions among neighbouring cells. The internal transformations are treated in a way similar to ordinary difference equations. The local interactions among cells can be often treated according to an algorithm for the minimisation of differences, which describes a tendency of conserved quantities to reach an equilibrium distribution. A large class of complex macroscopic phenomena seem to satisfy the applicability conditions of such an empirical method; some of them are briefly reviewed.</abstract>
	<keywords>Cellular automata; Complex systems simulation; Modelling methods; Fluid-dynamics; Parallel computing.</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=529>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Cellular automata models of biochemical phenomena</paper_heading>
	<authors>Lemont B. Kier, Chao-Kun Cheng, Bernard Testa</authors>
	<abstract>This paper describes the use of kinematic, asynchronous, stochastic cellular automata to model water and solution phenomena encountered in complex biological systems. These in silico experiments are designed to assess the ability of the dynamic simulations to model some physical properties observed in solutions. Several experiments are described with significant relationship to physical reality. These include solution behavior, dissolution, immiscible systems, micelle formation, diffusion, membrane passage, enzyme activity and acid dissociation. From the confidence developed in these models, it is possible to consider cellular automata as an exploratory method for the discovery and understanding of new, unexpected phenomena.</abstract>
	<keywords>Solution phenomena; Water structure; Cellular automataIn silico experiments; Dynamic simulation</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=530>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Generating high-quality random numbers in parallel by cellular automata</paper_heading>
	<authors>Marco Tomassini, Moshe Sipper, Mosé Zolla, Mathieu Perrenoud</authors>
	<abstract>Many important computer simulation methods rely on random numbers, including Monte Carlo techniques, Brownian dynamics, and stochastic optimization methods such as simulated annealing. Several deterministic algorithms for producing random numbers have been proposed to date. In this paper we concentrate on generating pseudo-random sequences by using cellular automata, which offer a number of advantages over other methods, especially where hardware implementation is concerned. We study both hand-designed random number generators as well as ones produced by artificial evolution. Applying an extensive suite of tests we demonstrate that cellular automata can be used to rapidly produce high-quality random number sequences. Such automata can be efficiently implemented in hardware, can be used in such applications as VLSI built-in self-test, and can be applied in the field of parallel computation.</abstract>
	<keywords>Cellular automata; Random number generators; Evolutionary Computing; Parallel computation</keywords>
	<publication_month_year>1999-12-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 2-3</volumes_issues>
</paper>
<paper no=531>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Protecting secret keys with personal entropy</paper_heading>
	<authors>Carl Ellison, Chris Hall, Randy Milbert, Bruce Schneier</authors>
	<abstract>Conventional encryption technology often requires users to protect a secret key by selecting a password or passphrase. While a good passphrase will only be known to the user, it also has the flaw that it must be remembered exactly in order to recover the secret key. As time passes, the ability to remember the passphrase fades and the user may eventually lose access to the secret key. We propose a scheme whereby a user can protect a secret key using the “personal entropy” in his own life, by encrypting the passphrase using the answers to several personal questions. We designed the scheme so the user can forget answers to a subset of the questions and still recover the secret key, while an attacker must learn the answer to a large subset of the questions in order to recover the secret key.</abstract>
	<keywords>Secret key; Personal entropy</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=532>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Key management in a non-trusted distributed environment</paper_heading>
	<authors>Ed Dawson, Andrew Clark, Mark Looi</authors>
	<abstract>Despite the fact that the World Wide Web is an untrusted environment, increasing use is being made of this network (the Internet) in electronic commerce applications. To prevent attacks a strong security architecture is required. A fundamental part of such an architecture is a method for key management. This paper discusses the various components of cryptographic key management especially in relation to the World Wide Web environment. Issues and problems with key generation, key distribution and key storage are raised. An overview is presented of key management systems in several security architectures including SSL, Kerberos and Sesame.</abstract>
	<keywords>Key management; Key distribution; Key storage; Random number generator; World Wide Web</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=533>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>WWW security and trusted third party services</paper_heading>
	<authors>Bruno Crispo, Peter Landrock, Václav Matyáš</authors>
	<abstract>We provide an overview of major findings from the EC project2 ‘TrustWeb – Security in the WWW, mutual impact of ETS and WWW’. While not all the technical details can be discussed here, we review some of the most relevant ones, referring readers to documents on the project webpage for details. Our message after the project is: WWW security has many problems, out of which only some can be solved with the use of TTP services, yet TTPs can greatly benefit from the developments of the WWW, where only secure WWW can provide useful interface and communication means for TTP services.</abstract>
	<keywords>Trusted third parties; Public-key cryptography; WWW security</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=534>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>New paradigms – old paradigms?</paper_heading>
	<authors>Dieter Gollmann</authors>
	<abstract>Cryptography plays a central role in security on the web, as documented by many contributions to this volume. However, sometimes the importance of cryptography is exaggerated. This paper explores the foundations that are needed to provide the basis for cryptographic protection, and for web security in general. We will discuss different paradigms that underpin the use of cryptography and of public-key infrastructures, observing a shift in paradigms as far as the protection requirements are concerned, and a clash of paradigms between communications and computer security. We conclude that assurance, a topic neglected in current discussions of security, is today the weakest link in web security.</abstract>
	<keywords>Cryptography; Web security; Access control policies; Public key infrastructures</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=535>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Keystroke dynamics as a biometric for authentication</paper_heading>
	<authors>Fabian Monrose, Aviel D. Rubin</authors>
	<abstract>More than ever before the Internet is changing computing as we know it. Global access to information and resources is becoming an integral part of nearly every aspect of our lives. Unfortunately, with this global network access comes increased chances of malicious attack and intrusion. In an effort to confront the new threats unveiled by the networking revolution of the past few years reliable, rapid, and unintrusive means for automatically recognizing the identity of individuals are now being sought. In this paper we examine an emerging non-static biometric technique that aims to identify users based on analyzing habitual rhythm patterns in the way they type.</abstract>
	<keywords>Identity verification; User authentication; Biometrics</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=536>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scaling issues in large PKI communities</paper_heading>
	<authors>A.P. Varvitsiotis</authors>
	<abstract>As the Internet is growing quickly, the demand for distributed systems security is gaining importance. Current Public Key Infrastructure (PKI) systems are based on public-key cryptography to provide secure communications over the Internet. The PKI user population is growing as well, stimulated by the interest in electronic commerce applications. Future PKI communities will probably count hundreds of thousands, or even millions of users. At the same time, PKI support systems consisting of server and client software are required to cope with a large number of requests in an environment of growing diversity. In this paper, requirements for PKI systems are examined from a scaling point of view. The scaling properties of clients and servers, as well as the impact of policies and management practices in scaling are discussed. Guidelines are given for PKI system designers and implementors.</abstract>
	<keywords>PKI systems; Security; Certification; Scaling</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=537>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Business insights in e-commerce and trusted services</paper_heading>
	<authors>Derek Kueter, Robbert Fisher</authors>
	<abstract>The BESTS study was one of the first to explore the business realities, market opportunities and economic implications of the European trusted services (TS) market, especially where certificate authorities (CAs), registration authorities, time stamping authorities – and the digital signature-based public key infrastructure that support them – are concerned. Trusted services are essential for Europe’s growth into the information society, Europe should have native competence in trusted services, but Europe is perceived as lagging behind. We outline the problems European players face when trying to become (and remain) profitable in TS, provide predictions for the future (using four scenarios: growth/explosion, stagnation, maturity and decline/crackdown), and rate the feasibility of the various possible business models. Pervasive myths and misunderstandings surrounding trusted services are clarified. Included are the top 10 findings and predictions from the study in the following areas: business opportunities, branding, infrastructure costs, product differentiation among CAs, user education, alliance education, degradation of consumer confidence, finance, regulation, and market stimulation by government.</abstract>
	<keywords>Electronic commerce; E-commerce; Trusted services; TTP; Certificate authority; Internet commerceI-commerce; Branding; Economic implications</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=538>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Privacy protection and anonymity services for the World Wide Web (WWW)</paper_heading>
	<authors>Rolf Oppliger</authors>
	<abstract>Although browsing on the World Wide Web (WWW) feels like an anonymous activity, it is hardly that way. Website administrators generally get a lot of information about users and their browsing behavior, enabling such things as one-to-one marketing. Even more information is available to Internet service providers whose HTTP proxy servers may keep track of every Website visited by their subscribers. Similarly, it is difficult to publish data on the Web without revealing the corresponding HTTP server name or IP address. In this situation, privacy protection and anonymity services for the WWW are becoming increasingly important fields of study. This paper overviews and briefly discusses some technologies that are available today and that can be used to provide support for anonymous browsing and anonymous publishing on the WWW.</abstract>
	<keywords>World Wide Web; Privacy protection; Anonymity services</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=539>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Secure linking of customers, merchants and banks in electronic commerce</paper_heading>
	<authors>N. Alexandris, M. Burmester, V. Chrissikopoulos, Y Desmedt</authors>
	<abstract>We use the concept of designated 2-verifier proofs to design simple and secure electronic payment systems. Two on-line protocols which link securely Customers, Merchants and Banks are presented. In the first the identity of the Customer is traceable. This protocol can be used for general electronic payment systems. The second protocol can be used for anonymous electronic cash payments. Both protocols have a simple structure and are provably secure.</abstract>
	<keywords>Electronic payment systems; Designated verifier proofs; Man-in-the-middle attacks; Non-malleability</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=540>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fingerprint recognition based on silicon chips</paper_heading>
	<authors>Jean-François Mainguet, Marc Pégulu, John B. Harris</authors>
	<abstract>After introducing the fingerprint recognition mechanism, different types of fingerprint sensors will be presented, with silicon-based sensors being shown as a less expensive solution. The fact that lower area means lower cost for silicon chips will be demonstrated by looking at ‘sweeping’ solutions such as the FingerChip™. Then, the fingerprint system will be detailed from a silicon point-of-view: how the integration of the sensor and processor may enhance the cost and the security, associated (or not) with a smart card.</abstract>
	<keywords>Biometrics; Fingerprint recognition; Fingerprint sensor</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=541>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>CORBA security on the web – an overview</paper_heading>
	<authors>Ulrich Lang</authors>
	<abstract>CORBA is a useful platform for world wide web based applications because it abstracts many of the inherent complexities of open distributed applications. Also, CORBA allows the transparent provision of security which is critical for many web based applications, e.g. electronic commerce. This paper outlines the principal security considerations for web-based CORBA applications and presents some of the implementation options to meet these requirements. Special focus is given to the often overlooked problem of integrating secure CORBA-based applications with current firewall technology.</abstract>
	<keywords>CORBA; Security; World wide web</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=542>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A secure brokerage network for retail banking services</paper_heading>
	<authors>S. Gamvroulas, D. Polemi, M. Anagnostou</authors>
	<abstract>The objective of this paper is to propose and describe a secure and trusted brokerage network for offering retail-banking services. The goal of this network is to collect information about retail-banking service offerings from various commercial banks and make this information available to the public. Facilities that will help prospective clients to decide on the right service according to their needs are also offered. Mobile intelligent agent technology will enhance functionality, efficiency and performance.</abstract>
	<keywords>Trusted third party services (TTPs); Mobile intelligent agent technology; Retail-banking services</keywords>
	<publication_month_year>2000-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 4</volumes_issues>
</paper>
<paper no=543>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual engineering of multi-disciplinary applications and the significance of seamless accessibility of geometry data</paper_heading>
	<authors>Vaibhav Deshpande, Luciano Fornasier, Edgar A. Gerteisen, Nils Hilbrink, ... Thomas Wöhler</authors>
	<abstract>The concept of virtual engineering (VEng) can be understood as a generalization of “multi-disciplinary problem solving”, an ever more used term in scientific computing. An abstract space consisting of the physical, the geometrical, and the cost function directions, called CGP, is introduced. The VEng problem can be seen as a complex manifold embedded in this space. Common standard data formats, unified data access, as well as open, non-monolithic systems, are discussed. These contribute to smoothing the sharp edges and closing the gaps of the manifold. The significance of seamless accessibility of data is illustrated by means of the tightly coupled fluid-structure interaction in aero-elasticity.</abstract>
	<keywords>Virtual engineering; CGP space; Homeomorphism; Problem solving environment; Data management; MemCom; CORBA; CAD; CAD-repair; CAD accessibility; Topological errors; Geometrical errors; Boundary representation; NURBS; Mesh generation; Finite element simulation; Multi-disciplinary applications</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=544>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>WebFlow: a framework for web based metacomputing</paper_heading>
	<authors>Tomasz Haupt, Erol Akarsu, Geoffrey Fox</authors>
	<abstract>We developed a platform independent, three-tier system, called WebFlow. The visual authoring tools implemented in the front end integrated with the middle-tier network of servers based on CORBA and following distributed object paradigm, facilitate seamless integration of commodity software components. We add high performance to commodity systems using Globus metacomputing toolkit as the backend.</abstract>
	<keywords>WebFlow; CORBA; Globus</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=545>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational experiments using distributed tools in a web-based electronic notebook environment</paper_heading>
	<authors>Allen D Malony, Janice E Cuny, Jenifer L Skidmore, Matthew J Sottile</authors>
	<abstract>Scientific computational environments should provide high-level support for the integrated and systematic use of tools familiar in the laboratory setting, including lab notebooks, instruments, experiments, and analysis tools. Further, they should shield the user from the complexities of the underlying computational platform. We report here on the Virtual Notebook Environment, ViNE, which provides the web-based equivalent of the standard (paper) lab notebook, adding features for sharing, security, and collaboration. More significantly, ViNE allows scientists to represent, manage, and execute computational experiments that involve the sequenced use of data, computational tools, and programs distributed across the World Wide Web. ViNE hides system-level complexities, freeing the scientist from concerns about inter-tool connectivity, data distribution, data management, and machine idiosyncrasies. It thus extends the notebook abstraction to support a broad range of scientific computational activities in a uniform manner.</abstract>
	<keywords>Distributed computing; Electronic notebook; Computational science; Tools; Experiments; Web</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=546>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Airport simulation using CORBA and DIS</paper_heading>
	<authors>Günther Rackl, Filippo de Stefani, Francois Héran, Antonello Pasquarelli, Thomas Ludwig</authors>
	<abstract>This paper presents the SEEDS simulation environment for the evaluation of distributed traffic control systems. Starting with an overview of the general simulator architecture, performance measurements of the simulation environment carried out with a prototype for airport ground-traffic simulation are described. The main aspects of the performance analysis are the attained application performance using CORBA and DIS as communication middleware, and the scalability of the overall approach. The evaluation shows that CORBA and DIS are well suited for distributed interactive simulation purposes because of their adequate performance, high scalability, and the high-level programming model which allows to rapidly develop and maintain complex distributed applications.</abstract>
	<keywords>Distributed interactive simulation; Airport simulation; Realtime; CORBA; DIS</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=547>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A data intensive distributed computing architecture for “Grid” applications</paper_heading>
	<authors>Brian Tierney, William Johnston, Jason Lee, Mary Thompson</authors>
	<abstract>Modern scientific computing involves organizing, moving, visualizing, and analyzing massive amounts of data from around the world, as well as employing large-scale computation. The distributed systems that solve large-scale problems will always involve aggregating and scheduling many resources. Data must be located and staged, cache and network capacity must be available at the same time as computing capacity, etc. Every aspect of such a system is dynamic: locating and scheduling resources, adapting running application systems to availability and congestion in the middleware and infrastructure, responding to human interaction, etc. The technologies, the middleware services, and the architectures that are used to build useful high-speed, wide area distributed systems, constitute the field of data intensive computing. This paper explores some of the history and future directions of that field, and describes some specific application examples.</abstract>
	<keywords>Distributed computing; Computational grid; Distributed parallel storage</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=548>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Buffer management in wormhole-routed torus multicomputer networks</paper_heading>
	<authors>Kamala Kotapati, Sivarama P. Dandamudi</authors>
	<abstract>This paper focuses on buffer management issues in wormhole-routed torus multicomputer networks. The commonly used buffer organizations are the centralized and dedicated buffer organizations. The results presented in this paper indicate that the dedicated buffer organization provides better performance than the centralized organization under uniform traffic. Under high hot-spot traffic, the centralized organization outperforms the dedicated. The hybrid buffer organization proposed in this paper inherits the merits of the centralized and distributed organizations. Our results suggest that the hybrid buffer organization can be designed to configure dynamically from the dedicated to centralized depending on the traffic conditions.</abstract>
	<keywords>Wormhole routing; Torus networks; Multicomputers; Buffer management; Hot-spot traffic; Negative-hop algorithm; Interconnection networks</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=549>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Network interface active messages for low overhead communication on SMP PC clusters</paper_heading>
	<authors>Motohiko Matsuda, Yoshio Tanaka, Kazuto Kubota, Mitsuhisa Sato</authors>
	<abstract>A communication layer NICAM is designed to reduce the overhead and latency by directly utilizing a micro-processor equipped on the network interface. NICAM runs active messages handlers on the network interface for flexibility in programming. Running message handlers on the network interface reduces the overhead by freeing the main processors from the work of polling incoming messages, and also reduces the latency in synchronizations by avoiding costly interactions between the main processors and the network interface. Moreover, this makes it possible to hide latency of barriers completely in data-parallel programs, because barriers can be performed in the background of the main processors.</abstract>
	<keywords>Cluster computing; Low-overhead communication; Barrier synchronization; Active messages; Latency hiding</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=550>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Link-time effective whole-program optimizations</paper_heading>
	<authors>Andrea G.M Cilio, Henk Corporaal</authors>
	<abstract>The use of a standard binary format in the later part of code generation promotes efficiency and interchangeability of tools, but leaves little information on the source file in the machine code representation. We propose a new approach to code generation, based on a single, highly structured internal format used during compilation proper, machine code generation and linkage. This format offers new opportunities for whole-program optimizations. We have implemented and tested a code generator based on this format. Although the use of a traditional binary format is more efficient, we believe that the increase in code size and compilation times are largely compensated by the opportunities offered by this new trajectory. To support this assertion, we discuss some of its possible applications and show the performance potential of one of them.</abstract>
	<keywords>Retargetable compilation; Embedded code generation; Code optimization; Register allocation</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=551>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulating synthetic polymer chains in parallel</paper_heading>
	<authors>Bernd Jung, Hans-Peter Lenhof, Peter Müller, Christine Rüb</authors>
	<abstract>We have investigated algorithms that are particularly suited for the parallel molecular dynamics (MD) simulations of synthetic polymers. These algorithms distribute the atoms of the polymer among the processors. Dynamic non-bonded interactions, which are the difficult part of an MD simulation, are realised with the help of a new coarse-grained representation of the chain structure. We have devised and compared a master version and a distributed version of the algorithm. Surprisingly, the master version is competitive for a relatively large number of processors. We also developed two methods to improve load balancing. The resulting simulation package will be made available in the near future.</abstract>
	<keywords>Load balancing; Molecular dynamics Parallel; Polymers; Simulation</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=552>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An object oriented approach to lattice gas modeling</paper_heading>
	<authors>Alexandre Dupuis, Bastien Chopard</authors>
	<abstract>We present a parallel library which can be used for any lattice gas (LG) application. A highly reusable implementation, as well as a general parallelization scheme, based on a graph partitioning technique are developed. We show that the performance we obtain with our approach compares favorably with the plain, classical implementation of LG models on regular domains and can be even better for irregular domains. We propose a theoretical expression for the execution time and we validate our analysis in the case of a specific application, namely wave propagation in urban areas.</abstract>
	<keywords>Lattice gas modeling; Object oriented programming; Parallel computing; Graph partitioning; Performance model</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=553>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel algorithm for 3D reconstruction of angiographic images</paper_heading>
	<authors>R. Rivas, M.B. Ibáñez, Y. Cardinale, P. Windyga</authors>
	<abstract>Accurate diagnosis and therapeutic evaluation of coronary dysfunction is possible by tri-dimensional (3D) visualization of Coronary arteries. Reconstruction based on bi-dimensional (2D) images can be presented as a discrete optimization problem. A blind search cannot be applied, instead a Branch-and-Bound algorithm is used to explore the state space and give an intermediate result. The heuristic information used is based on 2D and 3D a priori knowledge. A sequential algorithm using suitable filters leads to implementations where the execution time is measured in days. In order to minimize the execution time we propose to apply parallel computing techniques. The critical issue in parallel search algorithms is the distribution of the search space among the processors. We propose a technique to compute the total amount of work units among the processors. The technique is based on the enlargement of segments (unitary threads) representing pieces of arteries. We achieve a good load balancing and the speedup obtained is nearly optimum.</abstract>
	<keywords>Angiographic images reconstruction; Parallel application; Load balancing</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=554>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallelization of plasma simulation codes: gridless finite size particle versus particle in cell approach</paper_heading>
	<authors>S. Briguglio, G. Vlad, B. Di Martino, G. Fogaccia</authors>
	<abstract>The main features of gridless finite-size-particle codes are examined and compared with those of particle-in-cell ones, from the point of view of the performances that can be obtained, with respect to both the spatial-resolution level and the efficiency of parallel particle simulations. The particle-decomposition parallelization strategy is discussed, and its implementation in the framework of the High Performance Fortran paradigm is presented. It is shown that such codes are particularly suited for particle-decomposition parallelization on distributed-memory architectures, as they present a strong reduction, in comparison with particle-in-cell codes, of the memory and computational offsets related to storing and updating the replicated fluctuating-field arrays.</abstract>
	<keywords>Plasma; Particle simulation; Particle-in-cell; Finite-size-particle; Parallelization; Distributed memory; HPF</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=555>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance assessment of parallel spectral analysis: towards a practical performance model for parallel medical applications</paper_heading>
	<authors>F Munz, T Ludwig, S Ziegler, P Bartenstein, ... A Bode</authors>
	<abstract>We present the first parallel medical application for the analysis of dynamic positron emission tomography (PET) images together with a practical performance model. The parallel application may improve the diagnosis for a patient (e.g. in epilepsy surgery) because it enables the fast computation of parametric images on a pixel level as opposed to the traditionally used region of interest (ROI) approach which is applied to determine an average parametric value for a particular anatomic region of the brain. We derive the performance model from the application context and show its relation to abstract machine models. We demonstrate the accuracy of the model to predict the runtime of the application on a network of workstations and use it to determine an optimal value in the message frequency–size relationship.</abstract>
	<keywords>Functional imaging; Kinetic modeling; Practical performance prediction; Network of workstations; LogP; BSP; PPM</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=556>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Adaptive scheduling strategy optimizer for parallel rolling bearing simulation</paper_heading>
	<authors>Dag Fritzson, Patrik Nordling</authors>
	<abstract>Rolling bearing simulations are very computationally intensive and need to utilize the potential of parallel computing. The load distribution over the processors in a rolling bearing simulation is very dynamic. In this paper we present the Adaptive Scheduling Strategy Optimizer (ASSO) for scheduling parallel simulations. The result of this is that the application can automatically select a near optimal scheduling strategy (with respect to the available scheduling strategies). The ASSO is used daily in real bearing simulations.</abstract>
	<keywords>Rolling bearing; Multi-body system; Simulation; Scheduling; Automatic scheduling</keywords>
	<publication_month_year>2000-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 5</volumes_issues>
</paper>
<paper no=557>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>N-MAP — an environment for the performance oriented development process of efficient distributed programs</paper_heading>
	<authors>Alois Ferscha, James Johnson</authors>
	<abstract>The performance engineering activities in a performance oriented development process of distributed (and parallel) software have to cover a range of crucial issues: performance prediction in early development stages, (analytical or simulation) modeling in the detailed specification and coding phase, monitoring/measurements in the testing and correction phase, and — most importantly for distributed applications executing in heterogeneous distributed environments — automated performance management at run-time. In this paper we present a development environment, N-MAP (N-(virtual) processor map), to tackle these issues. Within N-MAP, the challenging aspect of performance prediction to support a performance oriented, incremental development of distributed programs is addressed, such that design choices can be investigated far ahead of the full coding of the application. Our approach considers the algorithmic idea as the first step towards an application, for which the programmer should not be forced to provide detailed program code, but just to focus on the constituent and performance critical program parts. Due to time-varying workloads and changing system resource availability in heterogeneous multicomputer environments in which the distributed application is embedded, self-managing applications are demanded which allow for dynamic reconfiguration in response to a new system states. N-MAP aims at the development of applications able to induce and assess the impact of such re-configurations at run-time in a “pro-active” way. As opposed to “re-active” systems which bring reconfigurations in effect after the system state has changed, the timeliness of reconfiguration activities is attempted by a future state forecast mechanism and performance management decisions are based on anticipated future system states.</abstract>
	<keywords>N-(virtual) processor map; Performance oriented development process; Distributed programs</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=558>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>EDPEPPS: an integrated environment for the parallel development life-cycle</paper_heading>
	<authors>T. Delaitre, M.J. Zemerly, G.R. Justo, O. Audo, S.C. Winter</authors>
	<abstract>This paper describes an environment for performance-oriented design of portable parallel software. The environment consists of a graphical design tool for building parallel algorithms, a state-of-the-art simulation engine, a CPU characterisation tool, a distributed debugging tool and a visualisation/replay tool. The environment is used to model a virtual machine composed of a cluster of heterogeneous workstations interconnected by a local area network. The simulation model used is modular and its components are interchangeable which allows easy re-configuration of the platform. The model is validated using experiments on two parallel Givens linear solver algorithms with average errors of about 8%.</abstract>
	<keywords>Performance prediction; Graphical design; Simulation Visualisation; Parallel programming environments</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=559>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Graph based characterization of distributed applications</paper_heading>
	<authors>Gabriele Kotsis, Markus Braun</authors>
	<abstract>A critical task in the development and execution of distributed applications is to identify the potential degree of parallelism contained in the application. This information is necessary in the design of applications in order to pursue only a promising algorithmic idea for implementation, but also in the execution of existing applications for resource allocation and scheduling decisions. In this paper, we present analytical techniques to derive the potential degree of parallelism of distributed applications described by means of timed structural parallelism graphs (TSPGs). A TSPG allows a specification of a distributed application in terms of its components, the activation and dependence relations among the components, and histogram/interval based estimates on the execution times of components. Based on an analysis of paths through the TSPG (corresponding to paths in the execution) and by applying interval arithmetics, we are able to derive from the TSPG model a set of potential parallelism profiles. From these profiles further performance indices as the average degree of parallelism as well as hypothetical speedup can be derived. In this paper we focus on an evaluation of the analysis technique with respect to its computational complexity and validate the proposed approach by a comparison with results obtained from simulation.</abstract>
	<keywords>Graph based characterization; Distributed applications; Computational complexities</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=560>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Systematic macrostep debugging of message passing parallel programs</paper_heading>
	<authors>P. Kacsuk</authors>
	<abstract>The paper introduces the concept of collective breakpoints and classifies the possible parallel breakpoints comparing their mechanisms. Based on the collective breakpoints the macrostep-by-macrostep debugging mode has been defined. After introducing the concept of the execution tree and meta-breakpoints a novel systematic debugging methodology of message passing parallel programs is explained. Finally, an algorithm is shown how to generate automatically the collective breakpoints in the GRADE graphical parallel programming environment.</abstract>
	<keywords>Parallel programming; Message passing programs; Debugging</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=561>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Suitability of the time controlled environment for race detection in distributed applications</paper_heading>
	<authors>Henryk Krawczyk, Bartosz Krysztop, Jerzy Proficz</authors>
	<abstract>The paper considers testing problem of time dependent errors detection in distributed applications. We determine time conditions for race occurrence and show how to obtain them in distributed environment. Then race detection testing strategies (RDS) following from the concept of a time controlled environment (TCE) is presented. Besides, we prepared some experiments to evaluate suitability of TCE and capability of RDS for improving reliability of distributed software applications.</abstract>
	<keywords>Distributed application; Message passing; Time dependent errors; Race detection</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=562>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Enhanced monitoring in the GRADE programming environment by using OMIS</paper_heading>
	<authors>Roland Wismüller, Gábor Dózsa, Dániel Drótos</authors>
	<abstract>To provide high-level graphical support for developing message passing programs, an integrated programming environment (GRADE) is being developed. GRADE provides tools to construct, execute, debug, monitor and visualise message-passing based parallel programs. The paper describes the integration of GRADE and an OMIS compliant monitor system. OMIS is a recent specification of a universal on-line monitoring interface for parallel and distributed programs. The integration provides GRADE with a more flexible monitoring support and makes possible the simultaneous usage of debugging and visualisation tools. The paper specifically addresses the requirements imposed by GRADE on the on-line event tracing mechanisms, as well as their implementation and performance.</abstract>
	<keywords>Message-passing; Visual programming; Parallel programming environments; Monitoring; Event tracing; Debugging; Performance visualisation</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=563>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel and distributed implementation of large industrial applications</paper_heading>
	<authors>Peter Luksch</authors>
	<abstract>Parallelization of high performance computing applications has been a field of active research for quite some time now. Most projects that have parallelized industrial software packages have focused on the specific application and did not attempt to document and generalize their lessons learned. We report on results of a project that has parallelized a state of the art industrial computational fluid dynamics (CFD) packages and that explicitly aimed at establishing software engineering guidelines for future similar projects. Thanks to the consequent application of the software engineering guidelines defined for the project, the parallel CFD code has proven excellent efficiency and scalability on a large number of parallel hardware platforms. The project also addressed software engineering issues such as object orientation and resource management. The CFD package has been redesigned as an object oriented program and implemented in C++ and Java. The object oriented CFD program has shown reasonable efficiency in preliminary benchmark tests. We expect that further optimizations to the OO code and advances in compiler technology will make the performance gap (relative to the fortran version) almost disappear in the near future. A resource manager has been developed that allows production runs of parallel scientific computing software to execute in batch mode on networks of workstations by dynamically allocating resources to parallel batch jobs that currently are not claimed by interactive users. The resource manager has been successfully tested with the parallel CFD code as workload.</abstract>
	<keywords>Parallel processing; Scientific computing; Software engineering; CFD; Object oriented programming; Resource management</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=564>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Managing the operator ordering problem in parallel databases</paper_heading>
	<authors>Harald Kosch</authors>
	<abstract>This paper focuses on parallel query optimization. We consider the operator problem and introduce a new class of execution strategies called Linear-oriented Bushy Trees (LBT). Compared to the related approach of the General Bushy Trees (GBT) a significant complexity reduction of the operator ordering problem can be derived theoretically and demonstrated experimentally (e.g. compared with GBTs, LBTs authorize optimization time improvement that can reach up to 49%) without losing quality. Finally we demonstrate that existing commercial parallel query optimizers need little extension mod ifications in order to handle LBTs.</abstract>
	<keywords>Parallel databases; Parallel query optimization; Linear-oriented Bushy Trees; Extending existing optimizers</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=565>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Logicflow execution model for parallel databases</paper_heading>
	<authors>Peter Kacsuk, Norbert Podhorszki</authors>
	<abstract>The LOGFLOW parallel Prolog system is similar to the recent parallel database systems concerning its dataflow execution model and its capability of running on shared-nothing architectures. In this paper the abstract execution and abstract machine models of LOGFLOW are examined from a database point of view. Transformations of relational operators into the Logicflow Graph representation of Prolog programs are explained. Thus, LOGFLOW can operate as a relational database machine.</abstract>
	<keywords>Parallel databases; Dataflow execution model; Shared-nothing architecture</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=566>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The replicator coordination design pattern</paper_heading>
	<authors>Heidemarie Wernhart, eva Kühn, Georg Trausmuth</authors>
	<abstract>The problem domain addressed in this paper refers to scenarios where several databases, which reside on different computers, platforms or database systems have to be kept synchronized. A solution design to this problem is presented as a coordination design pattern. A coordination design pattern describes the design of a solution to a recurring problem from distributed or parallel processing, where the solution is based on multiple processes that are coordinated using shared data objects. The coordination framework Corso is introduced as a possible implementation environment. The design pattern called Replicator is motivated with a scenario using distributed library databases.</abstract>
	<keywords>Design patterns; Heterogeneous databases; Replication; Virtual shared memory</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=567>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Abstract machine design on a multithreaded architecture</paper_heading>
	<authors>Zsolt Németh</authors>
	<abstract>Usual approaches of designing a Prolog abstract machine primarily aimed at different kinds of parallelism to be exploited (e.g. AND, OR-parallelism). Our novel approach focuses rather on some specific features, like macro-dataflow properties derived from the Prolog language. The work is aimed at finding a proper relationship between the essential characteristics of the macro-dataflow computational model and the underlying architecture. In this paper a part of this design work is introduced, where the variable binding method is established and its possible effect to the performance is analysed.</abstract>
	<keywords>Prolog abstract machine; Multithreading; Variable binding; Performance prediction</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=568>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed federative QoS resource management</paper_heading>
	<authors>Günther Hölzl, László Böszörményi</authors>
	<abstract>In a distributed multimedia system QoS resources have to be managed carefully to utilize the resource pool in a way that bottlenecks can be avoided. Our key idea is to let the applications participate on the resource management. We propose a distributed architecture with a fine granulated, balanced resource management with explicit QoS characteristics. The architecture is based on a distributed cooperative resource manager which combines both the adaption and reservation principle for guaranteeing QoS. We have designed and implemented a prototype of our federative QoS resource manager (FQRM) in the Java environment.</abstract>
	<keywords>QoS resource management; Distributed resources; Cooperative resource sharing</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=569>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The FTA design paradigm for distributed systems</paper_heading>
	<authors>Holger Veit, Gernot Richter</authors>
	<abstract>The Flip-Tick Architecture, or FTA for short, is a collection of organizing principles and generic functional units for building distributed systems which are based on cooperative and competitive teams of small or medium-grained agents called actors. We describe the design and operation principles of the architecture and present some ideas of how to build applications with an FTA structure. A typical application system comprises tens or hundreds of actors sharing one or several common data storages known as tagboards. There is no explicit point-to-point communication between actors. Instead, actors communicate by reading and posting tags with data on the tagboards using various mechanisms for synchronization. The distribution across several interconnected computers as well as the choice of actor granularity and synchronism are up to the application designers.</abstract>
	<keywords>Architectures; Distributed operating systems; Distributed problem solving; Distributed algorithms; Multiagent systems</keywords>
	<publication_month_year>2000-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 6</volumes_issues>
</paper>
<paper no=570>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Abstract machines for programming language implementation</paper_heading>
	<authors>Stephan Diehl, Pieter Hartel, Peter Sestoft</authors>
	<abstract>We present an extensive, annotated bibliography of the abstract machines designed for each of the main programming paradigms (imperative, object oriented, functional, logic and concurrent). We conclude that whilst a large number of efficient abstract machines have been designed for particular language implementations, relatively little work has been done to design abstract machines in a systematic fashion.</abstract>
	<keywords>Abstract machine; Compiler design; Programming language; Intermediate language</keywords>
	<publication_month_year>2000-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 7</volumes_issues>
</paper>
<paper no=571>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Abstract machine construction through operational semantics refinements</paper_heading>
	<authors>Frédéric Cabestre, Christian Percebois, Jean-Paul Bodeveix</authors>
	<abstract>This article describes the derivation of an abstract machine from an interpreter describing the operational semantics of a source language. This derivation process relies on the application of a set of gradual transformations to the interpreter written in a functional language. Through pass separation, the derivation process leads to the extraction of a compiler and an abstract machine from the transformed interpreter.</abstract>
	<keywords>Abstract machine; Semantics; Pass separation; Compiler</keywords>
	<publication_month_year>2000-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 7</volumes_issues>
</paper>
<paper no=572>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Compilation versus abstract machines for fast parsing of typed feature structure grammars</paper_heading>
	<authors>John C. Brown, Suresh Manandhar</authors>
	<abstract>onstraint-based grammars based on Head Driven Phrase Structure Grammar employ typed feature structures (TFSs) which are often deeply nested. Unification in Prolog (B. Carpenter, G. Penn, ALE: The Attribute Logic Engine User’s Guide, version 2.0, Technical report, Philosophy Department, Carnegie Mellon University, Pittsburgh, PA, 1994) is then too slow for them to be used in real products. A recent abstract machine (S. Wintner, N. Francez, Efficient compilation of unification-based grammars, in: BCN Summer School 97 on Topics in Constraint-Based Natural Language Processing, Groningen University, 1997, pp. 90–126) translates grammatical descriptions of TFSs into abstract code, with parse times between 3 and 11 times faster, for sentences between 4 and 12 words. We present a new faster method that allocates atomic composite types to TFSs, precompiles tables that replace unification by matching, and represents semantics in a compact skeletal form. A Prolog prototype already parses faster than the existing abstract machine, and would be even faster in an imperative implementation using arrays.</abstract>
	<keywords>Constraints; Types; Feature structures; HPSG; ALE</keywords>
	<publication_month_year>2000-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 7</volumes_issues>
</paper>
<paper no=573>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An abstract machine model of dynamic module replacement</paper_heading>
	<authors>Chris Walton, Dilsun Kırlı, Stephen Gilmore</authors>
	<abstract>In this paper we define an abstract machine model for the mλ typed intermediate language. This abstract machine is used to give a formal description of the operation of run-time module replacement for the programming language Dynamic ML. The essential technical device which we employ for module replacement is a modification of two-space copying garbage collection. We show how the operation of module replacement could be applied to other garbage-collected languages such as Java.</abstract>
	<keywords>Standard ML; Copying garbage collection; Typed abstract machine; Dynamic module replacement; Java</keywords>
	<publication_month_year>2000-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 7</volumes_issues>
</paper>
<paper no=574>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>CHAT: the copy-hybrid approach to tabling</paper_heading>
	<authors>Bart Demoen, Konstantinos Sagonas</authors>
	<abstract>The copying approach to tabling (CAT) is an alternative to SLG-WAM and based on incrementally copying the areas that the SLG-WAM freezes to preserve execution states of suspended computations. The main advantage of CAT over the SLG-WAM is that support for tabling does not affect the speed of the underlying abstract machine for strictly non-tabled execution. The disadvantage of CAT as pointed out in a previous paper is that in the worst case, CAT must copy so much that its tabled execution becomes arbitrarily worse than that of the SLG-WAM. Remedies to this problem have been studied, but a completely satisfactory solution has not emerged. Here, a hybrid approach is presented: abbreviated as CHAT. Its design was guided by the requirement that for non-tabled (i.e. Prolog) execution no changes to the underlying WAM engine need to be made. CHAT not only combines certain features of the SLG-WAM with features of CAT, but also introduces a technique for freezing WAM stacks without the use of the SLG-WAM’s freeze registers that is of independent interest. This article describes only the basic CHAT mechanism which allows for programs that perform arbitrarily worse than under the SLG-WAM. However, empirical results indicate that even basic CHAT is a better choice for implementing the control of tabling than SLG-WAM or CAT.</abstract>
	<keywords>Abstract machine; Prolog; Tabling; WAM</keywords>
	<publication_month_year>2000-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 7</volumes_issues>
</paper>
<paper no=575>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visualizing principles of abstract machines by generating interactive animations</paper_heading>
	<authors>Stephan Diehl, Thomas Kunze</authors>
	<abstract>In this paper we describe the design rationale of GANIMAM, a web-based system which generates interactive animations of abstract machines from specifications. Common principles of abstract machines come into play at three levels: the design of the specification language, the choice of graphical annotations to visualize higher-level abstractions and the use of the system to explore and better understand known and detect new principles.</abstract>
	<keywords>Abstract machines; Software visualization; Animation</keywords>
	<publication_month_year>2000-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 7</volumes_issues>
</paper>
<paper no=576>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Dynamic semantics of Java bytecode</paper_heading>
	<authors>Peter Bertelsen</authors>
	<abstract>We give a formal specification of the dynamic semantics of Java bytecode, in the form of an operational semantics for the Java Virtual Machine (JVM). For each JVM instruction we give a rule describing the instruction’s effect on the machine state and the conditions under which the instruction will execute without error. This paper outlines the formalization of the JVM machine state and illustrates our approach for a few select JVM instructions. Our full specification, covering the entire JVM instruction set except for synchronization instructions, is available in the work of Bertelsen (Semantics of Java byte code, Technical Report, Department of Mathematics and Physics, Royal Veterinary and Agricultural University, Copenhagen, Denmark, April 1997).</abstract>
	<keywords>JavaJava Virtual Machine; Formal specification; Semantics</keywords>
	<publication_month_year>2000-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 7</volumes_issues>
</paper>
<paper no=577>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Ant algorithms and stigmergy</paper_heading>
	<authors>Marco Dorigo, Eric Bonabeau, Guy Theraulaz</authors>
	<abstract>Ant colonies, and more generally social insect societies, are distributed systems that, in spite of the simplicity of their individuals, present a highly structured social organization. As a result of this organization, ant colonies can accomplish complex tasks that in some cases far exceed the individual capacities of a single ant. The study of ant colonies behavior and of their self-organizing capacities is interesting for computer scientists because it provides models of distributed organization which are useful to solve difficult optimization and distributed control problems. In this paper we overview some models derived from the observation of real ants, emphasizing the role played by stigmergy as distributed communication paradigm, and we show how these models have inspired a number of novel algorithms for the solution of distributed optimization and distributed control problems.</abstract>
	<keywords>Ant algorithms; Ant colony optimization; Swarm intelligence; Social insects; Self-organization; Metaheuristics</keywords>
	<publication_month_year>2000-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 8</volumes_issues>
</paper>
<paper no=578>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A Graph-based Ant System and its convergence</paper_heading>
	<authors>Walter J. Gutjahr</authors>
	<abstract>A general framework for solving combinatorial optimization problems heuristically by the Ant System approach is developed. The framework is based on the concept of a construction graph, a graph assigned to an instance of the optimization problem under consideration, encoding feasible solutions by walks. It is shown that under certain conditions, the solutions generated in each iteration of this Graph-based Ant System converge with a probability that can be made arbitrarily close to 1 to the optimal solution of the given problem instance.</abstract>
	<keywords>Heuristic; Ant System; Ant colony optimisation; Combinatorial optimisation; Markov process</keywords>
	<publication_month_year>2000-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 8</volumes_issues>
</paper>
<paper no=579>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>MAX–MIN Ant System</paper_heading>
	<authors>Thomas Stützle, Holger H. Hoos</authors>
	<abstract>Ant System, the first Ant Colony Optimization algorithm, showed to be a viable method for attacking hard combinatorial optimization problems. Yet, its performance, when compared to more fine-tuned algorithms, was rather poor for large instances of traditional benchmark problems like the Traveling Salesman Problem. To show that Ant Colony Optimization algorithms could be good alternatives to existing algorithms for hard combinatorial optimization problems, recent research in this area has mainly focused on the development of algorithmic variants which achieve better performance than Ant System. In this paper, we present – Ant System (), an Ant Colony Optimization algorithm derived from Ant System.  differs from Ant System in several important aspects, whose usefulness we demonstrate by means of an experimental study. Additionally, we relate one of the characteristics specific to  — that of using a greedier search than Ant System — to results from the search space analysis of the combinatorial optimization problems attacked in this paper. Our computational results on the Traveling Salesman Problem and the Quadratic Assignment Problem show that  is currently among the best performing algorithms for these problems.</abstract>
	<keywords>Ant Colony Optimization; Search space analysis; Traveling Salesman Problem; Quadratic Assignment Problem; Combinatorial optimization</keywords>
	<publication_month_year>2000-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 8</volumes_issues>
</paper>
<paper no=580>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ANTS: Agents on Networks, Trees, and Subgraphs</paper_heading>
	<authors>Israel A. Wagner, Michael Lindenbaum, Alfred M. Bruckstein</authors>
	<abstract>Efficient exploration of large networks is a central issue in data mining and network maintenance applications. In most existing work there is a distinction between the active ‘searcher’ which both executes the algorithm and holds the memory and the passive ‘searched graph’ over which the searcher has no control at all. Large dynamic networks like the Internet, where the nodes are powerful computers and the links have narrow bandwidth and are heavily-loaded, call for a different paradigm, in which a noncentralized group of one or more lightweight autonomous agents traverse the network in a completely distributed and parallelizable way. Potential advantages of such a paradigm would be fault tolerance against network and agent failures, and reduced load on the busy nodes due to the small amount of memory and computing resources required by the agent in each node. Algorithms for network covering based on this paradigm could be used in today’s Internet as a support for data mining and network control algorithms. Recently, a vertex ant walk (VAW) method has been suggested [I.A. Wagner, M. Lindenbaum, A.M. Bruckstein, Ann. Math. Artificial Intelligence 24 (1998) 211–223] for searching an undirected, connected graph by an a(ge)nt that walks along the edges of the graph, occasionally leaving ‘pheromone’ traces at nodes, and using those traces to guide its exploration. It was shown there that the ant can cover a static graph within time nd, where n is the number of vertices and d the diameter of the graph. In this work we further investigate the performance of the VAW method on dynamic graphs, where edges may appear or disappear during the search process. In particular we prove that (a) if a certain spanning subgraph S is stable during the period of covering, then the VAW method is guaranteed to cover the graph within time nds, where ds is the diameter of S, and (b) if a failure occurs on each edge with probability p, then the expected cover time is bounded from above by nd((logΔ/log(1/p))+((1+p)/(1−p))), where Δ is the maximum vertex degree in the graph. We also show that (c) if G is a static tree then it is covered within time 2n.</abstract>
	<keywords>Dynamic graph search; Edge failure model; Vertex ant walk; Edge ant walk; Cover time</keywords>
	<publication_month_year>2000-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 8</volumes_issues>
</paper>
<paper no=581>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An ANTS heuristic for the frequency assignment problem</paper_heading>
	<authors>Vittorio Maniezzo, Antonella Carbonaro</authors>
	<abstract>The problem considered in this paper consists in assigning frequencies to radio links between base stations and mobile transmitters in order to minimize the global interference over a given region. This problem is NP-hard and few results have been reported on techniques for solving it to optimality. We have applied to this problem an ANTS metaheuristic, that is, an approach following the ant colony optimization paradigm. Computational results, obtained on a number of standard problem instances, testify the effectiveness of the proposed approach.</abstract>
	<keywords>Frequency assignment problem; Ant colony optimization; Metaheuristic algorithms</keywords>
	<publication_month_year>2000-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 8</volumes_issues>
</paper>
<paper no=582>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On how Pachycondyla apicalis ants suggest a new search algorithm</paper_heading>
	<authors>N. Monmarché, G. Venturini, M. Slimane</authors>
	<abstract>In this paper we present a new optimization algorithm based on a model of the foraging behavior of a population of primitive ants (Pachycondyla apicalis). These ants are characterized by a relatively simple but efficient strategy for prey search in which individuals hunt alone and try to cover a given area around their nest. The ant colony search behavior consists of a set of parallel local searches on hunting sites with a sensitivity to successful sites. Also, their nest is periodically moved. Accordingly, the proposed algorithm performs parallel random searches in the neighborhood of points called hunting sites. Hunting sites are created in the neighborhood of a point called nest. At constant intervals of time the nest is moved, which corresponds to a restart operator which re-initializes the parallel searches. We have applied this algorithm, called API, to numerical optimization problems with encouraging results.</abstract>
	<keywords>Pachycondyla apicalis ants; Foraging behavior; Numerical optimization; Ant algorithms</keywords>
	<publication_month_year>2000-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 16, Issues 8</volumes_issues>
</paper>
<paper no=583>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VRML — enhanced learning in biology and medicine</paper_heading>
	<authors>Tomaz Amon, Vojko Valencic</authors>
	<abstract>Biological systems are three-dimensional structures living in three-dimensional space. So it is natural to use VRML as a tool in the biological or medical educational process. Our group has been developing VRML biological worlds for several years (http://verbena.fe.uni-lj.si/∼tomaz/VRML/). We are visualizing biological structures which are difficult to observe without special instruments and processes which are difficult to understand from a textbook alone. We are investigating how to build optimal biological VRML worlds small in file size, but still expressive enough to serve as an effective “fast learning tool” in biology.</abstract>
	<keywords>Education; Visualization; Software; Biology; Medicine; VRML; Cellular structure; Cellular function; Simulation; Cricket; Insect</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=584>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual biochemistry — a case study</paper_heading>
	<authors>Will Rourk</authors>
	<abstract>Two dimensional textbook illustrations provide a limited description of the content being discussed. The academic sciences teach mainly process oriented content that require more than a static image on a page. Dynamic scientific processes need dynamic presentations to fully get the concept across to students. This paper addresses this situation by exploring a case study in which a CD-ROM is being distributed with a textbook intended for the study of biochemistry. VRML and CHIME based 3D models provide students with fully interactive content to ensure an enriching learning experience. The use of these web-based technologies are shown to be important tools for visualization in the classroom.</abstract>
	<keywords>VRML; Education; CD-ROM; Biochemistry; Interactive; Dynamic media; World wide web; 3D content; Textbook illustration; Supplemental information</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=585>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards a visualization of arguing agents</paper_heading>
	<authors>Michael Schroeder</authors>
	<abstract>In this paper, we show how to visualize arguing agents. We describe a visualization of single-agent and multi-agent argumentation. In the former, we show how to visualize the argumentation process and space within one agent, in the latter we develop an animation of communicating agents and map the relations between agents to spatial distance. We discuss the advantages/disadvantages of our solutions and compare it to alternatives.</abstract>
	<keywords>VRML; Visualization; Agents; Multi-agent systems; Argumentation</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=586>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual Computer Project “Delivering content in a VRML world”</paper_heading>
	<authors>Carlo Zinzani</authors>
	<abstract>The main aim of this project is to deliver information within a Virtual Reality Modeling Language environment system. The content of Virtual Computer Project is: Showing and teaching how to build a computer to an end user.</abstract>
	<keywords>3D visualization; 3D communication; 3D user manual</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=587>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VRML as means of expressive 4D illustration in CAM education</paper_heading>
	<authors>Nikolay Avgoustinov</authors>
	<abstract>Computer graphics (CGr) lies in the fundaments of computer aided engineering (CAE). As a result from the fast development in both CGr and CAE, more and more computer aided systems (CA-systems) have been ported on low-end and personal computers. Despite the continuous drop in the CA-system prices, this type of software is still very expensive compared to typical office applications. This paper discusses the possibility to use VRML for scientific, educational and even industrial purposes. As an illustration, it is shown how VRML can be used as inexpensive means for simulation of one of the most interesting but also most time and resource consuming areas of the computer aided manufacturing (CAM) — machining of complex parts.</abstract>
	<keywords>Verification of NC-programs; Education; Simulation</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=588>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the use of VRML in educational software: Experiences from the project: JIMM Problem Solver</paper_heading>
	<authors>Hugo Jonkers</authors>
	<abstract>To demonstrate why education and VRML-97 are a strong combination, the paper describes the needs of modern education for interactive applications with a great degree of freedom. Some arguments in favor of VRML, derived from the project ‘JIMM Problem Solver’, will illustrate the role VRML can play in education. Education, however, does need some features that are not yet offered by VRML. Finally some issues regarding the design of 3D-widgets are discussed. During the presentation the use of 3D-widgets in our project will be demonstrated.</abstract>
	<keywords>EducationInteraction; Manipulation in 3D; Modeling; Multimedia; VRML</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=589>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>WonderSpace: web based humanoid animation</paper_heading>
	<authors>Toshiya Naka, Yoshiyuki Mochizuki, Shigeo Asahara</authors>
	<abstract>In VRML, a modeling language for 3D objects on the Internet, the specification to realize life-like movement of a 3D character with a skeletal structure (such as a human) has been standardized as the VRML Humanoid Animation (H-Anim) Specification Ver.1.0 in the H-Anim WG of the VRML Consortium. To extend this specification, we suggest a method named “WonderSpace”, it is possible to send/receive motion data in real time on a network with a narrow band width such as a telephone line. Moreover, by sending motion data with streaming data from server to client, the time required before playback can be greatly reduced.</abstract>
	<keywords>Virtual worlds; Streaming; Animation; VRML; Humanoid character; Internet</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=590>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Avatar as Content Delivery Platform</paper_heading>
	<authors>Steve Guynup, Kyle Carlson</authors>
	<abstract>Traditionally, an Avatar has been viewed as a representation of an individual, a humanoid representation. This approach applies unnecessary restrictions to the Avatar’s usefulness. Avatars, when viewed in the abstract, are Content Delivery Platforms. They are a point to distribute and to receive data. The humanoid frame simply provides a comfortable metaphor for this process. By looking beyond the human frame, we find a vast new area of interactivity and expressive power. Like a genie, Avatars can become any type of model, graphic or animation. They can do this in sync with any type of oral or text based presentation. They would inherently blend the best elements of three-dimensional interactive environments with human sensitivity and human flexibility. Furthermore, this approach is not something for tomorrow — it is something we can do now. We have successfully demonstrated it in practical application, an online multi-user virtual classroom environment.</abstract>
	<keywords>VRML; Avatar; Education; Teaching; Virtual reality</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=591>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>3D web environment for knowledge management</paper_heading>
	<authors>Ryo Yoshida, Takaaki Murao, Tatsuo Miyazawa</authors>
	<abstract>Our module for establishing the shared-state of 3D objects, which supports the LivingWorlds specification, is integrated with a VRML browser plugged into a web browser and a web server, as a 3D web environment system for knowledge management in an industrial manufacturing process. Users will be able to record the minutes of their online meetings using 3D models, which may include annotations or object behaviors, and will be able to store them in a database with related documents in preparation for another meeting. In this report, we discuss the direction of our work and the concept underlying the 3D web environment system design.</abstract>
	<keywords>Virtual Reality Modeling Language (VRML); Living; Worlds; Multi-user environments; Knowledge management</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=592>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Educational use of VRML and Java in agent-based AI and computer graphics</paper_heading>
	<authors>Jan-Torsten Milde, Bernhard Jung</authors>
	<abstract>This paper describes a VRML2 and Java-based software environment used in a course on interactive computer graphics and agent-oriented Artificial Intelligence (AI) held at MultiMedia Laboratory of the University of Bielefeld in the summer term 1998. A virtual environment was developed that can be populated with multiple, autonomous and communicative agents. Agents have limited perception of their environment and pursue goals like picking up objects. Environment simulation, agent control, and visualization are distributed using a client/server approach. Implementing their own agents, students were able to become familiar with key concepts of agent-oriented AI, such as autonomy, sociability, reactiveness, and proactiveness.</abstract>
	<keywords>Virtual Reality Modeling Language (VRML); Living; Worlds; Multi-user environments; Knowledge management</keywords>
	<publication_month_year>2000-09-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 1</volumes_issues>
</paper>
<paper no=593>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Collaborative distributed network system: a lightweight middleware supporting collaborative DEVS modeling</paper_heading>
	<authors>Hessam S Sarjoughian, Bernard P Zeigler, Sunwoo Park</authors>
	<abstract>The present and future of our world, in part, depends on the effective application of problem solving techniques and tools that are able to bring together expertise from many disciplines, e.g. problems such as how world wide distribution of commodities are inherently multidisciplinary in nature. As such they demand collaborative computer-based tools that enable subject matter experts to work cooperatively while overcoming the constraints of space and time. The realization of collaborative tools can benefit greatly by relying on “out-of-the-box” higher level network interconnectivity software. In this paper, we discuss the collaborative distributed network model (CDNM). Based on this model, a lightweight middleware called collaborative distributed network system (CDNS) is implemented using Java technologies. We discuss CDNM’s architecture and the services it can provide to modeling and simulation applications built on its foundation. Furthermore, we discuss Collaborative DEVS Modeler, a collaborative modeling environment, where dispersed clients interact with one another using the services offered by the CDNS. The explicit architectural design of CDNM will facilitate its future migration into emerging middleware standards such as High Level Architecture (HLA) and CORBA.</abstract>
	<keywords>Architecture; Collaboration; CSCW; Design; Distributed computing; Java; Middleware; Modeling; Network; Simulation</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=594>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Modeling and simulation of mobile agents</paper_heading>
	<authors>Adelinde M Uhrmacher, Petra Tyschler, Dirk Tyschler</authors>
	<abstract>Agent-oriented software implies the realization of software components, which are mobile, autonomous, and solve problems by creating new software components during run-time, moving between locations, initiating or joining groups of other software components. Modeling and simulating multiagent systems requires specific mechanisms for variable structure modeling. JAMES, a Java-Based Agent Modeling Environment for Simulation, realizes variable structure models including mobility from the perspective of single autonomous agents. JAMES itself is based on parallel DEVS and adopts its abstract simulator model. Simulation takes place as a sending of messages between concurrently active and locally distributed entities which reflect the model’s current structure. Thus, modeling and simulation are coined equally by an agent-based perspective.</abstract>
	<keywords>Mobile agents; Variable structure models; Distributed parallel discrete event simulation</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=595>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The JSIM web-based simulation environment</paper_heading>
	<authors>John A Miller, Andrew F Seila, Xuewei Xiang</authors>
	<abstract>In its ideal form, web-based simulation should allow simulation models as well as simulation results to be as readily distributable and composable as today’s web documents. The rapid advances in web technology, most notably Java, are helping to make this a possibility. Support for executable web content, universal portability, component technology, and standard high-level packages for accessing databases and producing graphical user interfaces are important enablers of web-based simulation. Component-based software can be used to develop highly modular simulation environments supporting high reusability of software components. Because of the potentially large scope of web-based simulation, greater demands are placed on simulation environments. They should support rapid visual model development, access to local and remote databases, techniques for executing models or federations of models in a variety of ways, and embedding of simulation within larger systems. The use of component technology in the JSIM web-based simulation environment allows simulation models to be treated as components that can be dynamically assembled to build model federations. It also allows simulation inputs and outputs to be dynamically linked to database systems, making storage of simulation results easy and flexible.</abstract>
	<keywords>Web-based simulation; Simulation environments; Component software; Java; Java beans</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=596>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A geographically distributed enterprise simulation system</paper_heading>
	<authors>Heidi R Ammerlahn, Michael E Goldsby, Michael M Johnson, David M Nicol</authors>
	<abstract>Sandia National Laboratories has the role of system integrator in the US Department of Energy’s Nuclear Weapons Complex (NWC) and the responsibility for maintaining the nuclear stockpile. Maintenance is a complex task that involves a great number of geographically distributed functions, and a variety of analysis tools and models are used to plan operations, assess capabilities and guide political decisions. We are developing a framework for modeling the NWC unde interactive human control. The framework consists of a simulation system whose users and components are distributed across the entire United States. The purpose of the system is to support decisions about questions ranging from low-level operational matters involving only a sub-component of the complex to high-level policy issues requiring simulation of the entire complex. The system integrates multiple domain-specific legacy models and policy models and allows simulation at varying levels of detail. To provide such a framework, we have had to address a number of technical problems, whose solution is the focus of this paper. The three most fundamental problems are to provide geographically distributed, synchronized human participation in the simulation; • to provide distributed ownership and security in such a way that the facility hosting a domain-specific model retains all rights to provide or deny a user access to its information; and • to integrate legacy models written in a variety of tools or languages and executed on different platforms. Key features of our approach are the use of the Java language and the coordination of the simulation through a new parallel discrete-event simulation system.</abstract>
	<keywords>Enterprise modeling; Parallel discrete-event simulation; Human-in-the-loop; Distributed role-based access control; Java; Security; Legacy systems</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=597>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Surgical training on the web</paper_heading>
	<authors>Nuha H. El-Khalili, Ken W. Brodlie</authors>
	<abstract>The World Wide Web has become part of our lives by providing services across a wide spectrum of fields. However, the capabilities of the web have not yet been fully exploited. Two years ago the web was largely a distributed information repository, when it had the potential to become a distributed computing environment. This paper looks at the potential for the web to host scientific applications. We review current technologies, and present examples of web-based applications. Our own interest is surgical simulation as a web-based application, and we describe one particular application, to train radiologists on minimally invasive procedures. This provides a training environment which models anatomical structures as well as surgical tools, and simulates the behaviour of the surgical tool under user manipulation.</abstract>
	<keywords>Web-based applications; Surgical training; Physically-based modelling; World Wide Web</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=598>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Investigating the application of web-based simulation principles within the architecture for a next-generation computer generated forces model</paper_heading>
	<authors>Ernest H Page, Jeffrey M Opper</authors>
	<abstract>With a heavy emphasis on distribution and reuse, web-based simulation portends a dramatic shift in the application of simulation as a problem-solving technique and decision-support tool. Next-generation simulation systems of all kinds should be evaluated and constructed with an appreciation of the potential paradigm shift that web-based simulation represents. Fiscal constraints indicate that next-generation computer generated forces (CGF) models will be used to support a wide range of missions, unifying — and replacing — a variety of CGF systems currently in existence. We describe the evolution of web-based simulation and derive a collection of modeling principles that characterize our vision of the web-based simulation future. We examine these principles in terms of their implications for next-generation CGF systems.</abstract>
	<keywords>Computer generated forces; High level architecture; Semi-automated forces; Web-based simulation</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=599>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Designing web-based simulation for learning</paper_heading>
	<authors>Rego Granlund, Erik Berglund, Henrik Eriksson</authors>
	<abstract>Web-based simulation can be a powerful tool in education and training. The nature of simulation-based learning combined with the availability of the web make learning supported by web-based simulation a powerful strategy. In simulation-based learning, learners can experience environments that would be too costly, time-consuming, complex or dangerous to provide through other means. This article discusses some basic properties of learning using web-based simulation with the focus on different types of learning goals (instructional goals) and on proper instructional strategies (pedagogical strategies) for web-based simulation. We exemplify the classifications discussed with three web-based systems, developed by the authors, that represent different types of web-based simulation. Chernobyl — a nuclear power plant simulation. C3Fire — a micro-world supporting command and control training. ERCIS — a group distance-exercise system supporting equipment handling, action-protocol performance and group interaction.</abstract>
	<keywords>Web-based simulation; Distance education; Instructional strategies; Simulation-based learning</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=600>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Discrete-event simulation on the Internet and the Web</paper_heading>
	<authors>Chien-Chung Shen</authors>
	<abstract>Existing software development environments for discrete-event simulation have adopted either a language-based or a library-based approach. Although these approaches have advantages, they suffer from three limitations: lack of portability, lack of interoperability, and difficulty in execution over the Internet/Web infrastructure. The paper addresses these limitations by proposing to facilitate discrete-event simulation as a service on the Internet/Web. The simulation service is defined as a corba facility with a well-defined interface, and components of simulation applications access the facility to obtain execution support. The simulation facility enables portability and interoperability of simulation applications which may be developed in heterogeneous languages and environments and communicate over the Internet. Moreover, by integrating with Java, the approach also enables simulation applications to be developed in Java and executed on the Web. Application of the simulation facility is illustrated by using a bounded-buffer producer/consumer system.</abstract>
	<keywords>corba; Discrete-event simulation; Internet; Java; Web</keywords>
	<publication_month_year>2000-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 2</volumes_issues>
</paper>
<paper no=601>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel image processing with one-dimensional DSP arrays</paper_heading>
	<authors>M.Fikret Ercan, Yu-Fai Fung, M.Suleyman Demokan</authors>
	<abstract>This paper describes a study of parallel image processing algorithms implemented on a one-dimensional DSP array. DSPs are developed for computationally intensive signal processing operations. Recently introduced parallel DSPs can be used for all levels of image processing operations and they provide easy development of a parallel system. In addition, due to the computing power delivered by these processors, we can employ coarse grain parallelism instead of the traditional fine-grain parallelism. Modularity, expandability and easy programming are other advantages of parallel DSPs. In this paper, parallel implementation of some selected image processing algorithms is described and performance results are presented.</abstract>
	<keywords>Parallel architectures; Computer vision; Image processing; Parallel DSP</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=602>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Eager scheduling with lazy retry in multiprocessors</paper_heading>
	<authors>Huey-Ling Chen, Chung-Ta King</authors>
	<abstract>Task scheduling is concerned with the sequence in which tasks entering a multiprocessor system are served. Task scheduling policies can be generally classified as eager scheduling and lazy scheduling. The former attempts to schedule the tasks whenever there are free processors available, while the latter delays the scheduling of some tasks so as to accommodate more appropriate tasks. In this paper we propose a topology-independent hybrid policy that combines the benefits of both. The new scheduling strategy, called the eager scheduling with lazy retry (ESLR) policy, tries to schedule a task eagerly upon its arrival. If the scheduling fails, then the task is rescheduled after a delay period. This delay in rescheduling gives tasks with different processor requests an equal opportunity to compete for free processors. The proposed scheduling policy is independent of the topology of the underlying multiprocessor. To study its performance, we simulated and compared several scheduling policies on hypercube multiprocessors. The results show that this ESLR policy can achieve a better system performance than previous approaches. We will study various considerations in implementing the ESLR policy and discuss their effects on the system performance.</abstract>
	<keywords>Processor allocation; Multiprocessor; Task scheduling; Hypercube; Parallel processing</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=603>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Handling side-effects and cuts with selective recomputation in parallel Prolog</paper_heading>
	<authors>Zhiyi Huang, Chengzheng Sun, Abdul Sattar</authors>
	<abstract>This paper proposes a Selective recomputation (SR) approach to handling side-effects and cut(!) in AND/OR parallel execution of Prolog programs. In contrast to the non-trivial recomputation involved in most of the existing solutions, the SR approach selectively recomputes the side-effect part of AND-parallel child trees in OR-forest while exploiting both independent AND-and OR-parallelism. In this paper, after introducing the background and motivation, we describe the complex side-effects ordering in AND/OR parallel execution models/systems by using an innovative concept – side-effect execution permit token. We then propose the recomputation line to divide an AND-parallel child tree into non-recomputation and recomputation parts. The recomputation is confined to the recomputation part of an AND-parallel child tree, rather than the entire tree. And the non-recomputation part can be explored in parallel, just as a pure child tree. Next, we adopt the delayed execution of soft side-effect built-ins, so that the non-recomputation part is enlarged and thus the recomputation is minimized and parallelism is maximized. Finally, we propose an optimal cut(!) implementation based on the SR approach. Through comparison and analysis, we conclude that the SR approach can solve the side-effect problem with the potential to minimize the recomputation and to maximize the parallelism.</abstract>
	<keywords>Prolog; Side-effects; Parallel processing; AND-parallelism; OR-parallelism; OR-forest</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=604>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Sharing objects in a distributed, single address space environment</paper_heading>
	<authors>Gianluca Dini, Lanfranco Lopriore</authors>
	<abstract>With reference to an object type defining the two basic operations, read and write, we present solutions to the object sharing problem, classified according to the migration and/or replication of the shared objects. We refer to a memory management system supporting a single address space view of storage in a distributed environment. Our system defines a small, powerful set of primitives that allow processes to explicitly control the allocation and deletion of the virtual pages in the physical storage as well as the page movements across the memory hierarchy. By using real programs, we demonstrate that these primitives make it possible to integrate an object sharing algorithm within the implementation of the given object at little programming effort. The discussion takes a number of salient issues into consideration, including network costs and a conceptual framework for actual implementation of the memory management system.</abstract>
	<keywords>Distributed system; Migration; Replication; Shared object; Single address space</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=605>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An efficient logging and recovery scheme for lazy release consistent distributed shared memory systems</paper_heading>
	<authors>Taesoon Park, Heon Y. Yeom</authors>
	<abstract>Checkpointing and logging are widely used techniques to provide fault-tolerance for the distributed systems. However, logging imposes too much overhead on the processing to be a practical solution. In this paper, we propose a low-overhead logging scheme for the distributed shared memory system based on the lazy release consistency memory model. Unlike the previous schemes in which the logging is performed when a new data item is accessed by a process, the stable logging in the proposed scheme is performed only when a lock grant causes an actual dependency relation between the processes, which significantly reduces the logging frequency. Also, instead of making a stable log of the accessed data items, a process logs stably only some access information, and the accessed data items are saved in the volatile log. For the recovery from a failure, the correct version of the accessed data items can be effectively traced by using the logged access information. As a result, the amount of logged information can also be reduced.</abstract>
	<keywords>Distributed shared memory system; Fault-tolerance; Message logging; Lazy release consistency; Rollback-recovery</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=606>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A scalable molecular-dynamics algorithm suite for materials simulations: design-space diagram on 1024 Cray T3E processors</paper_heading>
	<authors>Fuyuki Shimojo, Timothy J. Campbell, Rajiv K. Kalia, Aiichiro Nakano, ... Kenji Tsuruta</authors>
	<abstract>A suite of scalable molecular-dynamics (MD) algorithms has been developed for materials simulations. The linear scaling MD algorithms encompass a wide spectrum of physical reality: (i) classical MD based on a many-body interatomic potential model; (ii) environment-dependent, variable-charge MD; (iii) quantum mechanical MD based on the tight-binding method; and (iv) self-consistent quantum MD based on the density functional theory. Benchmark tests on 1024 Cray T3E processors including 1.02-billion-atom many-body and 22 500-atom density functional MD simulations demonstrate that these algorithms are highly scalable. A design-space diagram spanning seven decades of system size and computational time is constructed for materials scientists to design an optimal MD simulation incorporating maximal physical realism within a given computational budget.</abstract>
	<keywords>Parallel computing; Molecular dynamics; Variable-charge molecular dynamics; Tight-binding method; Density functional theory</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=607>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards an operating system managing parallelism of computing on clusters</paper_heading>
	<authors>Andrzej M Goscinski</authors>
	<abstract>Parallelism management is a difficult task, in particular in parallel systems based on clusters (of PCs or workstations). We claim in this paper that parallelism management should be provided by an operating system that inherits many features of a distributed operating system and provides new services that address the needs of parallel processes, cluster’s resources, and application programmers. This system, in order to allow parallel programs to achieve high performance, transparency and ease of use, should provide services such as establishment of a virtual machine; mapping processes to computers; concurrent process creation and process duplication supported by process migration; computation co-ordination; group communication; and distributed shared memory. In order to substantiate the claim the first version of a cluster operating system managing parallelism, called GENESIS, has been developed and presented in this paper. The results of the execution of some parallel applications, that further substantiate our claim, are shown.</abstract>
	<keywords>Parallel processing; Cluster operating systems; Parallelism management; Transparency; Performance</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=608>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A new-generation parallel computer and its performance evaluation</paper_heading>
	<authors>Sotirios G. Ziavras, Haim Grebel, Anthony T. Chronopoulos, Florent Marcelli</authors>
	<abstract>An innovative design is proposed for an MIMD distributed shared-memory (DSM) parallel computer capable of achieving gracious performance with technology expected to become feasible/viable in less than a decade. This New Millennium Computing Point Design was chosen by NSF, DARPA, and NASA as having the potential to deliver 100 TeraFLOPS and 1 PetaFLOPS performance by the year 2005 and 2007, respectively. Its scalability guarantees a lifetime extending well into the next century. Our design takes advantage of free-space optical technologies, with simple guided-wave concepts, to produce a 1D building block (BB) that implements efficiently a large, fully connected system of processors. Designing fully connected, large systems of electronic processors could be a very beneficial impact of optics on massively parallel processing. A 2D structure is proposed for the complete system, where the aforementioned 1D BB is extended into two dimensions. This architecture behaves like a 2D generalized hypercube, which is characterized by outstanding performance and extremely high wiring complexity that prohibits its electronics-only implementation. With readily available technology, a mesh of clear plastic/glass bars in our design facilitate point-to-point bit-parallel transmissions that utilize wavelength-division multiplexing (WDM) and follow dedicated optical paths. Each processor is mounted on a card. Each card contains eight processors interconnected locally via an electronic crossbar. Taking advantage of higher-speed optical technologies, all eight processors share the same communications interface to the optical medium using time-division multiplexing (TDM). A case study for 100 TeraFLOPS performance by the year 2005 is investigated in detail; the characteristics of chosen hardware components in the case study conform to SIA (Semiconductor Industry Association) projections. An impressive property of our system is that its bisection bandwidth matches, within an order of magnitude, the performance of its computation engine. Performance results based on the implementation of various important algorithmic kernels show that our design could have a tremendous, positive impact on massively parallel computing. 2D and 3D implementations of our design could achieve gracious (i.e., sustained) PetaFLOPS performance before the end of the next decade.</abstract>
	<keywords>PetaFLOPS computer performance; Advanced computer architecture; Generalized hypercube; Parallel computer; Optical interconnection network</keywords>
	<publication_month_year>2000-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issues 3</volumes_issues>
</paper>
<paper no=609>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An artificial immune system model for intelligent agents</paper_heading>
	<authors>Roger L. King, Samuel H. Russ, Aric B. Lambert, Donna S. Reese</authors>
	<abstract>This paper describes the human immune system and its functionalities from a computational viewpoint. The objective of this paper is to provide the biological basis for an artificial immune system. This paper will also serve to illustrate how a biological system can be studied and how inferences can be drawn from its operation that can be exploited in intelligent agents. Functionalities of the biological immune system (e.g., content addressable memory, adaptation, etc.) are identified for use in intelligent agents. Specifically, in this paper, an intelligent agent will be described for task allocation in a heterogeneous computing environment. Initial implementation of the agents will be described along with preliminary results. This research is not intended to develop an explicit model of the human immune system, but to exploit some of its functionalities in designing agent-based parallel and distributed control systems.</abstract>
	<keywords>Artificial immune system; Heterogeneous computing; Intelligent agents; Biological models; Predictive scheduling</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=610>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Biological metaphors in the design of complex software systems</paper_heading>
	<authors>Dan C. Marinescu, Ladislau Bölöni</authors>
	<abstract>In this paper we discuss metaphors inspired by structural biology, genetics, neurology and immunology for building complex software ystems. Structural biology offers hints for software composition. Genetics provides ideas to construct software modules from descriptions. A network of software agents could emulate the nervous system, coordinate various activities and mediate amongst interacting entities. Immunology inspires the design of secure systems. As a case study we present Bond, a distributed-object system providing agent support for network centric computing.</abstract>
	<keywords>Metaphors; Genetics; Neurology; Immunology</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=611>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DNA computing: implementation of data flow logical operations</paper_heading>
	<authors>Piotr Wa̧siewicz, Artur Malinowski, Robert Nowak, Jan J. Mulawka, ... Andrzej Płucienniczak</authors>
	<abstract>Self-assembly of DNA is considered a fundamental operation in realization of molecular logic circuits. We propose a new approach to implementation of data flow logical operations based on manipulating DNA strands. In our method the logic gates, input, and output signals are represented by DNA molecules. Each logical operation is carried out as soon as the operands are ready. This technique employs standard operations of genetic engineering including radioactive labeling as well as digestion by the second class restriction nuclease and polymerase chain reaction (PCR). To check practical utility of the method a series of genetic engineering experiments have been performed. The obtained information confirms interesting properties of the DNA-based molecular data flow logic gates. Some experimental results demonstrating implementation of a single logic NAND gate and only in one vessel calculation of a tree-like Boolean function with the help of the PCR are provided. These techniques may be utilized in massively parallel computers and on DNA chips.</abstract>
	<keywords>DNA computing; Molecular logic gates; Data flow computers; Boolean functions; One vessel PCR solution; Class 2 restriction nuclease; FokI; DNA chips</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=612>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An asynchronous parallel metaheuristic for the period vehicle routing problem</paper_heading>
	<authors>Lúcia M.A. Drummond, Luiz S. Ochi, Dalessandro S. Vianna</authors>
	<abstract>This paper presents an asynchronous parallel metaheuristic for the period vehicle routing problem (PVRP). The PVRP generalizes the classical vehicle routing problem by extending the planning period from a single day to M days. The algorithm proposed is based on concepts used in parallel genetic algorithms and local search heuristics. The algorithm employs the Island model in which the migration frequency must not be very high. The results of computational experiments carried out on problems taken from the literature indicate that the proposed approach outperforms existing heuristics in most cases.</abstract>
	<keywords>Parallel algorithms; Metaheuristics; Period vehicle routing problem</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=613>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed multiprocessor scheduling with decomposed optimization criterion</paper_heading>
	<authors>F. Seredyński, J. Koronacki, C.Z. Janikow</authors>
	<abstract>In this paper, a new approach to scheduling of parallel and distributed algorithms for multiprocessor systems is proposed. Its main innovation lies in evolving a decomposition of the global optimization criteria. For this purpose, agents — local decision making units — are associated with individual tasks of the program graph. Thus, the program can be interpreted as a multi-agent system. A game-theoretic model of interaction between agents is applied. Agents take part in an iterated game to find directions of migration in the system graph, with the objective of minimizing the total execution time of the program in a given multiprocessor topology. Competitive coevolutionary genetic algorithm, termed loosely coupled genetic algorithm, is used to implement the multi-agent system. The scheduling algorithm works with a global optimization function, what limits its efficiency. To make the algorithm truly distributed, decomposition of the global optimization criterion into local criteria is proposed. This decomposition is evolved with genetic programming. Results of successive experimental study of the proposed algorithm are presented.</abstract>
	<keywords>Multiprocessor scheduling; Multi-agent systems; Decomposition of optimization criterion; Genetic programming</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=614>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Mapping tasks onto nodes: a parallel local neighborhood approach</paper_heading>
	<authors>S.Mounir Alaoui, T. El-Ghazawi, O. Frieder, A. Bellachia, A. Bensaid</authors>
	<abstract>We consider the problem of mapping processes onto computing nodes so as to reduce the execution time by minimizing communication delays. Our approach relies on a genetic algorithm implementation of the local neighborhood search (LNS) approach and is called Genetic-LNS or GLNS. We also present our parallel version of the GLNS algorithm, called parallel genetic local neighborhood search (P-GLNS). LNS, GNLS, and P-GLNS were implemented and compared. Simulations demonstrate that the GLNS algorithm has better performance than LNS, and that, when the workload is sufficiently high, the P-GLNS algorithm achieved near linear scalability.</abstract>
	<keywords>Genetic algorithm; Mapping onto nodes; Parallel local neighborhood search</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=615>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evolution-based scheduling of multiple variant and multiple processor programs</paper_heading>
	<authors>Piotr Jędrzejowicz, Ireneusz Czarnowski, Aleksander Skakowski, Henryk Szreder</authors>
	<abstract>The paper proposes to manage complexity and cost issues of the fault-tolerant programs not at a single program level, but rather from the point of view of the whole set of such programs, which are to be run under time constraints. The paper introduces a family of scheduling problems called fault-tolerant programs scheduling. Since, the discussed problems are, in general, computationally difficult, a challenge is to find effective scheduling procedures. Several evolution-based algorithms solving three basic kinds of fault-tolerant programs scheduling problems have been proposed. The problems involve scheduling multiple variant or multiple processor tasks on multiple, identical processors, under time constraints. To validate the algorithms computational experiment has been conducted. Experimental results show that evolution based algorithms produce satisfactory to good solutions in a reasonable time.</abstract>
	<keywords>Scheduling; Fault-tolerance; Evolution-based algorithms; Multiple processors</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=616>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A genetic-based fault-tolerant routing strategy for multiprocessor networks</paper_heading>
	<authors>Peter K.K. Loh, Venson Shaw</authors>
	<abstract>AI-based search techniques have been adapted as viable, topology-independent fault-tolerant routing strategies on multiprocessor networks [P.K.K. Loh, Artificial intelligence search techniques as fault-tolerant routing strategies, Parallel Computing 22 (8) (1996) 1127–1147]. These fault-tolerant routing strategies are viable with the exception that the routes obtained were non-minimal. This meant that a large number of redundant node traversals were made in reaching the destination, increasing the likelihood of encountering further faulty network components. Here, we investigate the adaptation of a genetic-heuristic algorithm combination as a fault-tolerant routing strategy. Our results show that this hybrid fault-tolerant routing strategy produces minimal or near-minimal routes. Under certain fault conditions, this new strategy outperforms the heuristic AI-based ones with a significant reduction in the number of redundant traversals.</abstract>
	<keywords>routing strategy; topology-independent; near-minimal; fault-tolerant; hardware support.</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=617>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Structuring statement sequences in instance-based locality optimization</paper_heading>
	<authors>Claudia Leopold</authors>
	<abstract>Instance-based locality optimization [C. Leopold, Arranging statements and data of program instances for locality, Future Gen. Comput. Syst. 14 (1998) 293–311.] is a semi-automatic program restructuring method that reduces the amount of memory hierarchy traffic. The method imitates the human approach of considering several small program instances (PIs), optimizing the PIs by reordering their statements, and generalizing the structure of the optimized PIs to the program under consideration. This paper considers the generation of structured optimized PIs, which can be written compactly as a sequence of nested for-loops. We empirically establish an objective function that assesses the regularity of a statement sequence, and we integrate the function into the optimization algorithm of [C. Leopold, Arranging statements and data of program instances for locality, Future Gen. Comput. Syst. 14 (1998) 293–311.]. Experimental results are also included.</abstract>
	<keywords>Locality optimization; Memory hierarchy; Instance-based method; Program regularity</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=618>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel Ant Colonies for the quadratic assignment problem</paper_heading>
	<authors>E.-G. Talbi, O. Roux, C. Fonlupt, D. Robillard</authors>
	<abstract>Ant Colonies optimization take inspiration from the behavior of real ant colonies to solve optimization problems. This paper presents a parallel model for ant colonies to solve the quadratic assignment problem (QAP). The cooperation between simulated ants is provided by a pheromone matrix that plays the role of a global memory. The exploration of the search space is guided by the evolution of pheromones levels, while exploitation has been boosted by a tabu local search heuristic. Special care has also been taken in the design of a diversification phase, based on a frequency matrix. We give results that have been obtained on benchmarks from the QAP library. We show that they compare favorably with other algorithms dedicated for the QAP.</abstract>
	<keywords>Metaheuristics; Ant colonies; Tabu search; Parallel algorithm; Quadratic assignment problem; Combinatorial optimization</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=619>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Analyzing synchronous and asynchronous parallel distributed genetic algorithms</paper_heading>
	<authors>Enrique Alba, José M. Troya</authors>
	<abstract>Parallel genetic algorithms (PGAs) have been traditionally used to extend the power of serial genetic algorithms (GAs), since they often can be tailored to provide a larger efficiency on complex search problems. In a PGA several sub-algorithms cooperate in parallel to solve the problem. This high-level definition has led to a considerable number of different implementations that preclude direct comparisons and knowledge exchange. To fill this gap we begin by providing a common framework for studying PGAs. We then analyze the importance of the synchronism in the migration step of various parallel distributed GAs. This implementation issue could affect the evaluation effort as well as could provoke some differences in the search time and speedup. We cover in this study a set of popular evolution schemes relating panmictic (steady-state or generational) and structured-population (cellular) GAs for the islands. We aim at extending existing results to structured-population GAs, and also to new problems. The evaluated PGAs demonstrate linear and even super-linear speedup when run in a cluster of workstations. They also show important numerical benefits if compared with their sequential versions. In addition, we always report lower search times for the asynchronous versions.</abstract>
	<keywords>Asynchronous parallel Gas; Cellular Gas; Numeric performance; Speedup; Selection pressure</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=620>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>2-phase GA-based image registration on parallel clusters</paper_heading>
	<authors>Prachya Chalermwat, Tarek El-Ghazawi, Jacqueline LeMoigne</authors>
	<abstract>Genetic algorithms (GAs) are known to be robust for search and optimization problems. Image registration can take advantage of the robustness of GAs in finding best transformation between two images, of the same location with slightly different orientation, produced by moving spaceborne remote sensing instruments. In this paper, we present 2-phase sequential and coarse-grained parallel image registration algorithms using GAs as optimization mechanism. In its first phase, the algorithm finds a small set of good solutions using low-resolution versions of the images. Based on these candidate low-resolution solutions, the algorithm uses the full resolution image data to refine the final registration results in the second phase. Experimental results are presented and revealed that our algorithms yield very accurate registration results for LandSat Thematic Mapper images, and the parallel algorithm scales quite well on the Beowulf parallel cluster.</abstract>
	<keywords>Genetic algorithm; Image registration Parallel cluster</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=621>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation of a parallel Genetic Algorithm on a cluster of workstations: Traveling Salesman Problem, a case study</paper_heading>
	<authors>Giuseppe A. Sena, Dalila Megherbi, Germinal Isern</authors>
	<abstract>A parallel version of a Genetic Algorithm (GA) is presented and implemented on a cluster of workstations. Even though our algorithm is general enough to be applied to a wide variety of problems, we used it to obtain optimal and/or suboptimal solutions to the well-known Traveling Salesman Problem. The proposed algorithm is implemented using the Parallel Virtual Machine (PVM) library over a network of workstations. A master–slave paradigm is used to implement the proposed parallel/distributed Genetic Algorithm (PDGA), which is based on a distributed-memory approach. Tests were performed with clusters of 1, 2, 4, 8, and 16 workstations, using several real problems and population sizes. Results are presented to show how the performance of the algorithm is affected by variations on the number of slaves, population size, mutation rate, and mutation interval. The results presented show the utility, versatility, efficiency and potential value of the proposed parallel and distributed Genetic Algorithm to tackle NP-complete problems of the same nature.</abstract>
	<keywords>Genetic Algorithm; Parallel computing; Distributed system; Message-passing; Traveling Salesman Problem</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=622>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A formal definition of the phenomenon of collective intelligence and its IQ measure</paper_heading>
	<authors>Tadeusz Szuba</authors>
	<abstract>This paper presents a formalization of collective intelligence (CI). A molecular, quasi-chaotic model of computations allows us to model CI in social structures, and to define its measure (IQS). This methodology works for bacterial colonies and social insects as well as for human social structures. With the CI theory some patterns of human behavior receive formal justification, others can be explained as IQS optimization. The CI formalization assumes that it is a property of a social structure, initializing when individuals interact, and as a result, acquiring the ability to solve new or more complex problems. CI amplifies if the structure improves synergy, which further increases the spectrum and complexity of the problems, which can be solved together. The formalization covers areas where CI results in physical synergy and mental/logical cooperation.</abstract>
	<keywords>Collective intelligence; Quasi-chaotic model of computations; Synergy; IQ; PROLOG</keywords>
	<publication_month_year>2001-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 17, Issue 4</volumes_issues>
</paper>
<paper no=623>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational Grids in action: the National Fusion Collaboratory</paper_heading>
	<authors>K. Keahey, T. Fredian, Q. Peng, D.P. Schissel, ... D. McCune</authors>
	<abstract>The National Fusion Collaboratory (NFC) project was created to advance scientific understanding and innovation in magnetic fusion research by enabling more efficient use of existing experimental facilities through more effective integration of experiment, theory, and modeling. To achieve this objective, NFC introduced the concept of “network services”, which build on top of computational Grids, and provide Fusion codes, together with their maintenance and hardware resources as a service to the community. This mode of operation requires the development of new authorization and enforcement capabilities. In addition, the nature of Fusion experiments places strident quality of service requirements on codes run during the experimental cycle. In this paper, we describe Grid computing requirements of the Fusion community, and present our first experiments in meeting those requirements.</abstract>
	<keywords>Computational Grids; Network services; Distributed computing; Fusion; Collaboratory</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=624>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The UK e-Science Core Programme and the Grid</paper_heading>
	<authors>Tony Hey, Anne E. Trefethen</authors>
	<abstract>This paper describes the £120M UK ‘e-Science’ (http://www.research-councils.ac.uk/ and http://www.escience-grid.org.uk) initiative and begins by defining what is meant by the term e-Science. The majority of the £120M, some £75M, is funding large-scale e-Science pilot projects in many areas of science and engineering. The infrastructure needed to support such projects must permit routine sharing of distributed and heterogeneous computational and data resources as well as supporting effective collaboration between groups of scientists. Such an infrastructure is commonly referred to as the Grid. Apart from £10M towards a Teraflop computer, the remaining funds, some £35M, constitute the e-Science ‘Core Programme’. The goal of this Core Programme is to advance the development of robust and generic Grid middleware in collaboration with industry. The key elements of the Core Programme will be outlined including details of a UK e-Science Grid testbed. The pilot e-Science projects that have so far been announced are then briefly described. These projects span a range of disciplines from particle physics and astronomy to engineering and healthcare, and illustrate the breadth of the UK e-Science Programme. In addition to these major e-Science projects, the Core Programme is funding a series of short-term e-Science demonstrators across a number of disciplines as well as projects in network traffic engineering and some international collaborative activities. We conclude with some remarks about the need to develop a data architecture for the Grid that will allow federated access to relational databases as well as flat files.</abstract>
	<keywords>e-Science; Core Programme; Grid</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=625>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Legion support for advanced parameter-space studies on a grid</paper_heading>
	<authors>Anand Natrajan, Marty A. Humphrey, Andrew S. Grimshaw</authors>
	<abstract>Parameter-space (p-space) studies involve running a single application several times with different parameter sets. Since the jobs are mutually independent, many computing resources can be recruited to conduct an entire study in a distributed manner. The p-space studies are attractive applications for grids, which are networked collections of computing and other resources. Legion is a grid infrastructure that facilitates the secure and easy use of heterogeneous, geographically distributed resources by providing the illusion of a single virtual machine from those resources. Legion provides tools and services that support advanced p-space studies, i.e., studies that make complex demands such as transparent access to distributed files, fault-tolerance and security. We demonstrate these benefits with a protein-folding experiment in which a molecular simulation package was run over a grid managed by Legion.</abstract>
	<keywords>Legion; Parameter-space; Grid</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=626>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Early experiences with the GridFTP protocol using the GRB-GSIFTP library</paper_heading>
	<authors>Giovanni Aloisio, Massimo Cafaro, Italo Epicoco</authors>
	<abstract>The ISUFI/High Performance Computing Centre is actively experimenting with a grid infrastructure connecting sites located both in Europe and in the USA. This paper reports on our feasibility study related to the use of the GridFTP protocol for high performance data transfers. We also describe a client library we are developing to provide additional services beyond those provided by the Globus ftp client library and to ease code development.</abstract>
	<keywords>Computational grids; High performance data transfer; GridFTP protocol</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=627>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A computational economy for grid computing and its implementation in the Nimrod-G resource broker</paper_heading>
	<authors>David Abramson, Rajkumar Buyya, Jonathan Giddy</authors>
	<abstract>Computational grids that couple geographically distributed resources such as PCs, workstations, clusters, and scientific instruments, have emerged as a next generation computing platform for solving large-scale problems in science, engineering, and commerce. However, application development, resource management, and scheduling in these environments continue to be a complex undertaking. In this article, we discuss our efforts in developing a resource management system for scheduling computations on resources distributed across the world with varying quality of service (QoS). Our service-oriented grid computing system called Nimrod-G manages all operations associated with remote execution including resource discovery, trading, scheduling based on economic principles and a user-defined QoS requirement. The Nimrod-G resource broker is implemented by leveraging existing technologies such as Globus, and provides new services that are essential for constructing industrial-strength grids. We present the results of experiments using the Nimrod-G resource broker for scheduling parametric computations on the World Wide Grid (WWG) resources that span five continents.</abstract>
	<keywords>Grid computing; Computational economy; Nimrod-G broker; Grid scheduling; Resource management</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=628>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GuiGen: a toolset for creating customized interfaces for grid user communities</paper_heading>
	<authors>Alexander Reinefeld, Hinnerk Stüben, Florian Schintke, George Din</authors>
	<abstract>GuiGen is a comprehensive set of tools for creating customized graphical user interfaces (GUIs). It draws from the concept of computing portals, which are here seen as interfaces to application-specific computing services for user communities. While GuiGen was originally designed for the use in computational grids, it can be used in client/server environments as well. Compared to other GUI generators, GuiGen is more versatile and more portable. It can be employed in many different application domains and on different target platforms. With GuiGen, application experts (rather than computer scientists) are able to create their own individually tailored GUIs.</abstract>
	<keywords>Grid computing; Customized user interfaces; Grid user communities Web portals XML</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=629>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational and data Grids in large-scale science and engineering</paper_heading>
	<authors>William E. Johnston</authors>
	<abstract>As the practice of science moves beyond the single investigator due to the complexity of the problems that now dominate science, large collaborative and multi-institutional teams are needed to address these problems. In order to support this shift in science, the computing and data-handling infrastructure that is essential to most of modern science must also change in order to support this increased complexity. This is the goal of computing and data Grids: software infrastructure that facilitates solving large-scale problems by providing the mechanisms to access, aggregate, and manage the computer network-based infrastructure of science. This infrastructure includes computing systems, data archive systems, scientific instruments, and computer-mediated human collaborations. This paper examines several large-scale science problems, their requirements for computing and data Grid infrastructure, and the current approaches to providing the necessary functionality.</abstract>
	<keywords>Grids; Heterogeneous; Widely distributed computing; NASA’s Information Power Grid (IPG); DOE Science Grid; Grid applications</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=630>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed data mining on the grid</paper_heading>
	<authors>Mario Cannataro, Domenico Talia, Paolo Trunfio</authors>
	<abstract>In many industrial, scientific and commercial applications, it is often necessary to analyze large data sets, maintained over geographically distributed sites, by using the computational power of distributed and parallel systems. The grid can play a significant role in providing an effective computational support for knowledge discovery applications. We describe a software architecture for geographically distributed high-performance knowledge discovery applications called Knowledge Grid, which is designed on top of computational grid mechanisms, provided by grid environments such as Globus. The Knowledge Grid uses the basic grid services such as communication, authentication, information, and resource management to build more specific parallel and distributed knowledge discovery tools and services. The paper discusses how the Knowledge Grid can be used to implement distributed data mining services.</abstract>
	<keywords>Knowledge discovery; Distributed data mining; Grid services</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=631>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programming environments for high-performance Grid computing: the Albatross project</paper_heading>
	<authors>Thilo Kielmann, Henri E. Bal, Jason Maassen, Rob van Nieuwpoort, ... Kees Verstoep</authors>
	<abstract>The aim of the Albatross project is to study applications and programming environments for computational Grids. We focus on high-performance applications, running in parallel on multiple clusters or MPPs that are connected by wide-area networks (WANs). We briefly present three Grid programming environments developed in the context of the Albatross project: the MagPIe library for collective communication with MPI, the replicated method invocation (RepMI) mechanism for Java, and the Java-based Satin system for running divide-and-conquer programs on Grid platforms. A major challenge in investigating the performance of such applications is the actual WAN behavior. Typical wide-area links are just part of the Internet and thus shared among many applications, making runtime measurements irreproducible and thus scientifically hardly valuable. To overcome this problem, we developed a WAN emulator as part of Panda, our general-purpose communication substrate. The WAN emulator allows us to run parallel applications on a single (large) parallel machine with only the wide-area links being emulated. The Panda emulator is highly accurate and configurable at runtime. We present a case study in which Satin runs across various emulated WAN scenarios.</abstract>
	<keywords>Grid computing; Wide-area network emulation; Albatross; MagPIe; Panda; RepMI; Satin</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=632>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>HARNESS fault tolerant MPI design, usage and performance issues</paper_heading>
	<authors>Graham E. Fagg, Jack J. Dongarra</authors>
	<abstract>Initial versions of MPI were designed to work efficiently on multi-processors which had very little job control and thus static process models. Subsequently forcing them to support a dynamic process model suitable for use on clusters or distributed systems would have reduced their performance. As current HPC collaborative applications increase in size and distribution the potential levels of node and network failures increase. This is especially true when MPI implementations are used as the communication media for GRID applications where the GRID architectures themselves are inherently unreliable thus requiring new fault tolerant MPI systems to be developed. Here we present a new implementation of MPI called FT-MPI that allows the semantics and associated modes of failures to be explicitly controlled by an application via a modified MPI API. Given is an overview of the FT-MPI semantics, design, example applications and some performance issues such as efficient group communications and complex data handling. Also briefly described is the HARNESS g_hcore system that handles low-level system operations on behalf of the MPI implementation. This includes details of plug-in services developed and their interaction with the FT-MPI runtime library.</abstract>
	<keywords>MPI implementation; HARNESS; FT-MPI; Meta computing; Fault tolerant message passing</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=633>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GridLab—a grid application toolkit and testbed</paper_heading>
	<authors>Ed Seidel, Gabrielle Allen, André Merzky, Jarek Nabrzyski</authors>
	<abstract>In this paper we present the new project called GridLab which is funded by the European Commission under the Fifth Framework Programme. The GridLab project, made up of computer scientists, astrophysicists and other scientists from various application areas, will develop and implement the grid application toolkit (GAT) together with a set of services to enable easy and efficient use of Grid resources in a real and production grid environment. GAT will provide core, easy to use functionality through a carefully constructed set of generic higher level grid APIs through which an application will be able to call the grid services laying beneath in order to perform efficiently in the Grid environment using various, dramatically wild application scenarios.</abstract>
	<keywords>Grid computing; GridLab; GAT; Testbed</keywords>
	<publication_month_year>2002-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 18, Issue 8</volumes_issues>
</paper>
<paper no=634>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An analytical model of adaptive wormhole routing with time-out</paper_heading>
	<authors>A. Khonsari, H. Sarbazi-Azad, M. Ould-Khaoua</authors>
	<abstract>Although many adaptive routing algorithms have been proposed for wormhole-routed networks, it is necessary to have clear understanding of the factors that affect their potential performance before they can be widely adopted in commercial multicomputers. This paper proposes a new analytical model of an adaptive routing algorithm proposed by Duato [Future Gener. Comp. Sys. 10 (10) (1994) 45]. The main feature of this routing algorithm is the use of a time-out mechanism to select a particular class of virtual channels at a given router. This has the advantage of reducing virtual channels multiplexing, leading to improvement in network performance. Simulation experiments reveal that the proposed analytical model predicts message latency with a good degree of accuracy.</abstract>
	<keywords>Multicomputers; Interconnection networks; Adaptive routing; Deterministic routing; Deadlock; Time-out; Virtual channels; Message latency; Performance modelling</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=635>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A new remote user authentication scheme for multi-server architecture</paper_heading>
	<authors>Iuon-Chang Lin, Min-Shiang Hwang, Li-Hua Li</authors>
	<abstract>Remote user authentication is used to validate the legitimacy of a remote login user. Conventional user authentication schemes are suited to solve the privacy and security problems for the single client/server architecture environment. However, the use of computer networks and information technology has grown spectacularly. More and more network architectures are used in multi-server environments. In this paper, we propose a new remote user authentication scheme. The scheme can be used in multi-server environments. In our scheme, the system does not need to maintain any verification table, and the users who have registered in the servers do not need to remember different login passwords for various servers. In addition, our scheme can also withstand replay and modification attacks. Furthermore, it allows users to choose their passwords freely, and a user can be removed from the system easily when the subscription expires.</abstract>
	<keywords>Cryptography; User authentication; Remote login; Security</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=636>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A hierarchical disk scheduler for multimedia systems</paper_heading>
	<authors>Jesús Carretero, Javier Fernández, Félix García, Alok Choudhary</authors>
	<abstract>An integrated storage platform for open systems should be able to meet the requirements of deterministic applications, multimedia systems, and traditional best-effort applications. It should also provide a scheduling mechanism fitting all those types of applications. In this paper, we propose a two-level hierarchical disk scheduling scheme, named 2-Q, which can guarantee deterministic deadlines, maximize the number of statistic real-time streams processed by the disk system, and minimize the average latency for best-effort requests. The upper level of the scheduling architecture, server level, is divided into three queues: deterministic, statistic, and best-effort requests. Each server may have its own scheduling algorithm. The lower level, disk driver, chooses the ready streams using its own scheduling criteria. We also propose an adaptive admission control algorithm relying on worst and average values of disk server utilization. Only streams satisfying the admission algorithm criteria are accepted for further processing by the disk server. The solution is extended to a parallel disk system by using a third hierarchical level, named meta-scheduler, briefly described in the paper. The performance evaluations demonstrate that our scheduling architecture is adequated for handling stream sets with different deterministic, statistic, or best-effort requirements.</abstract>
	<keywords>Multimedia; Parallel I/O; Clusters; Disk scheduling</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=637>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An efficient recovery scheme for fault-tolerant mobile computing systems</paper_heading>
	<authors>Taesoon Park, Namyoon Woo, Heon Y. Yeom</authors>
	<abstract>This paper presents an efficient recovery scheme to provide fault-tolerance for the mobile computing systems. The proposed scheme is based on the message logging and the independent checkpointing, and for the efficient management of the recovery information, such as checkpoints and message logs, the movement-based scheme is suggested. The mobile host carrying its recovery information to the nearby mobile support station can recover instantly in case of a failure. However, the support stations visited by the mobile host may have to experience the high failure-free execution cost for transferring the recovery information and accessing the stable storage. On the other hand, the recovery cost can be too high, if the recovery information remains dispersed over a number of mobile support stations. The movement-based scheme considers both of the failure-free execution cost and the failure-recovery cost. Hence, while a mobile host moves within a certain range, the recovery information of the mobile host remains at the support stations where the information was first saved. However, if the mobile host moves out of the range, it transfers the recovery information into the nearby support station. As a result, the scheme can control the transfer cost as well as the recovery cost. The performance of the proposed scheme is evaluated with extensive simulation study.</abstract>
	<keywords>Distributed systems; Fault-tolerance; Mobile computing systems; Message logging; Asynchronous recovery</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=638>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Investigation of the importance of the genotype–phenotype mapping in information retrieval</paper_heading>
	<authors>José-Luis Fernández-Villacañas Martín, Mark Shackleton</authors>
	<abstract>An investigation of the role of the genotype–phenotype mapping (G-Pm) is presented for an evolutionary optimization task. A simple genetic algorithm (SGA) plus a mapping creates a new mapping genetic algorithm (MGA) that is used to optimize a Boolean decision tree for an information retrieval task, with the tree being created via a relatively complex mapping. Its performance is contrasted with that of a genetic programming algorithm, British Telecom Genetic Programming (BTGP) which operates directly on phenotypic trees. The mapping is observed to play an important role in the time evolution of the system allowing the MGA to achieve better results than the BTGP. We conclude that an appropriate G-Pm can improve the evolvability of evolutionary algorithms.</abstract>
	<keywords>Genotype–phenotype mapping; Information retrieval; Genetic algorithms; Genetic programming</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=639>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A secure and available electronic voting service for a large-scale distributed system</paper_heading>
	<authors>Gianluca Dini</authors>
	<abstract>We present a secure and available electronic voting service suitable for a large-scale distributed system such as the Internet. The proposed service is based on replication and tolerates both benign and fully arbitrary failures. If enough servers are correct, service availability and security are ensured despite the presence of faulty servers and malicious voters. A voter that is affected by a crash failure can vote after recovery. The proposed service satisfies common voting requirements including voter eligibility and privacy, and tally accuracy. In addition, the service satisfies a further important requirement, namely tally verifiability without any intervention of voters. Anyone, including an external observer, can easily be convinced that the voting outcome is fairly computed from the ballots that were correctly cast. It follows that the proposed voting scheme strengthens the security properties of the electronic voting procedure, and simplifies the interaction of voters with the electronic voting system.</abstract>
	<keywords>Distributed systems; Applied cryptography; Security and privacy; Voting protocols</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=640>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation of a multifunctional logic gate based on folding/unfolding transitions of a protein</paper_heading>
	<authors>Andrew S. Deonarine, Sonya M. Clark, Lars Konermann</authors>
	<abstract>Currently there is great interest in the development of (bio)chemical devices that can act as logic gates at the molecular level. In this work we show that the protein cytochrome c (cyt c) can be used to perform a variety of logic operations such as YES, NOT, AND, OR, and XOR. The protein accepts chemical input signals in the form of denaturants that can induce unfolding of the polypeptide chain. These conformational changes lead to alterations of the protein fluorescence intensity which is used as the output signal. The specific type of logic function performed by the protein depends on the types of denaturants and on the solvent conditions used in the experiment. We describe a dialysis cell that can be regarded as a simple hardware version of a protein gate. This device allows reversible switching of the input signals. The operation of this cell is based on a simplified strategy that employs just one type of chemical input signal, namely an aqueous solution of hydrochloric acid. The logic function performed by the device is determined by a “gate controller” (GC) input.</abstract>
	<keywords>Boolean logic; Protein folding; Molecular computing; Fluorescence</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=641>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Charging and accounting in high-speed networks</paper_heading>
	<authors>Burkhard Stiller, Peter Reichl, Jan Gerke, Hasan, Placi Flury</authors>
	<abstract>The commercialization of the Internet and the requirement to offer various different network services for emerging applications has driven the development of appropriate Internet technology, such as the differentiated services architecture. However, these developments focussed on the technological point of view only, neglecting the need to provide economic incentives to users and customers to chose the “right” service class. Once accepted that charging Internet services is an economically valid approach, suitable charging technology is required. Therefore, this paper motivates and introduces a suitable Internet charging system. In addition, the development of incentive-compatible pricing models for multi-service networks determines the corresponding economic question. The problem of today’s flat rates shows, that high-speed networks are priced simply and customers like them, however, Internet service providers tend to show a severe economic instability in the market, while technologies emerge on a fast time-scale. Therefore, a better than flat fee approach termed cumulus pricing scheme is introduced, solving the feasibility problem of pricing Internet services. This work addresses the design and implementation of suitable Internet charging technology as well as design of a novel pricing scheme, which extends the flat rate scheme with a feedback, both of which determine advantages over existing approaches.</abstract>
	<keywords>Charging; Accounting; High-speed networks</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=642>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parameterisation to tailor commodity clusters to applications</paper_heading>
	<authors>Ralf Gruber, Pieter Volgers, Alessandro De Vita, Massimiliano Stengel, Trach-Minh Tran</authors>
	<abstract>Parallel applications can be parameterised by the quotient γa of flop/s and data transfers between the processors, and by the machine-dependent, maximum local processor performance ra. Clusters can be parameterised by the quotient γm of ra and the per processor network communication bandwidth bm. All those parameters are predictable. In parallel machines, the communication time is smaller than the computing time if γm<γa. A first principles chemistry application is described and parameterised. Benchmarks on the Swiss-T1 cluster machine show that the predicted inter-processor communication and computing times correspond well to the measured times. These parameterisations can now be used to tailor clusters from commodity components to the applications.</abstract>
	<keywords>Cluster computing; Commodity computing; Scientific applications</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=643>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Searching for Backbones—a high-performance parallel algorithm for solving combinatorial optimization problems</paper_heading>
	<authors>Johannes Schneider</authors>
	<abstract>A highly efficient parallel algorithm called Searching for Backbones (SfB) is proposed: based on the finding, that many parts of a good configuration for a given optimization problem are the same in all other good solutions, SfB reduces the complexity of this problem by determining these “backbones” and eliminating them in order to get even better solutions in a very short time. Applications and results are presented for the Traveling Salesman Problem and the Vehicle Routing Problem.</abstract>
	<keywords>Massive Parallelization; Optimization; Traveling Salesman problem</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=644>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An interdisciplinary virtual laboratory on nanoscience</paper_heading>
	<authors>M Guggisberg, P Fornaro, T Gyalog, H Burkhart</authors>
	<abstract>Nanoscience is a booming field with manifold applications. Nanoscience equipment (e.g., microscopes) are however expensive and only major companies and organizations sponsored by research programs can afford on-site installation. Based on the Internet as a global communication platform, distance education and remote usage might offer a working environment by which complex and sensitive instruments can be shared. The Swiss Virtual Campus (SVC) project nano-world has exactly the following goal: establish a Virtual Nanoscience Laboratory to be shared and used by a distributed user community. Three main topics need to be realized in the framework of a virtual laboratory: user management, communication and co-operation, and the control of virtual experiments. The basic architecture of the prototype is a multi-tiered client–server model which supports individual nanoscience experiments to be implemented as a stand-alone web-service.</abstract>
	<keywords>Remote experiments; Virtual laboratory; Distance-learning; Nanoscience grid</keywords>
	<publication_month_year>2003-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 1</volumes_issues>
</paper>
<paper no=645>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using computer algebra systems in the development of scientific computer codes</paper_heading>
	<authors>Aldo Dall’Osso</authors>
	<abstract>During the first years of the computer era no tools were available to aid scientists in the development of their computer codes. Code production was a pretty handicraft job. The last years have seen the resurgence of many concepts whose aim is to reduce the man effort in building scientific systems. These ideas spread from the reusable component concept to the multi-purpose problem-solving environment (PSE). This paper presents an approach that takes all the power of computer algebra systems (CASs) to easily specify a scientific problem solver and to automatically generate a computer program in standard language (as Fortran or C) that implements it. An application using a well-known CAS, Mathematica, is presented to show the methodology.</abstract>
	<keywords>Computer algebra systems; Problem-solving environments; Scientific software development</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=646>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Combine concept of agent and service to build distributed object-oriented system</paper_heading>
	<authors>Chunlin Li, Layuan Li</authors>
	<abstract>This paper presents a Java-based agent-oriented and service-oriented environment, which combines the concept of services and agents to build distributed object-oriented system. Mobile agents are often subject to strong security restrictions, they should find services that help complete their tasks. Services are wrapped by service interface agents. This paper mainly describes how mobile agents locate specific service interface agent by submitting requests to the service server with descriptions of required services in the network, and how service interface agents dynamically register their services in service server. Finally, some conclusions are given.</abstract>
	<keywords>Mobile agent; Service; XML; Service discovery; Distributed system</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=647>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Migration control for mobile agents based on passport and visa</paper_heading>
	<authors>Sheng-Uei Guan, Tianhan Wang, Sim-Heng Ong</authors>
	<abstract>Research on mobile agents has attracted much attention as this paradigm has demonstrated great potential for the next-generation e-commerce. Proper solutions to security-related problems become key factors in the successful deployment of mobile agents in e-commerce systems. We propose the use of passport and visa (P/V) for securing mobile agent migration across communities based on the SAFER (secure agent fabrication, evolution, and roaming) for e-commerce framework. P/V not only serve as up-to-date digital credentials for agent–host authentication, but also provide effective security mechanisms for online communities to control mobile agent migration. Protection for mobile agents, network hosts, and online communities is enhanced using P/V. We discuss the design issues in detail and evaluate the implementation of the proposed system.</abstract>
	<keywords>Passports and visa; Migration; Mobile agents; E-commerce; SAFER</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=648>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Future applications and middleware, and their impact on the infrastructure</paper_heading>
	<authors>Brian E. Carpenter</authors>
	<abstract>The Internet today has progressed from being “just a network” through phases as a human communications mechanism and an unrivalled information system to the mere beginning of being a true services network. On the way, it has encountered a series of technical and external challenges that have created some barriers to further development. For the Internet to release its known potential and develop as a resource-sharing computing services platform, these barriers must be torn down.</abstract>
	<keywords>Grid computing; Web services; Internet transparency; IPv6; Open Grid Services Architecture</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=649>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Grid high performance networking in the DataGRID project</paper_heading>
	<authors>Pascale Vicat-Blanc-Primet</authors>
	<abstract>This paper presents an overview of the high performance Grid networking activities in the DataGRID project. We examine the main issues raised at network level by the design and the development of a large-scale Grid Testbed. We present the application requirement studies, the underlying network infrastructure and the initial network monitoring architecture that is under development in the context of the project.</abstract>
	<keywords>Computational Grid; Grid networking; Grid monitoring; Network performance; GMA</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=650>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VLAM-G: a grid-based virtual laboratory</paper_heading>
	<authors>Adam S.Z. Belloum, David L. Groep, Zeger W. Hendrikse, Bob L.O. Hertzberger, ... Dmitry Vasunin</authors>
	<abstract>The Grid-based Virtual Laboratory AMsterdam (VLAM-G) provides a science portal for distributed analysis in applied scientific research. By facilitating access to distributed compute and information resources held by multiple organizations, and providing remote experiment control, data management and information retrieval capabilities, it allows scientists to better analyze their data. The ability to use data from multiple sources and correlating these data sets without in-depth domain expertise is a prime goal of the system. This paper describes the design and an implementation prototype of the VLAM-G platform. The feasibility of the system is demonstrated by a generalized sample scenario from the chemo-physical analysis domain.</abstract>
	<keywords>VLAM-G; Virtual laboratories; Problem Solving Environments (PSEs); Grid</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=651>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Global serverless videoconferencing over IP</paper_heading>
	<authors>Thomas C. Schmidt, Matthias Wählisch, Hans L. Cycon, Mark Palkow</authors>
	<abstract>In recent years the capabilities of the common Internet infrastructure have increased to an extent where data intensive communication services may mature to become popular, reliable applications. Videoconferencing over IP can be seen as such a highly prominent candidate. However, heavy infrastructure and complicated call handling hinder acceptance of standard solutions. This paper presents a more lightweight framework—both communication scheme and conferencing software—to overcome these deficiencies. A simple, ready-to-use global location scheme for conference users is proposed. First practical experiences are reported.</abstract>
	<keywords>Peer-to-peer videoconferencing; User locating; User address resolution; Multicast videoconferencing; Wavelet transform</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=652>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and evaluation of a multi-user virtual audio chat</paper_heading>
	<authors>Maja Matijasevic, Lea Skorin-Kapov</authors>
	<abstract>As advanced Internet services, networked virtual reality (NVR) applications impose certain quality of service (QoS) requirements, due to rich multimedia content and perceived “real-time” interactivity. Two different representations of QoS are needed at the user/application level and at the communication level, which our approach attempts to relate using as a reference an interconnection model for NVR applications. We present the design and development of a multi-user virtual audio chat application and a performance evaluation based on QoS requirements. Objective and subjective methods for determining QoS have been addressed, and our results analysed for the purpose of possible QoS improvements.</abstract>
	<keywords>Virtual reality; Quality of service; Subjective quality; Multi-user application; Audio chat</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=653>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Architecture of a shared-image electronic whiteboard in telemedicine</paper_heading>
	<authors>Michael Fromme, Helmut Pralle</authors>
	<abstract>In this paper we present a software whiteboard for multimedia conferences. The whiteboard is specialised to load and share high-resolution colour images while using colour management functions to provide an accurate colour representation. The whiteboard is used in the telemedicine project INTER-FACE, which builds a virtual environment for tele-consultations between specialists in pre-operative treatment and the planning of the cranio-maxillofacial surgery for patients with facial abnormalities.</abstract>
	<keywords>Multimedia; Conferencing Whiteboard; Colour management systems; Telemedicine</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=654>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>New security services based on PKI</paper_heading>
	<authors>Antonio F. Gómez, Gregorio Martı́nez, Óscar Cánovas</authors>
	<abstract>The basic job of a public key infrastructure (PKI) is to define the mechanisms used both to allow a recipient of a signed message to trust a digital signature and to allow a sender to find the encryption key for a recipient. It is comprised of those elements needed to manage and enable the effective use of public key cryptography technology, particularly in medium and large-scale. Nowadays, PKIs are considered to be a key element for providing security to new distributed application environments. However, the sole use of a PKI as a mechanism to create a link between an identifier and a public key is not enough to offer effective mechanisms to those environments. New security services based on PKI try to address some existing drawbacks of current distributed systems. On the one hand, they can provide mechanisms to alleviate the lack of support for decentralized and automated management of access control and authorization policies, which are usually configured using inefficient and error-prone methods. On the other hand, new security services can also be used to link authorization information to the public keys being certified by the PKI, therefore enabling new opportunities for distributed access control procedures. In this paper, we describe two of these innovative security services built over our own designed and implemented Java IPv6 PKI: a distributed security policy management architecture and a distributed credential management system.</abstract>
	<keywords>Public key infrastructure; Security service; SPKI credential; Security policy; Distributed management</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=655>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PKIX-based certification infrastructure implementation adapted to non-personal end entities</paper_heading>
	<authors>Eduardo Jacob, Fidel Liberal, Juanjo Unzilla</authors>
	<abstract>Public Key Infrastructures (PKIs) are considered the most suitable system to provide basic security services through digital certificates use. Nevertheless, traditional way of operation, based on web interface and asynchronous interactions, as well as the cost and difficulty of registration processes, have caused their replacement by other systems in many of the scenarios for which PKIs were conceived. These more efficient solutions present generally laxer security mechanisms and require too much user knowledge. The system we propose tries to provide a bridge between both approaches by defining an automated PKI focused on specific application scopes by using on-line interaction procedures.</abstract>
	<keywords>PKI; Certificate; Management; X.509; Enrolment</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=656>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The PERMIS X.509 role based privilege management infrastructure</paper_heading>
	<authors>David W. Chadwick, Alexander Otenko</authors>
	<abstract>This paper describes the EC PERMIS project, which has developed a role based access control infrastructure that uses X.509 attribute certificates (ACs) to store the users’ roles. All access control decisions are driven by an authorisation policy, which is itself stored in an X.509 AC, thus guaranteeing its integrity. All the ACs can be stored in one or more LDAP directories, thus making them widely available. Authorisation policies are written in XML according to a DTD that has been published at XML.org. The Access Control Decision Function (ADF) is written in Java and the Java API is simple to use, comprising of just three methods and a constructor. There is also a Privilege Allocator, which is a tool that constructs and signs ACs and stores them in an LDAP directory for subsequent use by the ADF.</abstract>
	<keywords>Trust management; X.509; Attribute certificates; Role based access controls; XML; Privilege management infrastructure</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=657>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed policy-based management of measurement-based traffic engineering: design and implementation</paper_heading>
	<authors>S. Van den Berghe, P. Van Heuven, J. Coppens, F. De Turck, P. Demeester</authors>
	<abstract>This article discusses an architecture using monitoring feedback as an assisting factor for delivering QoS on packet-based networks. The handling of this feedback is done in an automated way, through the use of a policy-based management architecture. For this, a formal model for describing data plane and measurement objects was translated into an XML-based configuration language. On top of this, a proof-of-concept management architecture was developed and evaluated, using both a modified network simulator and enhanced Linux prototype routers.</abstract>
	<keywords>Traffic engineering; Monitoring; Policy-based management; Real-time management</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=658>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>NetSEC: metrology-based application for network security</paper_heading>
	<authors>Jean-François Scariot, Bernard Martinet</authors>
	<abstract>We know how to use metrology to discover functional problems on networks and how to measure error rate to detect active or passive abnormal behaviour items. We can even use it to observe the network use and to detect abnormal consumption of resources. This led us to develop a flow analyses application to secure networks. Built around a relational database system, filled by metrology platform and supporting multi-criteria sort queries, NetSEC allows analysis of unusual traffic in order to find its roots. A user-friendly web interface allows the use of the NetSEC application close to final users.</abstract>
	<keywords>Metrology; Security; Relational database; Data mining</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=659>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>QoS and SLA aspects across multiple management domains: the SEQUIN approach</paper_heading>
	<authors>Christos Bouras, Mauro Campanella, Michal Przybylski, Afrodite Sevasti</authors>
	<abstract>The aim of this work is to define and implement an end-to-end approach to quality of service (QoS), operating across multiple management domains and exploiting a combination of link layer technologies. The architecture for the Premium IP service is presented, which aims at offering the equivalent of an end-to-end virtual leased line service at the IP layer across multiple domains. Also, the results of the initial testing performed for the validation of the service and the provisioning model for Premium IP are described. The work presented has been carried out in the framework of SEQUIN, a European Commission-funded research project.</abstract>
	<keywords>Quality of service; Premium IP service; Differentiated services; Service level agreement; Monitoring</keywords>
	<publication_month_year>2003-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 2</volumes_issues>
</paper>
<paper no=660>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optimization of Lie group methods for differential equations</paper_heading>
	<authors>S. Blanes, F. Casas</authors>
	<abstract>In this paper we present a technique for reducing to a minimum the number of commutators required in the practical implementation of Lie group methods for integrating numerically matrix differential equations. This technique is subsequently applied to the linear and nonlinear case for constructing new geometric integrators, optimal with respect to the number of commutators.</abstract>
	<keywords>Lie group solvers; Magnus expansion; Geometric integration</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=661>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Commutator-free Lie group methods</paper_heading>
	<authors>Elena Celledoni, Arne Marthinsen, Brynjulf Owren</authors>
	<abstract>We propose a new format of Lie group methods which does not involve commutators and which uses a much lower number of exponentials than those proposed by Crouch and Grossman. By reusing flow calculations in different stages, the complexity is even further reduced. We argue that the new methods may be particularly useful when applied to problems on homogeneous manifolds with large isotropy groups, or when used for stiff problems. Numerical experiments verify these claims when applied to a problem on the orthogonal Stiefel manifold, and to an example arising from the semidiscretization of a linear inhomogeneous heat conduction problem.</abstract>
	<keywords>Lie group methods; Geometric integration; Numerical analysis; Commutators</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=662>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Singular value decomposition of time-varying matrices</paper_heading>
	<authors>Markus Baumann, Uwe Helmke</authors>
	<abstract>This paper is concerned with an algorithm to compute the singular value decomposition (SVD) of time-varying square matrices. In a first step we consider the task of diagonalizing symmetric time-varying matrices A(t). A differential equation is proposed, whose solutions asymptotically track the diagonalizing transformation. In particular, perfect matching of the initial conditions is not required and the solutions converge exponentially towards the desired transformation. Then the desired differential equation for tracking the SVD is derived. Robustness of the algorithms is guaranteed by our approach.</abstract>
	<keywords>Singular value decomposition; Time-varying matrices; Continuation methods</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=663>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Orthonormal integrators based on Householder and Givens transformations</paper_heading>
	<authors>Luca Dieci, Erik S. Van Vleck</authors>
	<abstract>We consider refined implementations of algorithms based on Householder and Givens transformations to find the Q-factor in the QR-factorization of a matrix solution of linear time dependent differential systems. After discussing the algorithms, we introduce a suite of integrators, QRINT, and provide numerical testing to show the efficiency and accuracy of our techniques.</abstract>
	<keywords>Householder; Givens transformations; Orthonormal integrators</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=664>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Increment formulations for rounding error reduction in the numerical solution of structured differential systems</paper_heading>
	<authors>Mark Sofroniou, Giulia Spaletta</authors>
	<abstract>Strategies for reducing the effect of cumulative rounding errors in geometric numerical integration are outlined. The focus is, in particular, on the solution of separable Hamiltonian systems using explicit symplectic integration methods and on solving orthogonal matrix differential systems using projection. Examples are given that demonstrate the advantages of an increment formulation over the standard implementation of conventional integrators. We describe how the aforementioned special purpose integration methods have been set up in a uniform, modular and extensible framework being developed in the problem solving environment Mathematica.</abstract>
	<keywords>Geometric numerical integration; Separable Hamiltonian differential equations; Symplectic methods; Composition methods; Orthogonal projection</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=665>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The global error of Magnus methods based on the Cayley map for some oscillatory problems</paper_heading>
	<authors>F. Diele, S. Ragni</authors>
	<abstract>This paper deals with numerical methods for the discretization of highly oscillatory systems. We approach the problem by writing the solution in terms of the Magnus expansion based on the Cayley map. The global error, obtained when the method is applied to the linear oscillator, is investigated. Moreover, we provide numerical experiments in order to validate our theoretical results.</abstract>
	<keywords>High oscillatory systems; Magnus methods; Cayley map; Global error</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=666>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multi-symplectic integration methods for Hamiltonian PDEs</paper_heading>
	<authors>Brian E. Moore, Sebastian Reich</authors>
	<abstract>Recent results on numerical integration methods that exactly preserve the symplectic structure in both space and time for Hamiltonian PDEs are discussed. The Preissman box scheme is considered as an example, and it is shown that the method exactly preserves a multi-symplectic conservation law and any conservation law related to linear symmetries of the PDE. Local energy and momentum are not, in general, conserved exactly, but semi-discrete versions of these conservation laws are. Then, using Taylor series expansions, one obtains a modified multi-symplectic PDE and modified conservation laws that are preserved to higher order. These results are applied to the nonlinear Schrödinger (NLS) equation and the sine-Gordon equation in relation to the numerical approximation of solitary wave solutions.</abstract>
	<keywords>Multi-symplectic PDEs; Preissman box scheme; Conservation laws; Modified equations; Backward error analysis</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=667>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multi-symplectic methods for generalized Schrödinger equations</paper_heading>
	<authors>A.L. Islas, C.M. Schober</authors>
	<abstract>Recent results on spectral and finite difference multi-symplectic schemes for one- and two-dimensional PDEs are discussed. Multi-symplectic schemes for the one-dimensional nonlinear Schrödinger equation and the two-dimensional Gross–Pitaevskii equation are developed. The new schemes exactly preserve a discrete multi-symplectic conservation law. The conservation of local energy and momentum is examined as well as preservation of several global invariants.</abstract>
	<keywords>Multi-symplectic integrator; Geometric integrator; Nonlinear Schrödinger equation; Gross–Pitaevskii equation</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=668>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel multiple time-scale reversible integrator for dynamics simulation</paper_heading>
	<authors>Zhidong Jia, Ben Leimkuhler</authors>
	<abstract>We investigate parallelizable schemes for simulating dynamics of conservative systems admitting a decomposition into weakly coupled subsystems. A new method, related to reversible averaging [B. Leimkuhler, S. Reich, J. Comput. Phys. 171 (2001) 95–114] is introduced for constrained and unconstrained dynamics. Stability issues are discussed.</abstract>
	<keywords>N-body problems; Hamiltonian systems; Time-reversible discretization; Averaging</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=669>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computation of few Lyapunov exponents by geodesic based algorithms</paper_heading>
	<authors>N. Del Buono, C. Elia</authors>
	<abstract>In this paper, we apply numerical methods based on the embedded geodesics for computing few Lyapunov exponents of a finite dimensional dynamical system. These new numerical algorithms are designed using geometric structure of the Stiefel manifold and they preserve orthogonality to machine accuracy. Numerical tests are also provided in order to show the features of our methods.</abstract>
	<keywords>Lyapunov exponents; Embedded geodesic; Parallel transport; Weakly skew-symmetric system; Runge–Kutta schemes</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=670>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Applying fixed point homotopy to nonlinear DAEs deriving from switching circuits</paper_heading>
	<authors>E. Chiarantoni, G. Fornarelli, S. Vergura, T. Politi</authors>
	<abstract>The time-domain analysis of switching circuits is a time consuming process as the change of switch state produces sharp discontinuities in switch variables. In this paper, a method for fast time-domain analysis of switching circuits is described. The proposed method is based on piecewise temporal transient analysis windows joined by DC analysis at the switching instants. The DC analysis is carried out by means of fixed point homotopy to join operating points between consecutive time windows. The proposed method guarantees accurate results reducing the number of iterations needed to simulate the circuit.</abstract>
	<keywords>Homotopy methods; Switching circuits; Time transient analysis</keywords>
	<publication_month_year>2003-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 3</volumes_issues>
</paper>
<paper no=671>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Global and localized parallel preconditioning techniques for large scale solid Earth simulations</paper_heading>
	<authors>Kai Wang, Sang-Bae Kim, Jun Zhang, Kengo Nakajima, Hiroshi Okuda</authors>
	<abstract>We investigate and compare a few parallel preconditioning techniques in the iterative solution of large sparse linear systems arising from solid Earth simulation with and without using contact information in the domain partitioning process. Previous studies are focused on using static or matrix pattern-based incomplete LU (ILU) preconditioners in a localized preconditioner implementation. Our current studies are concerned about preconditioner performance for solving two different problem configurations with and without known contact information. For the cases with contact information, we use localized threshold value-based ILU (ILUT) preconditioner to improve efficiency. For the cases without contact information, we use a global sparse approximate inverse preconditioner with a static sparsity pattern to achieve robustness. Numerical results from simulating ground motion on a parallel supercomputer are given to compare the effectiveness of these parallel preconditioning techniques.</abstract>
	<keywords>Solid Earth simulation; Fault contact problem; Sparse matrices; Preconditioning techniques</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=672>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A new key assignment scheme for enforcing complicated access control policies in hierarchy</paper_heading>
	<authors>Iuon-Chang Lin, Min-Shiang Hwang, Chin-Chen Chang</authors>
	<abstract>In a traditional key assignment scheme, an access control policy is used to solve the access control problem in a hierarchy. A higher security class can access lower security classes, but the opposite is not allowed. However, in some cases, this can be troublesome because of the lack of flexibility. In this paper, we shall propose a secure key assignment scheme which can be performed not only in a hierarchy but also in more complicated policies with anti-symmetrical and transitive exceptions.</abstract>
	<keywords>Access control; Cryptography; Data security; Key assignment; Multilevel security</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=673>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance of a distributed architecture for query processing on workstation clusters</paper_heading>
	<authors>Cyrus Soleimany, Sivarama P. Dandamudi</authors>
	<abstract>A network of workstations (NOWs) is an attractive alternative to parallel database systems due to the cost advantage. In a typical database, client workstations (nodes) submit queries/transactions and receive responses from the database server. With even recent PC-based client nodes providing traditional workstation-class performance, performance improvements can be obtained by offloading some of the processing typically done on the traditional server node to these powerful client nodes. Parallel query processing takes advantage of the idle cycles on the client nodes to process the query. In this paper we introduce a distributed architecture for parallel query processing and study the performance of this architecture in various scenarios. We have implemented the distributed architecture using parallel virtual machine (PVM) on a Pentium-based NOW system. Our results show that the distributed architecture successfully achieves good speedups and scaleups. Furthermore, our results show that the distributed architecture handles both light and heavy workloads in the presence of a background (non-query) load.</abstract>
	<keywords>Parallel query processing; Parallel database systems; Networks of workstations; Load sharing; Workstation clusters</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=674>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optimistic parallel simulation of a large-scale view storage system</paper_heading>
	<authors>Garrett Yaun, Christopher D. Carothers, Sibel Adali, David Spooner</authors>
	<abstract>In this paper, we present the design and implementation of a complex view storage system model. Here, a hierarchy of view storage servers are connected to an array of client-side local disks. The term view refers to the output or result of a query made on the part of an application that is executing on a client machine. These queries can to be arbitrarily complex and formulated using SQL. The goal of this system is to reduce the turnaround time of queries by exploiting locality both at the local disk level as well as between clients and servers prior to making the request to the highest level database server. This model has been designed for execution with an optimistic simulation engine. One of the primary drawbacks of this parallel synchronization mechanism has been high overheads due to state-saving. We attack this problem by implementing the model using reverse computation. Here, the event processing routines are made reversible, which avoids having incrementally state-saving values that can be reverse computed, such as ++ and −−. Destructive assignments of the form a=b are saved using a swap operation, which precludes the need for additional state space. In our performance study of this application, we find that speedups range from 1.5 to over 5 on four processors. Super-linear speedups are attributed to a slow memory subsystem and the increased availability of level-1 and level-2 cache when moving to a larger number of processors.</abstract>
	<keywords>Optimistic parallel simulation; Reverse computation; View storage systems</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=675>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Analysis of k-ary n-cubes with dimension-ordered routing</paper_heading>
	<authors>H. Sarbazi-Azad, A. Khonsari, M. Ould-Khaoua</authors>
	<abstract>K-ary n-cubes have been one of the most popular interconnection networks for practical multicomputers due to their ease of implementation and ability to exploit communication locality found in many parallel applications. This paper describes an analytical model for k-ary n-cubes with dimension-ordered routing. The main feature of the model is its ability to captures network performance when an arbitrary number of virtual channels are used to reduce message blocking. Simulation experiments reveal that the latency results predicted by the analytical model are in good agreement with those provided by the simulation model.</abstract>
	<keywords>Multicomputer networks; Dimension-ordered routing; Virtual channel multiplexing; Message latency M/G/1 queuing systems</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=676>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Adaptive scheduling under memory constraints on non-dedicated computational farms</paper_heading>
	<authors>Dimitrios S. Nikolopoulos, Constantine D. Polychronopoulos</authors>
	<abstract>This paper presents scheduler extensions that enable better adaptation of parallel programs to the execution conditions of non-dedicated computational farms with limited memory resources. The purpose of the techniques is to prevent thrashing and co-schedule communicating threads, using two disjoint, yet cooperating extensions to the kernel scheduler. A thrashing prevention module enables memory-bound programs to adapt to memory shortage, via suspending their threads at selected points of execution. Thread suspension is used so that memory is not over-committed by parallel jobs—which are assumed to be running as guests on the nodes of the computational farm—at memory allocation points. In the event of thrashing, parallel jobs are the first to release memory and help local resident jobs make progress. Adaptation is implemented using a shared-memory interface in the /proc filesystem and upcalls from the kernel to the user space. On an orthogonal axis, co-scheduling is implemented in the kernel with a heuristic that boosts periodically the priority of communicating threads. Using experiments on a cluster of workstations, we show that when a guest parallel job competes with general-purpose interactive, I/O-intensive, or CPU and memory-intensive load on the nodes of the cluster, thrashing prevention reduces drastically the slowdown of the job at memory utilization levels of 20% or higher. The slowdown of parallel jobs is reduced by up to a factor of 7. Co-scheduling provides a limited performance improvement at memory utilization levels below 20%, but has no significant effect at higher memory utilization levels.</abstract>
	<keywords>Clusters; Computational grids; Scheduling; Memory management</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=677>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SMiLE: an integrated, multi-paradigm software infrastructure for SCI-based clusters</paper_heading>
	<authors>Martin Schulz, Jie Tao, Carsten Trinitis, Wolfgang Karl</authors>
	<abstract>The availability of a comprehensive software infrastructure is essential for the success of a parallel architecture. In order to allow for the greatest possible flexibility, an infrastructure has to be designed in an integrated, easy-to-use manner and with the support of multiple parallel programming paradigms and models to address a wide code base. Shared Memory in a LAN-like Environment (SMiLE) provides such an infrastructure for SCI (Scalable Coherent Interface) based clusters. It includes support for a large range of message passing libraries as well as for almost arbitrary shared memory programming models. In addition, SMiLE contains initial work on an appropriate tool set for performance optimization. The complete infrastructure is closely optimized for the underlying hardware and therefore offers its benefits to the user without significant overheads.</abstract>
	<keywords>Software infrastructure; Cluster computing; Message passing; Shared memory; Monitoring support</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=678>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementing data cube construction using a cluster middleware: algorithms, implementation experience, and performance evaluation</paper_heading>
	<authors>Ge Yang, Ruoming Jin, Gagan Agrawal</authors>
	<abstract>With increases in the amount of data available for analysis in commercial settings, on line analytical processing (OLAP) and decision support have become important applications for high performance computing. Implementing such applications on clusters requires a lot of expertise and effort, particularly because of the sizes of input and output datasets. In this paper, we describe our experiences in developing one such application using a cluster middleware, called ADR. We focus on the problem of data cube construction, which commonly arises in multi-dimensional OLAP. We show how ADR, originally developed for scientific data intensive applications, can be used for carrying out an efficient and scalable data cube construction implementation. A particular issue with the use of ADR is tiling of output datasets. We present new algorithms that combine interprocessor communication and tiling within each processor. These algorithms preserve the important properties that are desirable from any parallel data cube construction algorithm. We have carried out a detailed evaluation of our implementation. The main results from our experiments are as follows: (1) high speedups are achieved on both dense and sparse datasets, even though we have used simple algorithms that sequentialize a part of the computation; (2) the execution time depends only upon the amount of computation, and does not increase in a super-linear fashion as the dataset size or the number of tiles increases; and (3) as the datasets become more sparse, sequential performance degrades, but the parallel speedups are still quite good. As part of our on-going work in this area, we are also looking at handling a larger number of dimensions and multi-dimensional partitionings. We describe our preliminary theoretical and experimental work in this direction.</abstract>
	<keywords>Data cube construction; Cluster middleware; Data intensive computing; Performance evaluation</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=679>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Internet Backplane Protocol: a study in resource sharing</paper_heading>
	<authors>Alessandro Bassi, Micah Beck, Terry Moore, James S. Plank, ... Graham Fagg</authors>
	<abstract>In this work we present the Internet Backplane Protocol (IBP), a middleware created to allow the sharing of storage resources, implemented as part of the network fabric. IBP allows an application to control intermediate data staging operations explicitly. As IBP follows a very simple philosophy, very similar to the Internet Protocol, and the resulting semantic might be too weak for some applications, we introduce the exNode, a data structure that aggregates storage allocations on the Internet.</abstract>
	<keywords>Internet Backplane Protocol; Resource sharing; Logistical networking; Grid computing; Store and forward networking; Asynchronous communications; Network storage; End-to-end design; Scalability; Computing center</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=680>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual private grid: a command shell for utilizing hundreds of machines efficiently</paper_heading>
	<authors>Kenji Kaneda, Kenjiro Taura, Akinori Yonezawa</authors>
	<abstract>We describe design and implementation of virtual private grid (VPG), a shell that can utilize many machines distributed over multiple subnets. VPG works around common security policies (e.g., firewall, private IP, DHCP) that restrict communication between machines and even break uniqueness of IP addresses. VPG provides the following functions: (1) a unique nickname to each machine that does not depend on a DNS name or a fixed IP address; (2) job submissions to any nicknamed machine; (3) redirections from/to a file on any nicknamed machine; (4) pipes between commands executed on any nicknamed machine. VPG implements the above functions by constructing a self-stabilizing spanning tree among machines and forwarding messages via a path in the tree. We ran VPG on about 100 nodes (270 CPUs) and measured a turn around time of a small job submission with VPG and other tools: rsh, SSH, and globus-job-run. The experimental result shows that VPG can submit a job faster than SSH and globus-job-run, since VPG performs authentication only when it constructs a tree.</abstract>
	<keywords>Distributed supercomputing; Collaborative grid applications; Peer-to-Peer computing</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=681>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PadicoTM: an open integration framework for communication middleware and runtimes</paper_heading>
	<authors>Alexandre Denis, Christian Pérez, Thierry Priol</authors>
	<abstract>Computational grids are seen as the future emergent computing infrastructures. Their programming requires the use of several paradigms that are implemented through communication middleware and runtimes. However some of these middleware systems and runtimes are unable to take benefit of specific networking technologies available in grid infrastructures. In this paper, we describe an open integration framework that allows several communication middleware and runtimes to efficiently share the networking resources. Such framework encourages grid programmers to use the most suited communication paradigms for their applications independently from the underlying networks. Therefore, there is no obstacle to deploy the applications on a specific grid configuration.</abstract>
	<keywords>Communication framework; Middleware integration; Code coupling; High-performance network</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=682>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using TOP-C and AMPIC to port large parallel applications to the Computational Grid</paper_heading>
	<authors>Gene Cooperman, Henri Casanova, Jim Hayes, Thomas Witzel</authors>
	<abstract>Porting large parallel applications to new and various distributed computing platforms is a challenging task from a software engineering perspective. The primary aim of this paper is to demonstrate how the development time to port very large applications to the Computational Grid can be significantly reduced. TOP-C and AMPIC are software packages that have each seen successful applications in their respective domains of parallel computing and process creation/communication over the Computational Grid. We combined the two packages in 1 man-week, thereby leveraging several man-years of previous independent software development. As a real world test case, the 1,000,000 line Geant4 sequential application was then deployed over the Computational Grid in 3 man-weeks by using TOP-C/AMPIC. The cluster parallelization of Geant4 using TOP-C is now included as part of the Geant4 4.1 distribution, and the integration of TOP-C/AMPIC and the Globus protocols will additionally enable the use of the fundamental Grid middleware services in the future.</abstract>
	<keywords>Computational Grid; Master–worker parallelism; TOP-C; AMPIC; Geant4</keywords>
	<publication_month_year>21-24 May 2002</publication_month_year>
	<volumes_issues>Volume 19, Issue 4</volumes_issues>
</paper>
<paper no=683>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automated scientific software scripting with SWIG</paper_heading>
	<authors>D.M. Beazley</authors>
	<abstract>Scripting languages such as Python and Tcl are a powerful tool for the construction of flexible scientific software because they provide scientists with an interpreted problem solving environment and they provide a modular framework for controlling software components written in C, C++, and Fortran. However, a common problem faced by the developers of a scripted scientific application is that of integrating compiled code with an interpreter. To solve this problem, an extensible compiler, simplified wrapper and interface generator (SWIG), has been developed to automate the task of integrating compiled code with scripting language interpreters. SWIG requires no modifications to existing code and uses existing source to create bindings for nine different target languages including Python, Perl, Tcl, Ruby, Guile, and Java. By automating language integration, SWIG enables scientists to use scripting languages at all stages of software development and allows existing software to be more easily integrated into a scripting environment. Although SWIG has been in use for more than 6 years, little has been published on its design and the underlying mechanisms that make it work. Therefore, the primary goal of this paper is to cover these topics.</abstract>
	<keywords>Scientific software; SWIG; Scripting languages; Python; Interface compiler</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=684>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An advanced environment supporting structured parallel programming in Java</paper_heading>
	<authors>M. Aldinucci, M. Danelutto, P. Teti</authors>
	<abstract>In this work we present Lithium, a pure Java structured parallel programming environment based on skeletons (common, reusable and efficient parallelism exploitation patterns). Lithium is implemented as a Java package and represents both the first skeleton based programming environment in Java and the first complete skeleton based Java environment exploiting macro-data flow implementation techniques. Lithium supports a set of user code optimizations which are based on skeleton rewriting techniques. These optimizations improve both absolute performance and resource usage with respect to original user code. Parallel programs developed using the library run on any network of workstations provided the workstations support plain JRE. The paper describes the library implementation, outlines the optimization techniques used and eventually presents the performance results obtained on both synthetic and real applications.</abstract>
	<keywords>Java; Parallel programming; Skeletons; Macro-data flow; Optimizations</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=685>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automatic component protocol adaptation with the CoConut/J tool suite</paper_heading>
	<authors>Ralf H. Reussner</authors>
	<abstract>While industrial middleware platforms such as CORBA, EJB, or .NET facilitate the development of distributed applications by providing certain infra-structural services required in many distributed systems (such as name services, remote method calls, parameter marshalling, etc.), these industrial platforms fail to support the development of distributed systems with independent components. In particular, their component models do not provide sufficient information for component interoperability checks or automated component adaptation. Especially when considering the personal and institutional separation between component developer and component vendor as one of the prerequisites of an independent component market, finding automatically as many component interoperability errors as possible is crucial. Hence, it is of practical concern that component interfaces not only model the correct way of calling the single methods but also the valid sequences of method calls. Likewise, practice clearly shows that component reuse usually requires component adaptation. This directly shows that only detecting incompatibilities is of limited use, but advocates for techniques of automated component adaptation. In this paper we describe algorithms and tools for specifying and analysing component interfaces in order to check interoperability and to generate adapted component interfaces automatically. Therefore, we introduce the concept of parameterised contracts and a new component interface model.</abstract>
	<keywords>Component protocol specification; Automatic protocol adaptation; Parameterised contracts; Distributed middleware platforms</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=686>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A monitoring system for multithreaded applications</paper_heading>
	<authors>Bartosz Baliś, Marian Bubak, Włodzimierz Funika, Roland Wismüller</authors>
	<abstract>Multithreading is an efficient solution for parallel programming, however, multithreaded applications development is rather poorly supported by specialized tools. This paper presents an analysis which has led to a concept of building an autonomous monitoring system for multithreaded programs on top of which various tools can be based. Many problems specific to monitoring multithreaded programs are presented, as well as the ideas to solve them. Specifically, we focus on efficiency, scalability and portability of the monitoring system.</abstract>
	<keywords>Multithreading; Monitoring; Parallel tools; Shared memory</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=687>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Monitoring of distributed Java applications</paper_heading>
	<authors>Marian Bubak, Włodzimierz Funika, Roland Wismüller, Piotr Mętel, Rafał Orłowski</authors>
	<abstract>The paper presents an approach to distributed Java monitoring based on an extension to the on-line monitoring interface specification that defines relationships between a monitoring system and on-line tools used to support the development of Java applications. The extension aims at defining a standard for an open interface that supports on-line software development tools. The paper also presents the implementation ideas on building a monitoring infrastructure based on the specification. This comprises the architecture of the infrastructure, the mechanism of interaction with the Java virtual machine, and sample scenarios of interactions between the tool and the monitoring system. The infrastructure provides a uniform monitoring support for different kinds of tools, like debuggers or performance analysers.</abstract>
	<keywords>Java; Monitoring system; Monitoring interface; Distributed object system</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=688>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Debugging scientific applications in the .NET Framework</paper_heading>
	<authors>David Abramson, Greg Watson</authors>
	<abstract>The Microsoft .NET Framework represents a major advance over previous runtime environments available for Windows platforms and offers a number of architectural features that would be of value in scientific programs. However there are such major differences between .NET and legacy environments under both Windows and UNIX, that the effort of migrating software is substantial. Accordingly, software migration is unlikely to occur unless tools are developed for supporting this process. In this paper we discuss a ‘relative debugger’ called Guard which provides powerful support for debugging programs as they are ported from one environment or platform to another. We describe a prototype implementation developed for Microsoft’s Visual Studio.NET—a rich interactive environment that supports code development for the .NET Framework. The paper discusses the overall architecture of Guard under VS.NET and highlights some of the technical challenges that were encountered during its development. A simple case study is provided that demonstrates the effectiveness of relative debugging in locating subtle errors that occur when even a minor upgrade is attempted from one version of a language to another. For this example, we illustrate the use of relative debugging using a Visual Basic program that was ported from Visual Basic 6.0 to Visual Basic.NET.</abstract>
	<keywords>Microsoft .NET Framework; Common Language Specification; Relative debugger</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=689>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Debugging shared memory parallel programs using record/replay</paper_heading>
	<authors>Michiel Ronsse, Mark Christiaens, Koen De Bosschere</authors>
	<abstract>This paper presents a debugging aid for parallel program developers. The tool presented enables programmers to use cyclic debugging techniques for debugging non-deterministic parallel programs running on multiprocessor systems with shared memory. The solution proposed consists of a combination of record/replay with automatic on-the-fly data race detection. This combination enables us to limit the record phase to the more efficient recording of the synchronization operations, and checking for data races (using intrusive methods) during a replayed execution. As the record phase is highly efficient, there is no need to switch it off, hereby eliminating the possibility of Heisenbugs because tracing can be left on all the time.</abstract>
	<keywords>Record/replay; Debugging; Non-deterministic parallel programs</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=690>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Error detection in large-scale parallel programs with long runtimes</paper_heading>
	<authors>Dieter Kranzlmüller, Nam Thoai, Jens Volkert</authors>
	<abstract>Error detection is an important activity of program development, which is applied to detect incorrect computations or runtime failures of software. The costs of debugging are strongly related to the complexity and the scale of the investigated programs. Both characteristics are especially cumbersome for large-scale parallel programs with long runtimes, which are quite common in computational science and engineering (CSE) applications. A solution is offered by a combination of techniques using the event graph model as a representation of parallel program behaviour. With process isolation, a subset of the original number of processes can be investigated, while the absent processes are simulated by the debugging system. With checkpointing, an arbitrary temporal section of a program’s runtime can be extracted for exhaustive analysis without the need to restart the program from the beginning. Additional benefits of the event graph are support of equivalent execution of nondeterministic programs, as well as a comprehensible visualisation as a space–time diagram.</abstract>
	<keywords>Error detection; Space–time diagrams; Parallel program; Debugging; Event graph</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=691>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Immersive and 3D viewers for CUMULVS: VTK/CAVE™ and AVS/Express</paper_heading>
	<authors>Torsten Wilde, James A. Kohl, Raymond E. Flanery</authors>
	<abstract>This paper will discuss the development of two viewer interfaces for the CUMULVS system, one for virtual reality (VR) visualization via ImmersaDesk™/CAVE™ systems, and one for 3D data visualization using the commercial component-based AVS/Express visualization environment. The CUMULVS (Collaborative, User Migration, User Library for Visualization and Steering) system, developed at Oak Ridge National Laboratory, is an essential platform for interacting with high-performance scientific simulation programs on-the-fly. It provides runtime visualization of data while they are being computed, as well as coordinated computational steering, application-directed checkpointing and fault recovery mechanisms, and rudimentary model coupling functions. CUMULVS consists of a set of cooperative libraries that enable the dynamic attachment of front-end “viewer” programs to running applications, for interactive visualization of extracted data snapshots using a variety of visualization tools. A development strategy will be presented for integrating CUMULVS with the given visualization libraries and environments, including discussion of the various data transformations and the visualization pipeline necessary for converting raw CUMULVS data into fully rendered graphical entities. In addition to the immersive VR CUMULVS viewer, an overview of the object-based AVS/Express CUMULVS viewer design will be presented, including discussion of the various components, modules, macros and user interfaces. A comparison will be made of the two viewer design approaches.</abstract>
	<keywords>CUMULVS; ImmersaDesk™; CAVE™; Scientific visualization; Immersive visualization</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=692>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>AVISPA: visualizing the performance prediction of parallel iterative solvers</paper_heading>
	<authors>V. Blanco, P. González, J.C. Cabaleiro, D.B. Heras, ... F.F. Rivera</authors>
	<abstract>The selection of the best method and preconditioner for solving a sparse linear system is as determinant as the efficient parallelization of the selected method. We propose a tool for helping to solve both problems on distributed memory multiprocessors using iterative methods. Based on a previously developed library of HPF and message-passing interface (MPI) codes, a performance prediction is developed and a visualization tool () is proposed. The tool combines theoretical features of the methods and preconditioners with practical considerations and predictions about aspects of the execution performance (computational cost, communications overhead, etc.). It offers detailed information about all the topics that can be useful for selecting the most suitable method and preconditioner. Another capability is to offer information on different parallel implementations of the code (HPF and MPI) varying the number of available processors.</abstract>
	<keywords>Performance tools; Performance prediction; Parallel iterative solvers; HPF; MPI</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=693>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Flexible performance visualization of parallel and distributed applications</paper_heading>
	<authors>J. Chassin de Kergommeaux, B. de Oliveira Stein</authors>
	<abstract>Performance debugging of parallel and distributed applications can benefit from behavioral visualization tools helping to capture the dynamics of the executions of applications. The Pajé generic tool presented in this article provides interactive and scalable behavioral visualizations; because of its genericity, it can be used unchanged in a large variety of contexts</abstract>
	<keywords>Performance debugging; Behavioral visualization; Interactivity; Scalability; Genericity; Pajé</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=694>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using SKaMPI for developing high-performance MPI programs with performance portability</paper_heading>
	<authors>Ralf H. Reussner</authors>
	<abstract>The current practice of developing high-performance software for parallel computers includes a tuning phase where the software’s performance is optimised for a specific hardware platform. This tuning phase often is costly and results in machine-specific, hence, less portable software. In this paper we present a publicly available database providing performance data for operations of the message-passing-interface (MPI) measured on several different platforms. This allows to design MPI programs for performance and portability in early stages of software development. Considering the performance of MPI operations while designing programmes allows the software developer (a) to select the fastest implementation alternative, (b) to write performance portable software (i.e., software showing high performance on several platforms without platform-specific tuning), if possible, and (c) to quantify the tradeoff between ultimate performance and performance portability for different platforms.</abstract>
	<keywords>High-performance MPI programs; Performance portability; Design of parallel software</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=695>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ARS: an adaptive runtime system for locality optimization</paper_heading>
	<authors>Jie Tao, Martin Schulz, Wolfgang Karl</authors>
	<abstract>Shared memory programs running on Non-Uniform Memory Access (NUMA) machines usually face inherent performance problems stemming from excessive remote memory accesses. A solution, called the Adaptive Runtime System (ARS), is presented in this paper. ARS is designed to adjust the data distribution at runtime through automatic page migrations. It uses memory access histograms gathered by hardware monitors to find access hot spots and, based on this detection, to dynamically and transparently modify the data layout. In this way, incorrectly allocated data can be moved to the most appropriate node and hence data locality can be improved. Simulations show that this allows to achieve a performance gain of as high as 40%.</abstract>
	<keywords>Page migration; Data locality optimization; Shared memory programming on NUMA; Hardware monitor</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=696>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Latency reduction from runtime-interference to the parallel Quantum Chemistry program GREMLIN in heterogeneous and homogeneous environments</paper_heading>
	<authors>Siegfried Höfinger</authors>
	<abstract>The runtime behaviour of the PVM version of the Quantum Chemistry program GREMLIN is monitored and analyzed with respect to the latency of the different participating hosts. A priori parallel performance is far from optimal, because of the multifold variabilities influencing the heterogeneous cluster environment. The article discusses the improvement gained when periodically re-partitioning the load based on the current actual latencies of the parallel nodes. It is also shown that even on homogeneous parallel architectures this dynamic version of Speed Weighted Load Balancing (SWLB) may aid to optimize parallel performance considerably.</abstract>
	<keywords>WAN-cluster; Load Balancing; Quantum Chemistry; PVM</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=697>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using an interactive parallelisation toolkit to parallelise an ocean modelling code</paper_heading>
	<authors>C.S. Ierotheou, S.P. Johnson, P.F. Leggett, M. Cross</authors>
	<abstract>This paper describes an interactive parallelisation toolkit that can be used to generate parallel code suitable for either a distributed memory system (using message passing) or a shared memory system (using OpenMP). This study focuses on how the toolkit is used to parallelise a complex heterogeneous ocean modelling code within a few hours for use on a shared memory parallel system. The generated parallel code is essentially the serial code with OpenMP directives added to express the parallelism. The results show that substantial gains in performance can be achieved over the single thread version with very little effort.</abstract>
	<keywords>Interactive parallelisation tools; Global ocean model code; OpenMP shared memory parallelisation; Automatic parallel code generation</keywords>
	<publication_month_year>2003-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 5</volumes_issues>
</paper>
<paper no=698>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GENIUS: a simple and easy way to access computational and data grids</paper_heading>
	<authors>A. Andronico, R. Barbera, A. Falzone, P. Kunszt, ... A. Rodolico</authors>
	<abstract>The architecture and the current implementation of the grid portal GENIUS (Grid Enabled web eNvironment for site Independent User job Submission), jointly developed by INFN and NICE within the context of INFN Grid and DataGrid projects, is presented and discussed. Particular care is devoted to the description of job submission and transparent access to user’s data and applications.</abstract>
	<keywords>Grid computing; Grid portal; INFN Grid Project; DataGrid project</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=699>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evaluating the VLAM-G toolkit on the DAS-2</paper_heading>
	<authors>Zeger W. Hendrikse, Adam S.Z. Belloum, Philip M.R. Jonkergouw, Gert B. Eijkel, ... Dmitry Vasunin</authors>
	<abstract>The Grid-based Virtual Laboratory AMsterdam (VLAM-G) provides a science portal for distributed analysis in applied scientific research. DAS-2 is a wide-area distributed computer of 200 Dual Pentium-III nodes, distributed over five Dutch universities. During the iGrid conference, the current reference implementation of VLAM-G was evaluated with an application from the chemo-physical application domain on the DAS-2. It was shown how data flows are instantiated on DAS-2 resources, driven by an information management system that is designed to extract information from raw data sets. Both the information management system and data processing modules are provided by the middleware of the Virtual Laboratory (VL). This paper describes the software and hardware setup of this study, and evaluates the use and performance of the VLAM-G science portal.</abstract>
	<keywords>VLAM-G; Virtual Laboratories; Problem solving environment; PSE; Grid; Globus</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=700>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>iGrid2002 demonstration: bandwidth from the low lands</paper_heading>
	<authors>R. Les Cottrell, Antony Antony, Connie Logg, Jiri Navratil</authors>
	<abstract>We report on a demonstration of several complementary high-performance end-to-end active network throughput measurement tools. These include: the PingWorld Java applet that displays the Round Trip Time (RTT) and losses to sites around the world from the user’s host; the multi-path analysis tool that visualizes common paths from traceroutes to selected hosts; the IEPM high-performance BandWidth monitoring toolkit which gives achievable throughput for several types of TCP data transfer applications; and the Available Bandwidth Estimation (ABWE) tool that reports in real-time the available bandwidth to several paths within the range from Mbits/s to Gbits/s. We also demonstrated sending high-speed data from 4 hosts at iGrid2002 to over 30 hosts in 10 countries to simulate a high energy physics experiment distributing data to collaborators. The demonstration utilized the high-speed, long latency, trans-Atlantic network set up for iGrid2002 in Amsterdam during September 2002.</abstract>
	<keywords>iGrid2002; High-throughput; Measurement tools; Monitoring; Networks; Tuning; TCP</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=701>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Exploring cosmology applications on distributed environments</paper_heading>
	<authors>Zhiling Lan, Valerie E. Taylor, Greg Bryan</authors>
	<abstract>A typical cosmological simulation requires a large amount of compute power, which is hard to satisfy with a single machine. Distributed systems provide the opportunity to execute such large-scale applications. As part of the iGrid Research Demonstration 2002, we explored a large-scale cosmology application on a distributed system composed of two supercomputers: one located in the Netherlands and the other in the United States. Our demo consisted of two parts: a distributed computing of the cosmology application and a visualization demonstration of the computational results. Our ultimate goal was to demonstrate the great potential of using distributed resources for large-scale applications, in particular cosmological applications, and further discuss the network requirements for efficient execution of such large-scale applications on distributed systems.</abstract>
	<keywords>Cosmology application; Distributed systems; Network requirement; Dynamic load balancing</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=702>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed, on-demand, data-intensive and collaborative simulation analysis</paper_heading>
	<authors>Arthurine Breckenridge, Lyndon Pierson, Sergiu Sanielevici, Joel Welling, ... Juergen Schulze</authors>
	<abstract>Distributed, on-demand, data-intensive, and collaborative simulation analysis tools are being developed by an international team to solve real problems such as bioinformatics applications. The project consists of three distinct focuses: compute, visualize, and collaborate. Each component utilizes software and hardware that performs across the International Grid. Computers in North America, Asia, and Europe are working on a common simulation programs. The results are visualized in a multi-way 3D visualization collaboration session where additional compute requests can be submitted in real-time. Navigation controls and data replication issues are addressed and solved with a scalable solution.</abstract>
	<keywords>Compute; Visualize; Collaborate; Data-intensive; Simulation analysis; Bioinformatics applications</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=703>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Video IBPster</paper_heading>
	<authors>Scott Atchley, Stephen Soltesz, James S. Plank, Micah Beck</authors>
	<abstract>At iGrid2002, members of the Logistical Computing and Internetworking (LoCI) Lab had two goals. The first was to present an application, Video IBPster, built using the tools of the Network Storage Stack that delivers DVD-quality video without dropping frames, without losing data and without specialized multi-media streaming servers. The Video IBPster demo easily played MPEG-2 video files encoded at bit-rates up to 15 Megabit/s (Mbps). The second goal was to determine performance limits when using multiple, untuned TCP streams to retrieve a striped and replicated file across a long network. Since tools built using the Network Storage Stack allow striped downloads from multiple servers in parallel and since the client machines were all connected to Gigabit Ethernet (GigE), we hoped that we would observe a linear scale up of throughput when downloading from multiple servers. Although we did see increased throughput, it was not linear.</abstract>
	<keywords>Logistical networking; Internet Backplane Protocol; exNode; Logistical Runtime System; Logistical Backbone; Network Storage Stack</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=704>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Griz: experience with remote visualization over an optical grid</paper_heading>
	<authors>Luc Renambot, Tom van der Schaaf, Henri E. Bal, Desmond Germans, Hans J.W. Spoelder</authors>
	<abstract>This paper describes the experiments of remote rendering over an intercontinental optical network during the iGrid2002 conference in Amsterdam from September 23 to 26. A rendering cluster in Chicago was used to generate images which were displayed in real-time on a 4-tile visualization setup in Amsterdam. On average, one gigabit per second (1 Gbps) was consumed to enable remote visualization, at interactive frame rate, with a 1600×1200 pixels configuration.</abstract>
	<keywords>Remote visualization; Parallel rendering; Optical network; Reliable UDP; Interactive application</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=705>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High data rate transmission in high resolution radio astronomy—vlbiGRID</paper_heading>
	<authors>Richard Hughes-Jones, Steve Parsley, Ralph Spencer</authors>
	<abstract>Recent developments in Very Long Baseline Interferometry (VLBI) radio astronomy are aimed at improving reliability and reducing the cost of operations by moving from a custom made tape based system for using exchangeable disks in PCs. The advent of the iGRID2002 exhibition in September 2002 with the availability of high data rate international links gave us the opportunity to try transferring the data via the internet. This would not only test the capacity of the links but also prove the viability of the use of the internet for VLBI. This development would also eventually lead to instant turn-around of VLBI data, which currently stands at several weeks. The tests were successful achieving data rates from Manchester of 500 Mbps over the production network. This was the first demonstration of fibre-optic link connected international VLBI.</abstract>
	<keywords>Radio astronomy; VLBI; Networks; ftp transfer; UDP packet stream; Real time; High performance; High throughput; Gigabit data rates</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=706>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Photonic TeraStream: enabling next generation applications through intelligent optical networking at iGRID2002</paper_heading>
	<authors>Joe Mambretti, Jeremy Weinberger, Jim Chen, Elizabeth Bacon, ... Marco Mazzucco</authors>
	<abstract>Traditionally, the design and implementation of network based applications, especially large-scale, high performance applications, have had to be compromised across multiple dimensions interfaces, services, performance, flexibility, protocols, architecture, technology, etc. These restrictions exist, in part, because the most widely deployed communications infrastructure was designed to optimize traditional communications, not high performance data communications. At iGRID2002, the International Center for Advanced Internet Research (iCAIR) and its research partners demonstrated “Photonic-Empowered Applications,” based on next generation intelligent optical networking technology and dynamic data services provisioning. These demonstrations indicated the potential for creating next generation global applications when traditional barriers to network optimization at multiple levels are removed. These application demonstrations were based on high performance communications infrastructure utilizing novel techniques for managing globally distributed resources and extremely large volume data streams. The innovative advanced optical networking technologies being developed by these research organizations will allow for many new types of high performance global applications across multiple disciplines and industries.</abstract>
	<keywords>TeraStream; iGRID2002; Advanced optical networking; Optical control planes; IP-over-DWDM; Advanced photonic technology; Dynamically switched wavelengths; Lambda Grid</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=707>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High-resolution remote rendering of large datasets in a collaborative environment</paper_heading>
	<authors>Nicholas T. Karonis, Michael E. Papka, Justin Binns, John Bresnahan, ... Joseph M. Link</authors>
	<abstract>In a time when computational and data resources are distributed around the globe, users need to interact with these resources and each other easily and efficient. The Grid, by definition, represents a connection of distributed resources that can be used regardless of the user’s location. We have built a prototype visualization system using the Globus Toolkit, MPICH-G2, and the Access Grid in order to explore how future scientific collaborations may occur over the Grid. We describe our experience in demonstrating our system at iGrid2002, where the United States and the Netherlands were connected via a high-latency, high-bandwidth network. In particular, we focus on issues related to a Grid-based application that couples a collaboration component (including a user interface to the Access Grid) with a high-resolution remote rendering component.</abstract>
	<keywords>MPI; Globus Toolkit; GridFTP; Access Grid; MPICH-G2</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=708>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Quanta: a toolkit for high performance data delivery over photonic networks</paper_heading>
	<authors>Eric He, Javid Alimohideen, Josh Eliason, Naveen K. Krishnaprasad, ... Thomas A. DeFanti</authors>
	<abstract>Quanta is a cross-platform adaptive networking toolkit for supporting the data delivery requirements of interactive and bandwidth intensive applications, such as Amplified Collaboration Environments. One of the unique goals of Quanta is to provide applications with the ability to provision optical pathways (commonly referred to as Lambdas) in dedicated photonic networks. This paper will introduce Quanta’s architecture and capabilities, with particular attention given to its aggressive and predictable high performance data transport scheme called Reliable Blast UDP (RBUDP). We provide an analytical model to predict RBUDP’s performance and compare the results of our model against experimental results performed over a high speed wide-area network.</abstract>
	<keywords>Quanta; Photonic network; Reliable Blast UDP; High performance data transfer; Light path provision</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=709>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>TeraScope: distributed visual data mining of terascale data sets over photonic networks</paper_heading>
	<authors>Chong Zhang, Jason Leigh, Thomas A. DeFanti, Marco Mazzucco, Robert Grossman</authors>
	<abstract>TeraScope is a framework and a suite of tools for interactively browsing and visualizing large terascale data sets. Unique to TeraScope is its utilization of the Optiputer paradigm to treat distributed computer clusters as a single giant computer, where the dedicated optical networks that connect the clusters serve as the computer’s system bus. TeraScope explores one aspect of the Optiputer architecture by employing a distributed pool of memory, called LambdaRAM, that serves as a massive data cache for supporting parallel data mining and visualization algorithms.</abstract>
	<keywords>TeraScope; Optiputer; Lambda; RAM</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=710>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experimental studies using photonic data services at IGrid 2002</paper_heading>
	<authors>Robert L. Grossman, Yunhong Gu, Don Hamelburg, Dave Hanley, ... Jeremy Weinberger</authors>
	<abstract>We describe an architecture for remote and distributed data intensive applications that integrates optical path services, network protocol services for high performance data transport, and data services for remote data analysis and distributed data mining. We also present experimental evidence using geoscience data that this architecture scales to long haul, high performance networks.</abstract>
	<keywords>High performance networks; Distributed data mining; Data webs; TCP; UDP</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=711>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>TeraVision: a high resolution graphics streaming device for amplified collaboration environments</paper_heading>
	<authors>Rajvikram Singh, Jason Leigh, Thomas A. DeFanti, Fotis Karayannis</authors>
	<abstract>One of the common problems faced in amplified collaboration environments (ACEs), such as the Continuum, is termed the ‘Display docking’ or ‘Display Pushing’ problem where the visualization or the presentation generated on one or more computers, has to be distributed to remote sites for viewing by a group of collaborators. A typical image source in such a case could be computers ranging from laptops showing presentations, to compute clusters number crunching terabytes of data and rendering high resolution visualizations. In this paper, we present a platform independent solution which is capable of transmitting multiple high resolution video streams from such video sources to one or more destinations. The unique capability of this concept is that it is a completely hardware oriented solution, where no special software/hardware has to be installed on the source or destination machines to enable them to transmit their video. These multiple streams can either be independent of each other or they might be component streams of a video system, such as a tiled display or stereoscopic display. We shall also present results with testing on high speed dedicated long haul networks, and local area gigabit LANs with different Layer 4 protocols.</abstract>
	<keywords>TeraVision; Continuum; Amplified collaboration environment</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=712>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Kites flying in and out of space—distributed physically based art on the grid</paper_heading>
	<authors>Shalini Venkataraman, Jason Leigh, Tom Coffin</authors>
	<abstract>In this paper, we describe the design and implementation of a Virtual Reality (VR) art piece—“Kites flying in and out of space” that was inspired by the kite-like art forms of French artist, Jackie Matisse. We use a physically based animation method known as the mass-spring model to realistically simulate the movement of these virtual kitetail forms in the CAVE VR theatre. In this immersive environment, the user can interact with these “virtual” kites by moving them, changing their imagery or adding a wind force. However, the real-time requirements imposed by immersive environments and the computational complexity in calculating these forms inhibit the number of kites we can “fly”. To address this limitation, we show how the use of distributed computing resources across the GRID can provide a scalable solution. Serendipitously, we also discovered that the movement of these virtual art forms became visual metaphors for the network performance and parameters.</abstract>
	<keywords>VR; GRID art; Kitetails; Physically-based animation</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=713>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Grid-enabled particle physics event analysis: experiences using a 10 Gb, high-latency network for a high-energy physics application</paper_heading>
	<authors>W. Allcock, J. Bresnahan, J. Bunn, S. Hegde, ... L. Winkler</authors>
	<abstract>This paper examines issues encountered attempting to exploit a high-bandwidth, high-latency link in support of a high-energy physics (HEP) analysis application. The primary issue was that the TCP additive increase/multiplicative decrease (AIMD) algorithm is not suitable for “long fat networks”. While this is a known problem, the magnitude of the impact on application performance was much greater than anticipated. We were able to overcome much of the impact, by altering the AIMD coefficients. Such an approach, of course, is non-TCP compliant, and there was insufficient time to test the network friendliness of these modifications.</abstract>
	<keywords>Networks; DataGrid; Congestion avoidance; 10 GigE; Web100</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=714>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The rationale of the current optical networking initiatives</paper_heading>
	<authors>Cees de Laat, Erik Radius, Steven Wallace</authors>
	<abstract>The future of networking is to move to an entirely optical infrastructure. Several leading National Research Networking organizations are creating test-beds to pilot the new paradigm. This paper explores some thoughts about the different usage models of optical networks. Different classes of users are identified. The services, required by the Internet traffic from those different classes of users, are analysed and a differentiated Internet architecture is proposed to minimize the cost per transported packet for the whole architecture.</abstract>
	<keywords>Lambda networking; Optical networking; Switching; Routing; DWDM; High performance; High throughput; Bandwidth on demand</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=715>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Authorization of a QoS path based on generic AAA</paper_heading>
	<authors>Leon Gommans, Cees de Laat, Bas van Oudenaarde, Arie Taal</authors>
	<abstract>For data intensive Grid applications, such as shown at iGrid2002, users may require short-lived guaranteed high bandwidth connections. These types of connections, providing a certain Quality of Service (QoS) will need to be authorized and provisioned, often through multiple administrative domains. We present a case study of a Bandwidth on Demand service that provides a QoS path based on Generic Authorization, Authentication, Accounting, that represents a first step forward towards a multi-domain solution.</abstract>
	<keywords>QoS; Bandwidth on demand; Multi-domain; Authentication; Authorization; Accounting</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=716>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Microscopic examination of TCP flows over transatlantic links</paper_heading>
	<authors>Antony Antony, Johan Blom, Cees de Laat, Jason Lee, Wim Sjouw</authors>
	<abstract>Much of the recent research and development in the area of high-speed TCP is focused on the steady state behavior of TCP flows. However, our experience with the first research only transatlantic 2.5 Gbps Lambda link clearly demonstrates the need to focus on the initial stages of TCP. The work we present here examines the behavior of TCP flows at microscopic level over high-bandwidth long delay networks. This examination has led us to study the influence of the minute properties of the underlying network on bursty protocols such as TCP at these very high speeds combined with high latency. In this paper we briefly describe the requirements for such an extreme network environment to support high-speed TCP flows. We also present results collected using transatlantic links at iGrid2002 where we tuned various host parameters and used modified TCP stacks.</abstract>
	<keywords>HSTCP; High-speed long-latency TCP; iGrid2002; Long fat networks</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=717>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Global Telescience featuring IPv6 at iGrid2002</paper_heading>
	<authors>David Lee, Abel W. Lin, Tom Hutton, Toyokazu Akiyama, ... Mark H. Ellisman</authors>
	<abstract>Electron tomography is a powerful technique for deriving 3D structural information from biological specimens. As advanced instrumentation, networking, and grid computing are applied to electron tomography and biological sciences in general, much work is needed to integrate and coordinate these advanced technologies in a transparent way to deliver them to the end user. The Telescience Portal (http://gridport.npaci.edu/Telescience) is a web-based solution for end-to-end electron tomography that centralizes applications and seamlessly interfaces with the grid to accelerate the throughput of data results. In this paper we will describe the architecture and design of the Telescience Portal in the context of our experiences leading up to and including the iGrid2002 workshop. We will examine the lessons learned in developing the production Telescience environment, leveraging a successful international collaboration with groups in Japan and Taiwan, building end-to-end native IPv6 networks across continents, and examining IPv6 enabled mechanisms for transferring large data from two unique, remotely accessible high performance scientific instruments. Traditional computer science communities develop next generation technologies. Applications like Telescience drive these next generation technologies into production quality applications for everyday research needs.</abstract>
	<keywords>Telescience; Grid; IPv6</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=718>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Ygdrasil—a framework for composing shared virtual worlds</paper_heading>
	<authors>Dave Pape, Josephine Anstey, Margaret Dolinsky, Edward J. Dambik</authors>
	<abstract>Ygdrasil is a programming framework for creating networked, multi-user virtual worlds, especially interactive artistic worlds. It provides a shared scene graph, a plug-in system for adding new behaviors, and a high-level script interface for composing these worlds. We describe the architecture of Ygdrasil, and its use in creating two applications that were demonstrated at the iGrid 2002 workshop.</abstract>
	<keywords>Virtual reality; Shared virtual environments; Computer art</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=719>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>ATLAS Canada lightpath data transfer trial</paper_heading>
	<authors>Corrie Kost, Steven McDonald, Bryan Caron, Wade Hong</authors>
	<abstract>Emerging grids will play a significant role in the computational, data, storage, and network requirements of High Energy Physics experiments coming online in the next few years. One such requirement, the bulk transfer of data over advanced high speed optical networks is necessary as such experiments are highly distributed with resources and participants from research laboratories and institutions spanning the globe. This trial at the iGrid 2002 conference attempts to stress the feasibility of high speed bulk data transfer over an end-to-end lightpath, a dedicated point-to-point optical link. Specifically, the objective was to transfer 1 TB of Monte Carlo data from TRIUMF in Vancouver, Canada, to CERN in Geneva, Switzerland. A rate equivalent to transferring a full CD of data every 8 s was achieved.</abstract>
	<keywords>Grid; End-to-end lightpath; 10 Gb Ethernet; Ethernet channel bonding</keywords>
	<publication_month_year>2003-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 6</volumes_issues>
</paper>
<paper no=720>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optimal Quality of Service routing and admission control using the Utility Model</paper_heading>
	<authors>Shahadat Khan, Kin F. Li, Eric G. Manning, Robert Watson, G.C. Shoja</authors>
	<abstract>Real-time multimedia applications such as video and audio streaming, video conferencing and online collaboration are becoming increasingly popular. In order to guarantee effective support of many of these applications, the Internet must provide absolute Quality of Service (QoS) guarantees for such parameters as network bandwidth and end-to-end delay by incorporating session admission control, session routing, and resource reservation. In this article, we present the Utility Model for optimal routing and admission control (RAC) of a data network supporting sessions requiring QoS guarantees. This model maps the optimal RAC problem to a Multiple-choice Multi-dimension 0-1 Knapsack Problem (MMKP), a variation of the classical 0-1 Knapsack Problem. We also present a design for an optimal RAC system based on the Utility Model. This method uses the MMKP formulation of the optimal RAC problem to provide an integrated solution of the session routing, admission control and resource allocation problems. Our experiments show that optimal RAC using the MMKP formulation (OptRAC) would provide 7–16% more revenue than the revenue provided by a traditional RAC system (TradRAC).</abstract>
	<keywords>Quality of Service; QoS routing; Admission control; Internet QoS; Internet pricing</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=721>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance evaluation of load balancing strategies for approximate string matching application on an MPI cluster of heterogeneous workstations</paper_heading>
	<authors>Panagiotis D. Michailidis, Konstantinos G. Margaritis</authors>
	<abstract>In this paper, we present three parallel approximate string matching methods on a parallel architecture with heterogeneous workstations to gain supercomputer power at low cost. The first method is the static master–worker with uniform distribution strategy, the second one is the dynamic master–worker with allocation of subtexts and the third one is the dynamic master–worker with allocation of text pointers. Further, we propose a hybrid parallel method that combines the advantages of static and dynamic parallel methods in order to reduce the load imbalance and communication overhead. This hybrid method is based on the following optimal distribution strategy: the text collection is distributed proportional to workstation’s speed. We evaluated and compared the performance of the four methods with clusters one, two, four, six and eight heterogeneous workstations. The experimental results demonstrate that dynamic allocation of text pointers and hybrid methods achieve better performance than the two original ones. We also present an analytical performance model for the four methods that confirms the actual behaviour of the experimental results.</abstract>
	<keywords>Approximate string matching; Cluster of heterogeneous workstations; Message passing interface; Load balancing strategies; Performance prediction</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=722>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and implementation of a fine-grained menu control processor for web-based information systems</paper_heading>
	<authors>Eric Jui-Lin Lu, Rai-Fu Chen</authors>
	<abstract>Web-based information systems (WISs) have become mainstream systems on the Internet and are widely deployed by enterprises worldwide. Although it is extremely important to secure access to WISs, the development of access control for WISs is still in its infancy stage. In addition, existing access control models for web applications are not suitable for WISs. In this paper, we proposed an access control model, called X-Menu, for WISs. Also, a prototype had been designed and implemented. The proposed model provides fine-grained control up to the element level of documents and is flexible and secured. The maintenance cost of the proposed model is low, and the proposed model can prevent users from performing any unauthorized task.</abstract>
	<keywords>WIS; Access control; XML; Fine-grained control</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=723>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Continuation of eigendecompositions</paper_heading>
	<authors>L. Dieci, A. Papini</authors>
	<abstract>In this work, we consider continuation of block eigendecompositions of a matrix valued function. We give new theoretical results on reduction to Hessenberg and bidiagonal forms, introduce and implement algorithms to continue eigendecompositions, and give numerical examples.</abstract>
	<keywords>Smooth eigendecompositions; Schur and Hessenberg forms; Riccati equations; Continuation</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=724>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On robust matrix completion with prescribed eigenvalues</paper_heading>
	<authors>M.T. Chu, F. Diele, I. Sgura</authors>
	<abstract>Matrix completion with prescribed eigenvalues is a special kind of inverse eigenvalue problems. Thus far, only a handful of specific cases concerning its existence and construction have been studied in the literature. The general problem where the prescribed entries are at arbitrary locations with arbitrary cardinalities proves to be challenging both theoretically and computationally. This paper investigates some continuation techniques by recasting the completion problem as an optimization of the distance between the isospectral matrices with the prescribed eigenvalues and the affine matrices with the prescribed entries. The approach not only offers an avenue to solving the completion problem in its most general setting but also makes it possible to seek a robust solution that is least sensitive to perturbation.</abstract>
	<keywords>Matrix completionInverse eigenvalue problem; Robust solution; Isospectral matrices; Continuation method; Steepest descent gradient flow</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=725>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Differential approaches for computing Euclidean diagonal norm balanced realizations in control theory</paper_heading>
	<authors>N. Del Buono, L. Lopez</authors>
	<abstract>In this paper we propose dynamical systems for computing diagonal balanced realizations arising in control theory, particularly, we deal with Euclidean diagonal norm balanced realizations. The limiting solution of a differential flow appears to be a possible way of finding such realizations since no direct algebraic algorithm is known.</abstract>
	<keywords>Balanced realization; Gradient flow; Isodynamical methods</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=726>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Isospectral-like flows and eigenvalue problem</paper_heading>
	<authors>Maria Przybylska</authors>
	<abstract>We show that the Lax equation L̇=[N,L] applied for calculating the eigenvalues of matrices is only one in a whole family of matrix differential equations related to eigenvalue problem. All these equations are built by means of various representations ρ of a Lie algebra g on Kn×n. A general connection between matrix differential equation L̇=ρN(L) and FG eigenvalue algorithm is shown. Remarks concerning practical applications of this connection to matrix eigenvalues calculations are also presented.</abstract>
	<keywords>Isospectral flows; Lax equation; Matrix decompositions; Eigenvalue algorithms</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=727>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the ℓ1 Procrustes problem</paper_heading>
	<authors>Nickolay T. Trendafilov</authors>
	<abstract>In this paper, the well-known Procrustes problem is reconsidered. The usual least squares objective function is replaced by more robust one, based on a smooth approximation of the ℓ1 matrix norm. This smooth approximation to the ℓ1 Procrustes problem is solved making use of the projected gradient method. The Procrustes problem with partially specified target is treated and solved as well. Several classical numerical examples from factor analysis (well-known with their least squares Procrustes solutions) are solved with respect to the smooth approximation of the ℓ1 matrix norm goodness-of-fit measure.</abstract>
	<keywords>Fitting configurations; Constrained optimization; Dynamical system on manifolds; Descent flows; Optimality conditions; Factor-pattern and reference-structure; Partially specified target; MATLAB</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=728>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Exponential monotonicity of quadratic forms in ODEs and preserving methods</paper_heading>
	<authors>C. Elia, L. Lopez</authors>
	<abstract>In this paper we consider ODEs whose solutions satisfy exponential monotonic quadratic forms. We show that the quadratic preserving Gauss–Legendre–Runge–Kutta methods do not preserve this qualitative feature, while certain Lie group preserving methods, as Crouch–Grossman methods and methods based on projection techniques are suitable integrators for such a kind of differential problems. We also show that these differential problems may be solved via associated differential systems with quadratic invariants. Numerical tests are reported in order to confirm our theoretical results.</abstract>
	<keywords>Exponential monotonicity; ODEs; Crouch–Grossman methods</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=729>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On enumeration problems in Lie–Butcher theory</paper_heading>
	<authors>H. Munthe-Kaas, S. Krogstad</authors>
	<abstract>The algebraic structure underlying non-commutative Lie–Butcher series is the free Lie algebra over ordered trees. In this paper we present a characterization of this algebra in terms of balanced Lyndon words over a binary alphabet. This yields a systematic manner of enumerating terms in non-commutative Lie–Butcher series.</abstract>
	<keywords>Butcher theory; Trees; Enumeration; Lie group integrators; Runge–Kutta methods</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=730>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lie group foliations: dynamical systems and integrators</paper_heading>
	<authors>R.I. McLachlan, M. Perlmutter, G.R.W. Quispel</authors>
	<abstract>Foliate systems are those which preserve some (possibly singular) foliation of phase space, such as systems with integrals, systems with continuous symmetries, and skew product systems. We study numerical integrators which also preserve the foliation. The case in which the foliation is given by the orbits of an action of a Lie group has a particularly nice structure, which we study in detail, giving conditions under which all foliate vector fields can be written as the sum of a vector field tangent to the orbits and a vector field invariant under the group action. This allows the application of many techniques of geometric integration, including splitting methods and Lie group integrators.</abstract>
	<keywords>Lie group foliations; Dynamical systems; Numerical integrators</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=731>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A numerically reliable approach to robust pole assignment for descriptor systems</paper_heading>
	<authors>A. Varga</authors>
	<abstract>We propose a general, numerically reliable computational approach to solve the pole and eigenstructure assignment problem for descriptor systems. In the multi-input case, the proposed approach addresses the intrinsic non-uniqueness of the pole assignment problem solution by simultaneously minimizing the sensitivity of the feedback gain and of closed-loop eigenvalues. For this purpose, a minimum norm robust pole assignment problem is formulated and solved as an unconstrained minimization problem for a suitably chosen cost function. By using a generalized Sylvester equation-based parameterization, an explicit expression of the gradient of the cost function is derived to allow the efficient solution of the minimization problem by using powerful gradient search-based minimization techniques.</abstract>
	<keywords>Descriptor systems; Pole assignment; Numerical methods</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=732>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>New algorithms for the iterative refinement of estimates of invariant subspaces</paper_heading>
	<authors>K. Hüper, P. Van Dooren</authors>
	<abstract>New methods for refining estimates of invariant subspaces of a non-symmetric matrix are presented. We use global analysis to show local quadratic convergence of our method under mild conditions on the spectrum of the matrix.</abstract>
	<keywords>Non-symmetric eigenvalue problem; Invariant subspace; Riccati equations; Sylvester equations; Global analysis; Local quadratic convergence</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=733>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Structure preservation: a challenge in computational control</paper_heading>
	<authors>Peter Benner, Daniel Kressner, Volker Mehrmann</authors>
	<abstract>Current and future directions in the development of numerical methods and numerical software for control problems are discussed. Major challenges include the demand for higher accuracy, robustness of the method with respect to uncertainties in the data or the model, and the need for methods to solve large scale problems. To address these demands it is essential to preserve any underlying physical structure of the problem. At the same time, to obtain the required accuracy it is necessary to avoid all inversions or unnecessary matrix products. We will demonstrate how these demands can be met to a great extent for some important tasks in control, the linear-quadratic optimal control problem for first and second order systems as well as stability radius and H∞ norm computations.</abstract>
	<keywords>Linear-quadratic optimal control; H∞ control; Hamiltonian eigenproblem; Computational methods; Structure-preserving methods</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=734>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Krylov subspace methods for large-scale matrix problems in control</paper_heading>
	<authors>Biswa Nath Datta</authors>
	<abstract>This paper presents a brief but state-of-the-art survey of some of existing Krylov subspace methods for large-scale matrix problems in control. Based on the discussions and observations, some research problems are suggested.</abstract>
	<keywords>Krylov subspace; Large-scale matrix problems; SISO</keywords>
	<publication_month_year>2003-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 7</volumes_issues>
</paper>
<paper no=735>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational science and engineering: a new master’s program at the Technische Universität München</paper_heading>
	<authors>Hans-Joachim Bungartz</authors>
	<abstract>In the winter semester of 2001/2002, the Technische Universität München (TUM) started its new master’s program computational science and engineering (CSE) as a joint initiative of seven faculties. It is the objective of this contribution to outline the underlying ideas and concepts and their curricular implementation as well as to emphasize some features that are probably non-standard in other comparable programs.</abstract>
	<keywords>International master’s program; Computational science and engineering; Curriculum</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=736>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Swedish National Graduate School in Scientific Computing (NGSSC)</paper_heading>
	<authors>Sverker Holmgren, Anders Ynnerman</authors>
	<abstract>The Swedish National Graduate School in Scientific Computing (NGSSC) was founded in 1996, and has so far admitted 75 graduate students in five nationally announced admission rounds. The aim of NGSSC is to produce PhDs with skills in strategically important disciplines together with a broad knowledge of numerical and computational techniques, and to spread the use of computations into new areas of research. NGSSC consists of both a geographical and a disciplinary network and focuses on providing core knowledge on advanced computational techniques to graduate students in application fields. This presents some special challenges to the program, such as developing its own identity and dealing with the very different backgrounds of the students. The NGSSC activities focus on a package of specially developed courses, tailored for student groups with very mixed backgrounds. The courses are given in intensive format, and are compulsory for the students participating in the program.</abstract>
	<keywords>NGSSC; Graduate school; Computational techniques; Geographic and disciplinary network</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=737>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The computational science major at SUNY Brockport</paper_heading>
	<authors>Leigh J. Little</authors>
	<abstract>The field of computational science is a recent addition to academic study. While the content of such an education is generally agreed upon, effective methods for imparting this knowledge are still being investigated. This paper describes the current state of the computational science degree programs at SUNY Brockport and the successes that have been obtained. Issues relating to the implementation of such programs in the context of a small, liberal arts college are also discussed.</abstract>
	<keywords>Computational science; Education; Undergraduate; Graduate</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=738>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The doctoral program in Computing and Information Sciences and Engineering of the University of Puerto Rico</paper_heading>
	<authors>Jaime Seguel, Domingo Rodrı́guez</authors>
	<abstract>The doctoral program in Computing and Information Sciences and Engineering (CISE) of the University of Puerto Rico is a research and education program intended to form investigators and build the academic field of computing and informatics with a comprehensive, multidisciplinary and integrative view. The program is based on the premise that the impact of computing and informatics on science and engineering goes beyond the mere empowering of data analysis and simulation capabilities. Indeed, computing and informatics offer a new approach to discovery and engineering design, which we call the informatics approach. We believe that the philosophy and methodology of data representation and processing exerts a decisive influence on what can be achieved in science and engineering, and in the scope and quality of results. The doctoral program in CISE is intended to catalyze this approach as an academic discipline. This article describes the main features, driving forces, and projections of this doctoral program.</abstract>
	<keywords>Computing; Informatics; Computational Sciences and Engineering</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=739>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The synergistic integration of mathematics, software engineering, and user-centred design: exploring new trends in education</paper_heading>
	<authors>C. Angelov, R.V.N. Melnik, J. Buur</authors>
	<abstract>There is an increasing recognition in the society that interdisciplinary challenges must be part of new educational practices. In this paper, we describe the key curriculum activities at the University of Southern Denmark that combine mathematical modelling, software engineering, and user-centred design courses. These three disciplines represent a core of our graduate program, aiming at educating the professionals that will be capable of not only using but also further developing new technologies, and therefore, will be capable of fostering further the progress in computational science and engineering. Finally, we show how the learning environment, with emphases on broadening the student experience by industrial links, affects the student career aspiration.</abstract>
	<keywords>Education; Computational science and engineering; Pervasive computing and mechatronics</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=740>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel programming in computational science: an introductory practical training course for computer science undergraduates at Aachen University</paper_heading>
	<authors>H.M. Bücker, B. Lang, C.H. Bischof</authors>
	<abstract>Parallel programming of high-performance computers has emerged as a key technology for the numerical solution of large-scale problems arising in computational science and engineering (CSE). The authors believe that principles and techniques of parallel programming are among the essential ingredients of any CSE as well as computer science curriculum. Today, opinions on the role and importance of parallel programming are diverse. Rather than seeing it as a marginal beneficial skill optionally taught at the graduate level, we understand parallel programming as crucial basic skill that should be taught as an integral part of the undergraduate computer science curriculum. A practical training course developed for computer science undergraduates at Aachen University is described. Its goal is to introduce young computer science students to different parallel programming paradigms for shared and distributed memory computers as well as to give a first exposition to the field of computational science by simple, yet carefully chosen sample problems.</abstract>
	<keywords>Parallel programming; Java; Computational science and engineering; Education</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=741>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Floating point arithmetic teaching for computational science</paper_heading>
	<authors>José-Jesús Fernández, Inmaculada Garcı́a, Ester M. Garzón</authors>
	<abstract>Computational science is based upon numerical computing and, consequently, requires excellent knowledge of floating point computer arithmetic. In general, the average computational science student has a relatively limited understanding of the implications of floating point computation. This paper presents an initiative to teach floating point number representation and arithmetic in undergraduate courses in computational science. The approach is based on carefully designed practical exercises which highlight the main properties and computational issues of finite length number representation and arithmetic. In conjunction to the exercises, an auxiliary educational tool constitutes a valuable support for students to learn and understand the concepts involved. Simpler formats are used as an introduction to the IEEE 754 standard, with the aim of presenting the fundamentals of the floating point computation and emphasizing its limitations. This approach could be included in courses related to computer organization, programming, discrete mathematics, numerical methods or scientific computing in computational science curricula.</abstract>
	<keywords>Computer arithmetic; Floating point computation; Computational science education</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=742>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Local supercomputing training in the computational sciences using remote national centers</paper_heading>
	<authors>Floyd B. Hanson</authors>
	<abstract>Local training for high performance computing using remote national supercomputing centers is quite different from training at the centers themselves or using local machines. The local site computing and communication resources are a fraction of those available at the national centers. However, training at the local site has the potential of training more computational science and engineering students in high performance computing by including those who are unable to travel to the national center for training. The experience gained from supercomputing courses and workshops in the last 17 years at the University of Illinois at Chicago is described. These courses serve as the kernel in the program for training computational science and engineering students. Many training techniques are illustrated, such as key local user’s guides and starter problems that would be portable to other local sites. Training techniques are continually evolving to keep up with rapid changes in supercomputing. An essential feature of this program is the use of real supercomputer time on several supercomputer platforms at national centers with emphasis in solving large scale problems.</abstract>
	<keywords>High performance computer training; Supercomputing education; Computational sciences and engineering</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=743>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational science in high schools: defining curricula and environments</paper_heading>
	<authors>Paolo Mori, Laura Ricci</authors>
	<abstract>The paper presents a new approach for the introduction of computational science into high level school curricula. It also discusses a set of real life problems that are appropriate for these curricula because they can be described through simple models. The computer based simulation of these systems require an ad hoc environment, including a programming language, suitable for this target age. The paper proposes a new environment, the ORESPICS environment, including a new programming language. The sequential part of the language integrates the classical imperative constructs with a simple set of graphical primitives, mostly taken from the LOGO language. The concurrent part of the language is based on the message passing paradigm. The solutions of some classical problems through ORESPICS are shown.</abstract>
	<keywords>Computational science; Parallel programming; Didactic language; Message passing</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=744>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The role of modeling in computational science education</paper_heading>
	<authors>W. Wiechert</authors>
	<abstract>Modeling and simulation skills are two core competences of computational science and thus should be a central part of any curriculum. While there is a well-founded methodology for the design of simulation algorithms today the teaching of modeling skills carries some intrinsic problems. The reason is that modeling is still partly an art and partly a science. As an important consequence for university education, the concepts for teaching modeling must be quite different from those for teaching simulation algorithms. Experiences made with the courses on ‘Modeling and Simulation’ at the University of Siegen are summarized and some general concepts for the teaching of modeling skills are presented. In particular, three practical approaches to modeling education are discussed with several examples.</abstract>
	<keywords>Computational science education; Modeling and simulation; Modeling education</keywords>
	<publication_month_year>2003-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 19, Issue 8</volumes_issues>
</paper>
<paper no=745>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SGrid: a service-oriented model for the Semantic Grid</paper_heading>
	<authors>M. Li, P. van Santen, D.W. Walker, O.F. Rana, M.A. Baker</authors>
	<abstract>This paper presents SGrid, a service-oriented model for the Semantic Grid. Each Grid service in SGrid is a Web service with certain domain knowledge. A Web services oriented wrapper generator has been implemented to automatically wrap legacy codes as Grid services exposed as Web services. Each wrapped Grid service is supplemented with domain ontology and registered with a Semantic Grid Service Ontology Repository using a Semantic Services Register. Using the wrapper generator, a finite element based computational fluid dynamics (CFDs) code has been wrapped as a Grid service, which can be published, discovered and reused in SGrid.</abstract>
	<keywords>Semantic Grid; Web services; Service-oriented model; Wrapper generator; Domain ontology</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=746>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A predicate-ordered logic for knowledge representation on the web</paper_heading>
	<authors>Yuzhong Qu</authors>
	<abstract>Web ontology languages play an important role in the Semantic Web, and its approach to knowledge representation on the web can be adopted to represent knowledge in the knowledge grid model and system. However, the logic foundation of web ontology languages needs to be further explored. This paper presents a predicate-ordered logic, called Predicate-Ordered Sort-Ordered Logic (POSOL), as a rigid logic framework for knowledge representation on the web. Based on an order-sorted logic for knowledge representation, it introduces a partially ordering on predicates to model the sub-property construct in web ontology languages such as RDFS and OWL. Some structures and properties of the POSOL signature and the POSOL knowledge base are studied. The bridging between the POSOL and the order-sorted logic is given. In addition, it briefly discusses the mappings from RDFS and OWL into POSOL, as well as some usage of POSOL within the knowledge grid model.</abstract>
	<keywords>Knowledge grid; Semantic Web; Knowledge representation; Order-sorted logic; Predicate-ordered</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=747>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Extending RDF in distributed knowledge-intensive applications</paper_heading>
	<authors>Jun Shen, Yun Yang</authors>
	<abstract>The resource description framework (RDF) has become a formal language tool to specify the semantics of distributed systems, such as web services nowadays. In fact, it can also be extended to describe entities and relationships within specific application environments to support knowledge sharing and ontology construction. This paper presents two case studies on a network management knowledge model and a distributed workflow system process ontology. With practical experiences, the authors suggest how RDF can be applied innovatively and effectively to reengineer data integration solutions in different novel knowledge-intensive areas, which, in the past, were built upon traditional modelling languages, such as XML.</abstract>
	<keywords>Resource description framework; Network management; Knowledge sharing; Distributed workflow systems; Ontology construction</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=748>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Semantic profile-based document logistics for cooperative research</paper_heading>
	<authors>Hai Zhuge, Yanyan Li</authors>
	<abstract>This paper proposes a document logistics approach for cooperative research based on the Web and Knowledge Grid. The approach realizes effective research document collection, organization and provision as well as knowledge sharing by incorporating the following functions: construction of semantic profiles representing interests, continuous discovery and collection of potentially relevant documents, synthesis of evaluation feedbacks, and support of flexible management operations and document recommendation services. The prototype has been implemented and is available for use online. Experiments show that the proposed approach is feasible and effective.</abstract>
	<keywords>Feedback synthesis; Knowledge Grid; Logistics; ProfileTeamwork</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=749>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge logistics in information grid environment</paper_heading>
	<authors>Alexander Smirnov, Mikhail Pashkin, Nikolai Chilov, Tatiana Levashova</authors>
	<abstract>Rapidity of decision making process is an important factor for different areas of human life (business, healthcare, industry, military applications, etc.). Since responsible persons make decisions using available knowledge a delivery of necessary and timely information for knowledge management systems is important. Knowledge logistics is a new direction of knowledge management addressing this issue. It provides a set of activities for knowledge search, acquisition and integration from distributed sources located in information grid environment. The paper proposes a developed Knowledge Source Network approach (KSNet-approach) to knowledge logistics and its multi-agent architecture, and describes a research prototype of the system “KSNet” based on this approach.</abstract>
	<keywords>Knowledge logistics; Intelligent systems; Ontologies; Multi-agent systems; Information grid</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=750>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>HyO-XTM: a set of hyper-graph operations on XML Topic Map toward knowledge management</paper_heading>
	<authors>Ying Dong, Mingshu Li</authors>
	<abstract>Knowledge management is a critical issue for the next-generation web application, because the next-generation web is becoming a semantic web, a knowledge-intensive network. XML Topic Map (XTM), a new standard, is appearing in this field as one of the structures for the semantic web. It organizes information in a way that can be optimized for navigation. In this paper, a new set of hyper-graph operations on XTM (HyO-XTM) is proposed to manage the distributed knowledge resources. HyO-XTM is based on the XTM hyper-graph model. It is well applied upon XTM to simplify the workload of knowledge management. The application of the XTM hyper-graph operations is demonstrated by the knowledge management system of a consulting firm. HyO-XTM shows the potential to lead the knowledge management to the next-generation web.</abstract>
	<keywords>Knowledge management; Topic maps; XML Topic Map; Hyper-graph</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=751>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A fuzzy collaborative assessment approach for Knowledge Grid</paper_heading>
	<authors>Hai Zhuge, Jie Liu</authors>
	<abstract>The assessment of Knowledge Grid is important to its design, development and maintenance. The difference between Knowledge Grid and website in structure and content determines that the current website assessment approaches are not suitable for assessing the Knowledge Grid. This paper proposes a fuzzy collaborative assessment approach combining the objective and subjective assessment strategies. The objective assessment strategy integrates the criteria used in website assessment and those related to knowledge organization and management. The subjective assessment strategy assesses the quality of knowledge service through the cooperation between experts and agents. The proposed approach also provides the optimum solution for improving the performance of the Knowledge Grid based on the assessment result and constraints. We demonstrate the proposed approach through a real application.</abstract>
	<keywords>Assessment; Fuzzy sets; Knowledge Grid; Knowledge space; Optimization</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=752>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Access control in semantic grid</paper_heading>
	<authors>Guanying Bu, Zhiwei Xu</authors>
	<abstract>The semantic grid (SG) aims at connecting information silos of web into a gigantic database, and supplying only the useful information to the users. Much advance has been made on information representation, extraction and sharing, but access control in SG is seldom discussed. This paper presents an access control model for semantic gird, which uses asynchronous automata to simulate grid nodes and grids. The classic Bell–LaPadula model is extended to deal with security properties of SG. Based on application scenarios of information sharing in China railways industry, this model is used to study convergence and consistence problems in SG. A prototype is implemented to verify our model.</abstract>
	<keywords>Semantic grid; Access control; Vega Grid; Asynchronous automaton</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=753>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Modeling adaptable multimedia and self-modifying protocol execution</paper_heading>
	<authors>Sheng-Uei Guan, Sok-Seng Lim</authors>
	<abstract>Over the years, researchers have tried to extend Petri net to model multimedia. The focus of the research flows from the synchronization of multimedia without user interactions (UIs), to interactions in distributed environments. The issues in concern are the flexibility and compactness of the model when applied to model a system under change. Most existing models lack the power to model a system under change during execution. Petri net extensions have been developed to facilitate UIs in distributed environments, however, they require sophisticated pre-planning to lay out detailed schedule changes. On the other hand, there has been active research on self-modifying protocols (SMPs) or adaptive protocols in recent years. Plenty of models have been developed to model communication protocol execution, to name a few, finite state machines, communicating finite state machines, Petri nets. However, there exist no suitable models to simulate protocols that are self-modifying or adaptive during execution. In this paper, we propose a Reconfigurable Petri net (RPN) for adaptable multimedia. A RPN comprises of a novel mechanism called modifier. This modifier can create a new change or delete an existing mechanism (e.g. arc, place, token, transition, etc.) of the net. In a way, modifier embraces controllability, reconfigurability, and programmability into the Petri net, and enhances the real-time adaptive modeling power. This development allows a RPN to have a greater modeling power over other extended Petri nets. The paper includes both the model and theory required to establish the technique’s validity. Examples are also shown how RPN can be used to model interactive multimedia, and simulate SMPs. A simulator has been developed using Visual C++ under Windows NT to show that RPN is feasible.</abstract>
	<keywords>Interactive multimedia; Self-modifying protocols; Reconfigurable Petri nets (RPN); Multimedia synchronization; Adaptive protocols</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=754>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A virtual server queueing network method for component based performance modelling of metacomputing</paper_heading>
	<authors>Yong Woo Lee</authors>
	<abstract>In this paper, we introduce a virtual server queueing network concept to evaluate the performance of metacomputing systems. It adds a virtual server concept to the existing queueing network theory. It enables us to effectively and accurately evaluate the performance of metacomputing systems. Component based software development now gains much interest and is especially regarded as very useful and necessary in the area of metacomputing. We find that the virtual server queueing network concept is well suited for a component based performance modelling and evaluation approach. In this paper, we have derived component based performance models of a metacomputing environment using the virtual server queueing network concept and validated them through measurement. With it, we quantitatively characterize the file access performance of caching in the metacomputing environment. We used a stochastic discrete event simulation as the main methodology to solve the models.</abstract>
	<keywords>Virtual server; Performance modelling; CBSD; Metacomputing; Caching</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=755>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Generalized data retrieval for pyramid-based periodic broadcasting of videos</paper_heading>
	<authors>Jin B. Kwon, Heon Y. Yeom</authors>
	<abstract>A true video-on-demand system allows users to view any video program, at any time, and perform any VCR functions, but its per-user video delivery cost is too expensive to have commercial use. Periodic broadcasting (PB), which is a near video-on-demand technique, broadcasts videos repeatedly over broadcast channels. In this way, PB can service an unlimited number of clients simultaneously with a bounded service latency. We propose a data retrieval scheme, consisting of buffer management and data placement, for PB servers. Unlike existing schemes devised for a specific PB technique, our scheme can be adopted by general PB. Furthermore, it is devised considering variations in disk load induced by VBR-encoded videos.</abstract>
	<keywords>Video-on-demand servers; Data placement; Buffer management; Video broadcasting</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=756>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Characterization and delivery of directly coupled causal messages in distributed systems</paper_heading>
	<authors>Yi Zeng, Wentong Cai, Stephen J Turner, Suiping Zhou, Bu-Sung Lee</authors>
	<abstract>Causal Order (CO) is an important but basic property of message-passing based distributed systems. Violations of causal order delivery normally lead to significant inconsistencies, particularly in distributed virtual environments. However, CO is only a partial order, which captures messages that have “cause-effect” relations. It does not order concurrent messages which occur independently at different processes. This paper proposes a new relation, i.e., direct-follow relation, to characterize directly coupled causal messages and a corresponding message delivery scheme, i.e., direct-follow order (DFO) delivery. The DFO delivery effectively enforces the delivery order of certain concurrent messages and thus can be used to further eliminate inconsistencies caused by these messages. It is a stronger ordering mechanism than CO. However, as shown in the paper, messages with the direct-follow relation cannot be always delivered according to the DFO, since the ordering of concurrent messages required by the DFO may result in conflicts in message dependency. This paper explains the motivation of defining the DFO, explores its properties and gives the conditions under which direct-follow messages can be delivered according to the DFO.</abstract>
	<keywords>Distributed systems; Distributed virtual environments; Happen-before relationship; Causal order delivery; Causal and total order delivery; Direct-follow relation; Direct-follow order delivery; Dependency set</keywords>
	<publication_month_year>2004-01-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 1</volumes_issues>
</paper>
<paper no=757>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Discord model for detecting unexpected demands in mobile networks</paper_heading>
	<authors>Vladimir V. Shakhov, Hyunseung Choo, Young-Cheol Bang</authors>
	<abstract>Predicting the number of calls in a cell in average must be an important issue in mobile computing. It is quite useful in performance modeling and usually assumed as a random Poisson value. For effective management of cellular networks, the average number of calls should be carefully traced and the unexpected changes in the numbers need to be promptly detected. In this paper, we propose an efficient algorithm detecting those changes in the behavior of users based on the technique proposed for point-of-change problem just in terms of the number of call arrivals. Computer simulation reveals the proposed method effectively detect the discord, and the developed model is very accurate that the difference is as small as mostly less than 1%.</abstract>
	<keywords>Discord model; Mobile Networks; Computer simulation</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=758>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance analysis of adaptive modulation and coding combined with transmit diversity in next generation mobile communication systems</paper_heading>
	<authors>Intae Hwang, Taewon Jang, Mingoo Kang, Sangmin No, ... Changeon Kang</authors>
	<abstract>In this paper, we combine Adaptive Modulation and Coding (AMC) with Transmit Diversity (TD) to improve the performance of AMC in CDMA systems. An improved SNR increases a probability of selecting Modulation and Coding Scheme (MCS) level that supports higher data transfer rate. Therefore, higher average throughput can be achieved. As an open-loop TD, Space–Time block coding based Transmit antenna Diversity (STTD) is applied to AMC. For a closed-loop TD, Selection Transmit Diversity (STD) is combined with AMC. The results show that higher throughput is achieved by AMC–TD schemes. Compared to the conventional AMC, AMC–STTD scheme shows about 250 kbps increase in throughput. In addition, AMC–STD with two transmit antennas achieves about 420 kbps throughput improvement over the conventional AMC at 9 dB SNR in flat Rayleigh fading channel.</abstract>
	<keywords>Adaptive Modulation and Coding; Transmit Diversity; Modulation and Coding Scheme</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=759>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Improving prediction level of prefetching for location-aware mobile information service</paper_heading>
	<authors>Seungmin Park, Daeyoung Kim, Gihwan Cho</authors>
	<abstract>An essential quality for future information services is their ability to evolve and adapt to deliver the information corresponding into the current situation of user and/or device in the face of constantly changing environment. This paper deals with the prefetching schemes to improve prediction level of for location-aware mobile information service. The prefetching aims to reduce the latency to get refreshed information appropriated to the current location. Thus, in order to effectively limit the prefetched information into the most likely future location context, the rectangle-shaped Prefetching Zone (PZ) is firstly defined with a user’s moving speed and direction. Then, a method is given to make use of the mobility reference count that shows how often the user has been visited to a given area, to further predict the prefetching candidates.</abstract>
	<keywords>Prefetching; Situation; Location-aware; Mobile information system</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=760>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A location management scheme to provide IP mobility over wireless ATM</paper_heading>
	<authors>Youngsong Mun, Youngyuk Kim</authors>
	<abstract>Increasing user demand for more bandwidth over wireless communications has motivated a new challenge to utilize benefits of wireless ATM technology. This paper focuses on how to provide IP services to a mobile user over wireless ATM networks. By enhancing the NHRP standard, we improve several shortcomings that are resulted from the naı̈ve combinations of mobile IP over ATM or IP over wireless ATM. This paper presents a mobile location management algorithm that is referred to as the mobile NHRP scheme. The performance analysis is given by formulating equations for costs. We estimate the total costs per move and per setup over ATM/IP overlaid networks.</abstract>
	<keywords>Location management; Mobility; IP; Wireless ATM</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=761>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A handover scheme in clustered cellular networks</paper_heading>
	<authors>Sang-Joon Park, Ji-Young Song, Jongchan Lee, Kwan-Joong Kim, Byung-Gi Kim</authors>
	<abstract>There are two types of handovers for mobile users in the clustered cellular networks: inter-cluster and intra-cluster handovers. An inter-cluster handover which occurs as a result of mobile user’s migration between clusters gives rise to additional handover delay, because more complex handover mechanism should be performed by mobile network. In this paper, we analyze multi-layered cluster architecture and we study the effect of cluster size on the inter-cluster handover performance. Then we propose a new hard handover scheme for inter-cluster handover. For an inter-cluster hard handover, additional delay is required for new bearer setup and reroute calculation. When a mobile terminal (MT) call crosses a cluster boundary, the proposed scheme performs some preparatory activities before actual handover is requested. It results in fast hard handover. We also analyze its performance and show that it reduces forced termination probability.</abstract>
	<keywords>Handover scheme; Clustered cellular networks; Mobile network</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=762>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance evaluation of scheduling schemes for NOW with heterogeneous computing power</paper_heading>
	<authors>Young-Chul Shim</authors>
	<abstract>In this paper we consider a NOW (Network of Workstations) in which workstations may have different computing power. We introduce 10 space-sharing scheduling policies and four hybrid scheduling policies applicable to the NOW with heterogeneous computing power. We compare the performance of these scheduling policies through simulation varying parameters such as process migration cost, slowdown of parallel processes due to sequential jobs, the parallel job inter-arrival time, the load imbalance in parallel processes, and the ratio of idle workstations.</abstract>
	<keywords>Network of Workstations; Scheduling; Speed heterogeneity</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=763>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance test and analysis for an adaptive load balancing mechanism on distributed server cluster systems</paper_heading>
	<authors>Eunmi Choi</authors>
	<abstract>As the next generation of Internet services requires more highly scalable and available server systems, the cost-effective cluster of a large number of distributed computers becomes a popular solution. In this paper, we investigate to design and develop a server load balancing mechanism on cluster architecture, called the ALBM cluster. In order to construct the more scalable and reliable Internet service system, the ALBM cluster system consists of independent but co-operable components. The ALBM cluster system supports adaptive load balancing among servers via its adaptive load balancer (ALB) component in Layer 4 level and Layer 7 level. The Management Station (M-Station) and the Administrator Console are in charge of cluster management, system configuration management, and performance counter management. The Node Service is a system-level agent that is deployed into a server node. The node management and cluster management are the major tasks of the agent. Beside, the ALBM cluster is a flexible open system whose features of functionality can change or be easily expanded without affecting the rest of the system. In this paper, we also present a set of our experimental results to compare the performance of the ALBM cluster with that of LVS scheduling cluster system. We compare performance results of the ALBM with RR, LC, and WLC scheduling algorithms of a LVS cluster in both of homogeneous and heterogeneous system environments.</abstract>
	<keywords>Cluster computing; Load balancing; LVS (Linux Virtual Server); Performance counter; Scalability; Availability</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=764>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance evaluation of a list scheduling algorithm in distributed memory multiprocessor systems</paper_heading>
	<authors>Gyung-Leen Park</authors>
	<abstract>Since it has been shown that the multiprocessor scheduling problem is NP-complete, research efforts have resulted in numerous scheduling algorithms based on heuristics. The need for measuring the effectiveness of the various task scheduling algorithms leads to a great demand on modeling and evaluation tools. This paper proposes to use the stochastic Petri net (SPN) model and its equivalent continuous time Markov chain (CTMC) as the tool for modeling and evaluation of task scheduling algorithms. The proposed approach is applied to DPS (decisive path scheduling) algorithm to investigate its effectiveness. The proposed approach is verified by comparing the result obtained using the SPN model with that obtained by actually running DPS algorithm. The performance comparison reveals that the proposed approach provides very accurate performance evaluation for the scheduling algorithm when the CCR (communication to computation ratio) value is small.</abstract>
	<keywords>Stochastic Petri net; Continuous time Markov chain; Communication to computation ratio</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=765>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An efficient event publish technique for real-time monitor on real-time grid computing</paper_heading>
	<authors>Eui-Nam Huh, Y Mun, Hyoung-Woo Park</authors>
	<abstract>Schedulability analysis (SA) approaches in large scaled distributed systems for real-time constraints are conventionally based on a priori information. Such systems use fixed execution time with constant workload, work well in many application domains, and allow pre-deployment guarantees of real-time performance such as rate monotonic analysis (RMA) [J. ACM 20 (1973) 46]. However, certain grid applications must operate in highly dynamic environments, thereby precluding accurate characterization of the applications’ workloads by static models. This considers issues that a new SA trigger needs to guarantee real-time performance during run-time on a dynamic environment of which applications experience large variations in the workload. This paper, especially, for the dynamic real-time grid computing environments, describes an efficient event (that are monitored in real-time) publish or policing technique which can trigger effective SA or reporting monitored-data appropriately, and uses a dynamic threshold which becomes sensitive when the quality of service (QoS) of the dynamic real-time application approaches to its deadline or constraint.</abstract>
	<keywords>Schedulability analysis; Real-time; Computing</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=766>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Inference of recommendation information on the internet using improved FAM</paper_heading>
	<authors>Won Kim, Il-Ju Ko, Jin-Sung Yoon, Gye-Young Kim</authors>
	<abstract>This paper proposes a collaborative filtering system using Improved Fuzzy Associative Memory (IFAM) which re-adjusts the connection weights between the nodes of FAM using error back propagation and simplifies the fuzzy rules. The proposed technique automatically recommends high-quality information to users with similar interests on arbitrarily narrow information domains. It asks a user to rate a gauge set of items. It then evaluates the user’s rates and suggests a recommendation set of items. The proposed system is implemented in a web server and tested its performance in the domain of retrieval of technical papers, especially in the field of information technologies. The experimental results show that it may provide reliable recommendations.</abstract>
	<keywords>Collaborative filtering; Preference; FAMFuzzy; Recommendation item</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=767>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Topological-order based dynamic polling scheme using biconnected component computation</paper_heading>
	<authors>Byungoh Ahn, Seongjin Ahn, Jinwook Chung</authors>
	<abstract>For network monitoring, this paper shows the dynamic polling scheme which controls over-head of polling traffic through an approach based on network topology information, unlike other existing schemes. This proposed scheme understands the connections between nodes through the network topology information of the observer domain, and by using this, decides polling order and controls polling rate, thus obtaining management information effectively and correctly. This method increased the rate of polling management compared to the former proposed schemes, and proved the efficiency of the proposed scheme by the comparison to the other schemes.</abstract>
	<keywords>Network management; Network monitoring; Polling; Biconnected component; Network topology</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=768>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Active security management based on Secure Zone Cooperation</paper_heading>
	<authors>Beom-Hwan Chang, Dong-Soo Kim, Hyun-Ku Kim, Jung-Chan Na, Tai-Myoung Chung</authors>
	<abstract>Due to its open protocol, the Internet has revolutionized computer networks, but this revolution brings new risks and threats. The best way to protect computer networks is to prevent attackers from intruding, using fast automated procedures. However, the current state of protection is insufficient, because providing for all attacks or preventing unknown types of attack is almost impossible, and the methods used are manual. We solve this problem by using active security management, based on sharing information about attacks and cooperation between organizations. Secure Zone Cooperation, a framework that establishes mutual collaboration and cooperation between trusted zones, can protect systems and networks from potential attacks. This framework can predict and respond to attacks by exchanging security information and cooperating with each zone. It is a dynamic, powerful security architecture that rapidly enables security policy to be updated and response modules to be deployed.</abstract>
	<keywords>Security management; Active network; Secure Zone Cooperation</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=769>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lip print recognition for security systems by multi-resolution architecture</paper_heading>
	<authors>Jin Ok Kim, Woongjae Lee, Jun Hwang, Kyong Seok Baik, Chin Hyun Chung</authors>
	<abstract>Biometric systems are forms of technology that use unique human physical characteristics to automatically identify individuals. They have sensors to pick up physical characteristics, convert them into digital patterns, and compare them with patterns stored for individual identification. However, lip print recognition has been less developed than the recognition of other human physical attributes such as the fingerprint, voice patterns, retinal blood vessel patterns, or the face. Lip print recognition by a CCD camera has the merit of being linked with other recognition systems such as the retinal/iris eye and the face. A new method using multi-resolution architecture is proposed to recognize a lip print from pattern kernels. A set of pattern kernels is a function of some local lip print masks. This function converts the information from a lip print into digital data. Recognition in the multi-resolution system is more reliable than recognition in the single-resolution system. Multi-resolution architecture allows us to reduce the false recognition rate from 15 to 4.7%. This paper shows that a lip print is sufficiently used by the measurements of biometric systems.</abstract>
	<keywords>Lip print recognition; Multi-resolution architecture; Pattern kernels; FAR; FRR</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=770>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An approach to intrusion tolerance for mission-critical services using adaptability and diverse replication</paper_heading>
	<authors>Byoung Joon Min, Joong Sup Choi</authors>
	<abstract>In many mission-critical applications, important services should be maintained properly under any circumstances including the presence of compromised components incurred by outside intentional attacks. In the paper, a two-level approach for the intrusion tolerance is presented. At the node level, by means of dynamic resource reallocation within a computing node, the critical services previously selected are to survive even after the occurrence of an attack. If it becomes impossible to find enough resources for the critical services within the node in spite of the adaptive actions taken at the node level, it moves to the system level. The system level mechanism is to deliver the intended services transparently to the clients even when a node fails. An architecture adopting diverse redundant computing nodes is proposed for that purpose. Through the experiments on a test-bed, especially, for web services, the approach turned out very effective to cope with not only denial of service attacks but also confidentiality and integrity attacks. Although the measurement of the timing overhead incurred by the approach represents 50% loss in performance, it seem possible to decrease the cost by optimizing the implementation.</abstract>
	<keywords>Intrusion; Mission-critical services; Adaptability</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=771>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and implementation of secure e-mail system using elliptic curve cryptosystem</paper_heading>
	<authors>Wongoo Lee, Jaekwang Lee</authors>
	<abstract>In this paper, we design and implement a secure mail system using elliptic curve cryptosystems (ECCs) to provide the highest cipher strength as well as security services such as message origin authentication, content integrity, content integrity, and content confidentiality. Of course, this is based on the difficulty of ECCs.</abstract>
	<keywords>Elliptic curve; Cryptosystem; Secure mail system</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=772>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and evaluation of a block encryption algorithm using dynamic-key mechanism</paper_heading>
	<authors>Chang-Doo Lee, Bong-Jun Choi, Kyoo-Seok Park</authors>
	<abstract>The existing block encryption algorithms have been designed for the encryption key value to be unchanged and applied to the round functions of each block, and enciphered. Therefore, it has such a weak point that the plain text or encryption key could be easily exposed by differential cryptanalysis or linear cryptanalysis, both are the most powerful methods for decoding block encryption of a round-repeating structure. In order to cope with this weak point, an encryption algorithm using a more efficient key should be designed. In this paper, a block encryption algorithm which is designed for each encryption key value to be applied to each round block with a different value is proposed. This algorithm needs a short processing time in encryption and decryption, has a high intensity, can apply to electronic commerce and various applications of data protection.</abstract>
	<keywords>Block encryption algorithm; Dynamic-key mechanism; Cryptanalysis; Codebook encryptor; Decryptor algorithm; Round key generator algorithm</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=773>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and implementation of MPλS network simulator</paper_heading>
	<authors>Bong-Hwan Lee, Il-Hong Jung, Chan-Hyun Youn</authors>
	<abstract>IP over WDM has been identified as one of the most attractive alternative for the new Internet due to its potential ability to meet rising demands of high bandwidth and low latency. In this paper, we present an MPλS simulator which enables to simulate various MPλS functions such as optical crossconnect (OXC), multi-wavelength links, routing and wavelength assignment (RWA), and MPλS signaling and control. The simulator is developed based on the ns-2, a widely used multi-protocol network simulator. This paper presents the architecture and details of the design of the simulator. The simulator is validated by running many simulations based on various scenarios and performance measures.</abstract>
	<keywords>MPλS; IP over WDM; OXC; Simulator; RWA</keywords>
	<publication_month_year>2004-02-16 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 2</volumes_issues>
</paper>
<paper no=774>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An irregular grid method for high-dimensional free-boundary problems in finance</paper_heading>
	<authors>Steffan Berridge, J.M. Schumacher</authors>
	<abstract>We propose and test a new method for pricing American options in a high-dimensional setting. The method is centred around the approximation of the associated variational inequality on an irregular grid. We approximate the effect of the partial differential operator on this grid by appealing to the stochastic differential equation (SDE) representation of the stock process and computing a root of the transition probability matrix of an approximating Markov chain. The results of numerical tests in five dimensions are promising.</abstract>
	<keywords>American options; High-dimensional problems; Free-boundary problems; Optimal stopping; Variational inequalities; Numerical methods</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=775>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A Jacobi–Davidson-type projection method for nonlinear eigenvalue problems</paper_heading>
	<authors>Timo Betcke, Heinrich Voss</authors>
	<abstract>This paper discusses a projection method for nonlinear eigenvalue problems. The subspace of approximants is constructed by a Jacobi–Davidson-type approach, and the arising eigenproblems of small dimension are solved by safeguarded iteration. The method is applied to a rational eigenvalue problem governing the vibrations of tube bundle immersed in an inviscid compressible fluid.</abstract>
	<keywords>Nonlinear eigenvalue problem; Jacobi–Davidson method; Projection method; Rayleigh functional; Minmax characterization</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=776>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Faster PDE-based simulations using robust composite linear solvers</paper_heading>
	<authors>S. Bhowmick, P. Raghavan, L. McInnes, B. Norris</authors>
	<abstract>Many large-scale scientific simulations require the solution of nonlinear partial differential equations (PDEs). The effective solution of such nonlinear PDEs depends to a large extent on efficient and robust sparse linear system solution. In this paper, we show how fast and reliable sparse linear solvers can be composed from several underlying linear solution methods. We present a combinatorial framework for developing optimal composite solvers using metrics such as the execution times and failure rates of base solution schemes. We demonstrate how such composites can be easily instantiated using advanced software environments. Our experiments indicate that overall simulation time can be reduced through highly reliable linear system solution using composite solvers.</abstract>
	<keywords>Large-scale PDE-based simulations; Composite methods; Multi-algorithms; Sparse linear solution; Newton–Krylov methods</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=777>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Restarted Simpler GMRES augmented with harmonic Ritz vectors</paper_heading>
	<authors>Ravindra Boojhawon, Muddun Bhuruth</authors>
	<abstract>We describe an augmented version of Simpler GMRES (SGMRES) for problems with small eigenvalues. The advantage is that the SGMRES variant requires a lesser amount of work than augmented GMRES. We show that the problem of finding harmonic Ritz vectors for augmenting the Krylov subspace can be easily reduced to an eigenvalue problem by computing recurrences for a transformation matrix. Numerical examples are described to illustrate the better performance of the augmented version of SGMRES.</abstract>
	<keywords>Krylov subspace methods; Augmented Simpler GMRES; Harmonic Ritz vectors; Sparse approximate inverse preconditioning</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=778>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On a new class of splitting type iterative methods</paper_heading>
	<authors>R. Čiegis</authors>
	<abstract>This paper deals with the stability analysis of a new class of iterative methods for elliptic problems. These schemes are based on a general splitting method, which decomposes a multidimensional parabolic problem into a system of 1D implicit problems. The solution of the given linear system of equations is approximated by p-component vector of approximations. Each splitted operator is applied to only one specific component of this vector. We use energy and spectral stability analysis and investigate the convergence rate of two iterative schemes. Finally, some results of numerical experiments are presented. The dependence of the convergence rate on the smoothness of the solution is investigated.</abstract>
	<keywords>Systems of linear equations; Iterative methods; Splitting methods; Convergence analysis</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=779>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the asymptotically stochastic computational modeling of microstructures</paper_heading>
	<authors>Dennis D. Cox, Petr Klouček, Daniel R. Reynolds</authors>
	<abstract>We consider a class of alloys and ceramics with equilibria described by non-attainable infima of non-quasiconvex variational integrals. Such situations frequently arise when atomic lattice structure plays an important role at the mesoscopic continuum level. We prove that standard variational approaches associated with gradient based relaxation of non-quasiconvex integrals in Banach or Hilbert spaces are not capable of generating relaxing sequences for problems with non-attainable structure. We introduce a variational principle suitable for the computational purposes of approaching non-attainable infima of variational integrals. We demonstrate that this principle is suitable for direct calculations of the Young Measures on a computational example in one dimension. The new variational principle provides the possibility to approximate crystalline microstructures using a Fokker–Planck equation at the meso-scale. We provide an example of such a construction.</abstract>
	<keywords>Microstructures; Crystalline materials; Weak white noise; Steepest descent algorithm; Weak convergence; Calculus of variations</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=780>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel and fully recursive multifrontal sparse Cholesky</paper_heading>
	<authors>Dror Irony, Gil Shklarski, Sivan Toledo</authors>
	<abstract>We describe the design, implementation, and performance of a new parallel sparse Cholesky factorization code. The code uses a multifrontal factorization strategy. Operations on small dense submatrices are performed using new dense matrix subroutines that are part of the code, although the code can also use the blas and lapack. The new code is recursive at both the sparse and the dense levels, it uses a novel recursive data layout for dense submatrices, and it is parallelized using Cilk, an extension of C specifically designed to parallelize recursive codes. We demonstrate that the new code performs well and scales well on SMPs. In particular, on up to 16 processors, the code outperforms two state-of-the-art message-passing codes. The scalability and high performance that the code achieves imply that recursive schedules, blocked data layouts, and dynamic scheduling are effective in the implementation of sparse factorization codes.</abstract>
	<keywords>Sparse Cholesky factorization; Parallel Cholesky factorization; Multifrontal factorizations; Cilk; Recursive factorizations; Block layouts; Recursive layouts</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=781>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A comparative study of Dirichlet and Neumann conditions for path planning through harmonic functions</paper_heading>
	<authors>Madhuri Karnik, Bhaskar Dasgupta, Vinayak Eswaran</authors>
	<abstract>Harmonic functions, by virtue of their extrema appearing only on the domain boundary, are known to have an advantage as a global potential function in the potential field-based approach for robot path planning. However, a wide range of possibilities exist for the global harmonic function for a particular situation, depending on the boundary conditions. This paper conducts a comparison of two major classes of boundary conditions, namely Dirichlet and Neumann conditions, and attempts to discover their relative merits and demerits. It is found that the Neumann conditions offer a surer and faster approach to the path-planning problem, though suffering from the disadvantage of occasional tendency of the planned path to graze along the domain boundary. This minor disadvantage, however, can be remedied by a two-stage strategy, in which the solution with the Neumann condition is used to generate the Dirichlet boundary conditions for the second stage of solution.</abstract>
	<keywords>Harmonic function; Motion planning; Global potential; Dirichlet condition; Neumann condition</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=782>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computational models for multi-scale coupled dynamic problems</paper_heading>
	<authors>R.V.N. Melnik, A.H. Roberts</authors>
	<abstract>In this paper we describe a systematic methodology for the construction of computational models for coupled multi-scale dynamic problems with a major focus given to a case study example related to the design of shape memory alloy actuators controlled by thermoelectric effect. From a mathematical point of view, the problem in hand is a coupled dynamic system of partial differential equations which is not amenable to analytical treatments and requires an efficient computational tool for its solution. The developed methodology is based on a combination of the analysis of such invariant sets that keep the essentials of the system dynamics and on the reduction procedures of the original model on such sets. A presented numerical example demonstrates the efficiency of the developed tool in describing phase transformations in actuators based on materials with shape memory effects.</abstract>
	<keywords>Coupled problems; Dynamics of phase transitions; Invariant sets; Multi-scale analysis</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=783>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Application of the Kurganov–Levy semi-discrete numerical scheme to hyperbolic problems with nonlinear source terms</paper_heading>
	<authors>R. Naidoo, S. Baboolal</authors>
	<abstract>In this paper are outlined the details required in adapting the third-order semi-discrete numerical scheme of Kurganov and Levy [SIAM J. Sci. Comput. 22 (2000) 1461] to handle hyperbolic systems which include source terms. The performance of the scheme is then assessed against a fully discrete scheme, as well as against reference solutions on problems such as shock propagation in a Broadwell gas and shocks in an Eulerian gas with heat transfer.</abstract>
	<keywords>Hyperbolic systems; Semi-discrete scheme; Shock capturing</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=784>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Solving unsymmetric sparse systems of linear equations with PARDISO</paper_heading>
	<authors>Olaf Schenk, Klaus Gärtner</authors>
	<abstract>Supernode partitioning for unsymmetric matrices together with complete block diagonal supernode pivoting and asynchronous computation can achieve high gigaflop rates for parallel sparse LU factorization on shared memory parallel computers. The progress in weighted graph matching algorithms helps to extend these concepts further and unsymmetric prepermutation of rows is used to place large matrix entries on the diagonal. Complete block diagonal supernode pivoting allows dynamical interchanges of columns and rows during the factorization process. The level-3 BLAS efficiency is retained and an advanced two-level left–right looking scheduling scheme results in good speedup on SMP machines. These algorithms have been integrated into the recent unsymmetric version of the PARDISO solver. Experiments demonstrate that a wide set of unsymmetric linear systems can be solved and high performance is consistently achieved for large sparse unsymmetric matrices from real world applications.</abstract>
	<keywords>Computational sciences; Numerical linear algebra; Direct solver; Unsymmetric linear systems</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=785>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using the parallel algebraic recursive multilevel solver in modern physical applications</paper_heading>
	<authors>M. Sosonkina, Y. Saad, X. Cai</authors>
	<abstract>This paper discusses the application of a few parallel preconditioning techniques, which are collected in a recently developed suite of codes Parallel Algebraic Recursive Multilevel Solver (pARMS), to tackling large-scale sparse linear systems arising from real-life applications. In particular, we study the effect of different algorithmic variations and parameter choices on the overall performance of the distributed preconditioners in pARMS by means of numerical experiments related to a few realistic applications. These applications include magnetohydrodynamics, nonlinear acoustic field simulation, and tire design.</abstract>
	<keywords>Parallel algebraic multilevel preconditioning; Distributed sparse linear systems; Nonlinear acoustic field simulation; Magnetohydrodynamic flows; Tire design</keywords>
	<publication_month_year>2004-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 3</volumes_issues>
</paper>
<paper no=786>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards an efficient single system image cluster operating system</paper_heading>
	<authors>Christine Morin, Pascal Gallard, Renaud Lottiaux, Geoffroy Vallée</authors>
	<abstract>The lack of a single system image Operating System (OS) for clusters restricts their use for parallel processing. We propose an approach for building an efficient single system image cluster operating system. The proposed system implements distributed services performing global and dynamic resource management to offer high performance, high availability and ease of use and programming. The programming API of the OS running on each cluster node is kept unmodified but the high level OS services can take benefit of all cluster resources. Our approach has been validated by a prototype based on Linux. Our prototype comprises of a limited kernel patch and a set of modules extending the kernel to implement the cluster distributed services. Existing applications running on symmetric multiprocessors (SMP) on top of Linux can be executed on top of our cluster OS without modification.</abstract>
	<keywords>Cluster; Operating system; Distributed system; Single system image</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=787>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The development of an efficient checkpointing facility exploiting operating systems services of the GENESIS cluster operating system</paper_heading>
	<authors>J.T. Rough, A.M. Goscinski</authors>
	<abstract>Recent research efforts of parallel processing on non-dedicated clusters have focused on high execution performance, parallelism management, transparent access to resources, and making clusters easy to use. However, as a collection of independent computers used by multiple users, clusters are susceptible to failure. This paper shows the development of a coordinated checkpointing facility for the GENESIS cluster operating system. This facility was developed by exploiting existing operating system services. High performance and low overheads are achieved by allowing the processes of a parallel application to continue executing during the creation of checkpoints, while maintaining low demands on cluster resources by using coordinated checkpointing.</abstract>
	<keywords>Cluster computing; Fault tolerance; Checkpointing; Operating system services</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=788>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Atomic Writes for data integrity and consistency in shared storage devices for clusters</paper_heading>
	<authors>Michael Okun, Amnon Barak</authors>
	<abstract>With the recent trend to use storage area networks in distributed and cluster systems there is a need to improve the integrity and consistency guarantees of stored data in the presence of node or network failures. Currently, the main method for preserving data integrity and consistency is by logging techniques, e.g. journaling. This paper presents a new general method for preserving data consistency by Atomic multi-block Writes. The Atomic Writes method guarantees that either all the blocks in a write operation are written or no blocks are written at all. Its main advantage is that it does not require a recovery phase after a failure. The Atomic Writes method should be implemented in both the operating system and the storage system levels. It is easy to use and to implement. We present the method, its implementation and an example of its use for handling meta-data consistency in an existing file system that supports shared storage.</abstract>
	<keywords>System fault tolerance; Data consistency; Atomic transactions; Storage area networks; Cluster systems</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=789>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>TODS: cluster object storage platform designed for scalable services</paper_heading>
	<authors>Feng Zhou, Chao Jin, Yinghui Wu, Weimin Zheng</authors>
	<abstract>In this paper we present the design and implementation of Tsinghua Object Data Store (TODS), a cluster object storage system to support the building of scalable Internet services. TODS provides a unified, transparent and object-oriented view of the storage devices of the whole cluster, which greatly simplifies cluster service development. In the meantime, it is designed to be scalable and efficient. Services built on it can simply inherit these properties in a lot of cases. TODS supports ACID transactions, which facilitates the building of complex transactional services. TODS abstracts away from service logic most complexities of data management, which have often become major obstacles in developing high quality Internet services. The design principles, architecture and implementation of TODS are discussed. In our performance experiments, the system scales smoothly to a 36-node server cluster and achieves 11,160 in-memory reads per second and 396 transactions per second. We also demonstrate that the programming interface is significantly easier to use than that of RDBMS with a head-to-head comparative experiment.</abstract>
	<keywords>Object store; Cluster storage; Transparent persistence</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=790>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An analysis of update ordering in distributed replication systems</paper_heading>
	<authors>Wanlei Zhou, Li Wang, Weijia Jia</authors>
	<abstract>This paper analyses update ordering and its impact on the performance of a distributed replication system. We propose a model for update orderings and constraints and develop a number of algorithms for implementing different ordering constraints. A performance study is then carried out to analyse the update-ordering model. We show that our model allows the definition of an ordering constraint on each update operation, and the ordering implementation takes account of detailed inter-operation semantics denoted by commutative operations and causal operations to reduce unnecessary delay and results in a better response time for update requests.</abstract>
	<keywords>Replication; Data consistency; Update ordering; Distributed databases; Performance evaluation</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=791>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A framework of using cooperating mobile agents to achieve load sharing in distributed web server groups</paper_heading>
	<authors>Jiannong Cao, Xianbing Wang, Sajal K. Das</authors>
	<abstract>This paper studies the issues of using mobile agents to achieve load sharing for network services in a wide-area network environment such as the Internet. Traditionally, load sharing algorithms are based on the message-passing paradigm. In this paper, we propose the use of mobile agents as an aid to design fully distributed and dynamic load-sharing mechanisms for wide-area network services, which provide several advantages over the pure message-passing-based approach. A framework (called MALS—mobile agent-enabled load-sharing) for structuring and designing load sharing in wide-area network services is presented. In particular, we describe the design of a mobile agent-enabled distributed dynamic load-sharing scheme within the MALS framework. A simulation environment of the load-sharing protocol is being implemented using Aglet™, a Java-compliant mobile agent platform from IBM. Preliminary experimental results demonstrated that the proposed framework is effective.</abstract>
	<keywords>Mobile agent; Dynamic load sharing; Internet computing</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=792>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Improving real-time collaboration with highlighting</paper_heading>
	<authors>Haifeng Shen, Chengzheng Sun</authors>
	<abstract>Highlighting is a common tool in most single-user editors. It provides users with a mechanism of communication and collaboration between the author and readers by emphasizing some important text. This tool is also necessary and even more valuable in multi-user collaborative editors. However, it is non-trivial to extend it from single-user environment to multi-user environment because of the following challenges: (1) the need to differentiate highlights performed by different users, (2) the need to tackle inconsistency problems caused by concurrent operations and non-deterministic communication latency, and (3) the need to provide a flexible undo facility with the capability of undoing any highlighting operation at any time. In this paper, we will systematically address these issues and offer our solutions. These solutions lay a foundation for handling update operations similar in nature to highlighting, which update attributes of text instead of inserting/deleting text.</abstract>
	<keywords>Collaborative editing; Consistency maintenance; Group awareness; Highlighting; Operational transformation; Undo</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=793>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Pseudorandom number generation based on controllable cellular automata</paper_heading>
	<authors>Sheng-Uei Guan, Shu Zhang</authors>
	<abstract>A novel cellular automata (CA)—controllable CA (CCA) is proposed in this paper. Further, CCA are applied in pseudorandom number generation. Randomness test results on CCA pseudorandom number generators (PRNGs) show that they are better than one-dimensional (1D) CA PRNGs and can be comparable to two-dimensional (2D) ones. But they do not lose the structure simplicity of 1D CA. Further, we develop several different types of CCA PRNGs. Based on the comparison of the randomness of different CCA PRNGs, we find that their properties are decided by the actions of the controllable cells and their neighbors. These novel CCA may be applied in other applications where structure non-uniformity or asymmetry is desired.</abstract>
	<keywords>Cellular automata; Randomness test; Pseudorandom number generator; Controllable; Hybrid</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=794>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Merging, sorting and matrix operations on the SOME-Bus multiprocessor architecture</paper_heading>
	<authors>Constantine Katsinis</authors>
	<abstract>Due to advances in fiber-optics and VLSI technology, interconnection networks which allow multiple simultaneous broadcasts are becoming feasible. This paper presents the multiprocessor architecture of the Simultaneous Optical Multiprocessor Exchange Bus (SOME-Bus), and examines the performance of representative algorithms for matrix operations, merging and sorting, using the message-passing and distributed-shared-memory paradigms. It shows that simple enhancements to the network interface and the cache and directory controllers can result in communication time of O(1) for the matrix-vector multiplication algorithm using DSM. The SOME-Bus is a low-latency, high-bandwidth, fiber-optic interconnection network which directly links arbitrary pairs of processor nodes without contention, and can efficiently interconnect over 100 nodes. It contains a dedicated channel for the data output of each node, eliminating the need for global arbitration and providing bandwidth that scales directly with the number of nodes in the system. Each of P nodes has an array of receivers, with one receiver dedicated to each node output channel. No node is ever blocked from transmitting by another transmitter or due to contention for shared switching logic. The entire P receiver array can be integrated on a single chip at a comparatively minor cost resulting in O(P) complexity. The SOME-Bus has much more functionality than a crossbar by supporting multiple simultaneous broadcasts of messages, allowing cache consistency protocols to complete much faster.</abstract>
	<keywords>Multiprocessors; Broadcast architectures; Numerical algorithms</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=795>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A causal message logging protocol for mobile nodes in mobile computing systems</paper_heading>
	<authors>JinHo Ahn, Sung-Gi Min, Chong-Sun Hwang</authors>
	<abstract>This paper presents a causal message logging protocol with independent checkpointing for mobile nodes with the aim of efficiently handling several constraints of the mobile nodes such as mobility and disconnection, limited life of battery power, small amount of storage and low bandwidth on wireless link. For this purpose, the protocol includes a low-cost failure-free mechanism requiring only locating the mobility agent maintaining the latest checkpoint of each process on an mobile node during its handoff process. This mechanism forces only the latest checkpoint to be maintained on the stable storage while incurring low failure-free overhead. Also, the protocol uses two garbage collection schemes to remove log information of mobile nodes. The first scheme enables each mobile node to autonomously remove useless log information in its storage by piggybacking only some additional information without requiring any extra message and forced checkpoint. The second scheme allows the mobile node to remove a part of log information in its storage if more empty storage space is required after executing the first scheme. It reduces the number of processes to participate in the garbage collection by using the size of the log information of each process. Simulation results show that the two proposed schemes significantly reduce the garbage collection overhead compared with traditional schemes. Additionally, we present an efficient recovery algorithm to avoid frequent stable storage accesses, impose no restriction on the execution of live processes during recovery and ensure consistent recovery in case of being integrated with independent checkpointing.</abstract>
	<keywords>Mobile IP; Checkpointing; Message logging; Garbage collection; Recovery</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=796>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance of ILU preconditioning techniques in simulating anisotropic diffusion in the human brain</paper_heading>
	<authors>Ning Kang, Jun Zhang, Eric S. Carlson</authors>
	<abstract>We conduct simulations for the unsteady state anisotropic diffusion process in the human brain by discretizing the governing diffusion equation on a face-centered cubic grid and adopting a high performance differential-algebraic equation solver, IDA, to deal with the resulting large-scale system of DAEs. Incomplete LU preconditioning techniques are used with the GMRES method to accelerate the convergence rate of the iterative solution. We then investigate and compare the efficiency and effectiveness of a number of ILU preconditioners, and find out that the ILUT with a dual dropping strategy gives the best overall performance when it is provided with the optimum choices of the fill-in parameter and the threshold dropping tolerance.</abstract>
	<keywords>Anisotropic diffusion; DT-MRI; FCC grid; Preconditioning</keywords>
	<publication_month_year>2004-05-03 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 4</volumes_issues>
</paper>
<paper no=797>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>SIMBEX: a portal for the a priori simulation of crossed beam experiments</paper_heading>
	<authors>O. Gervasi, A. Laganà</authors>
	<abstract>The architecture and the computational kernels of Simulation of Crossed Molecular Beam Experiments, an Internet portal managing the simulation of elementary bimolecular processes as those occurring in crossed beam apparatuses, is discussed. The construction of this portal is our contribution to project 003/001 of the COST in Chemistry action D23 (METACHEM: Metalaboratories for complex computational applications in chemistry). The portal is specifically designed to support the collaborative efforts of various Computational Chemistry and Computer Science European laboratories aimed at building an a priori molecular simulator based on a Grid infrastructure. Such an environment makes use of free-software packages and was implemented using Web technologies.</abstract>
	<keywords>Internet portal; Molecular simulator; Computing Grid; Reaction dynamics</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=798>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VMSLab-G: a virtual laboratory prototype for molecular science on the Grid</paper_heading>
	<authors>O. Gervasi, A. Riganelli, L. Pacifici, A. Laganà</authors>
	<abstract>A Grid-based Virtual Molecular Science Laboratory (VMSLab-G) based on both human (meter scale) and molecular (nanometer scale) virtual reality aimed at facilitating ubiquitous laboratory practice and stimulate insight in scientific research is illustrated. Details are given of its structure, prototypical implementation and illustrative examples.</abstract>
	<keywords>Virtual laboratorye-learning; Human virtual reality (HVR); Molecular virtual reality (MVR); Molecular science simulation</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=799>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A prototype of distributed molecular visualization on computational grids</paper_heading>
	<authors>Huabing Zhu, Tony Kai-Yun Chan, Lizhe Wang, Wentong Cai, Simon See</authors>
	<abstract>This paper presents a distributed rendering system for visualization of massive molecular data sets on computational grids. It is designed with the ability to animate molecular dynamics (MD) simulation trajectories imported from a simulation engine in a distributed environment. MD simulation and visualization are computationally intensive tasks and computational grids are promising technologies for these applications. The framework of the system is introduced and the grid technologies and the parallel rendering approach are discussed in this paper.</abstract>
	<keywords>Molecular visualization; Parallel rendering; Load balance; Distributed rendering; Computational grids</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=800>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallelization of a relativistic DFT code</paper_heading>
	<authors>Leonardo Belpassi, Loriano Storchi, Francesco Tarantelli, Antonio Sgamellotti, Harry M. Quiney</authors>
	<abstract>We describe the implementation of some parallelization schemes for a relativistic four-component Dirac–Kohn–Sham program (BERTHA). The coulomb matrix construction has been parallelized by block-distributing the two-electron integral calculation, while for the exchange-correlation the numerical integration grid was distributed. Both schemes yield excellent performance results in terms of speed-up and scalability. The results obtained widen considerably the range of applicability of the method.</abstract>
	<keywords>DFT; Relativistic; Four-component; Parallelization</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=801>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fullerenes as polyradicals</paper_heading>
	<authors>Elena F. Sheka</authors>
	<abstract>Electronic structure of X60 molecules (X=C, Si) is considered in terms of 60 odd electrons and spin-dependent interaction between them. Conditions for the electrons to be excluded from the covalent pairing are discussed. A computational spin-polarized quantum-chemical scheme is suggested to evaluate four parameters (energy of radicalization, exchange integral, atom spin density, and squared spin) to characterize the effect quantitatively. A polyradical character of the species, weak for C60 and strong for Si60, is established.</abstract>
	<keywords>Fullerenes X60 (X=C, Si); Polyradicals; Quantum chemistry</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=802>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Effect of size and deformation on polarizabilities of carbon nanotubes from atomic increments</paper_heading>
	<authors>Francisco Torrens</authors>
	<abstract>The interacting induced-dipole polarization model implemented in program POLAR is used for the calculation of the polarizability α. The method is tested with single-wall carbon nanotubes (SWNTs) as a function of radius and elliptical deformation. This work gives a partial success with the application of POLAR when compared with reference calculations performed with program PAPID. α follows a simple law. PAPID differentiates more effectively than POLAR among SWNTs with increasing radial deformation. α can be modified reversibly by external radial deformation. Different effective αeff are calculated for the atoms at the highest and lowest curvature sites. The difference between POLAR and PAPID is due to the different parameterization scheme used for the initial atomic polarizabilities.</abstract>
	<keywords>Polarizability; Interacting induced-dipole polarization; Elliptical deformation; Nanostructure; Carbon nanotube</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=803>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Coherent triplet and singlet states in tubulin dynamics</paper_heading>
	<authors>Alexander A. Tulub</authors>
	<abstract>Tubulin, a natural nanoscale protein, is a subject of intense study due to its ability to form nets of dynamical information structures—tubulin microtubules, which are thought to be a prototype of biological quantum computer. The current study demonstrates the use of a combined QM(DFT-B3LYP, 6-31G** basis set)/MM method in modeling the tubulin dynamics over the time interval 0–100 ps. The hydrated tubulin (895 amino acids+2468 water molecules) forms a two-zone electronic structure, composed of the occupied and conductivity zones. The binding of Mg2+ to guanosinetriphosphate (GTP) site within the tubulin generate repeated in time strongly coherent triplet (T) and singlet (S) states. These states, when trapped by external electromagnetic field, produce T-,S-entanglement, which is essential for constructing quantum logic.</abstract>
	<keywords>Tubulin; Molecular dynamics; QM/MM method; Triplet–singlet transitions; Coherent states</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=804>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A theoretical investigation of the Chalk–Harrod and modified Chalk–Harrod mechanisms involved in hybrid integrated circuit building</paper_heading>
	<authors>G. Giorgi, F. De Angelis, N. Re, A. Sgamellotti</authors>
	<abstract>Nowadays the relevance of silicon chemistry is spreading due to the applications of Si, in particular in semiconductors, in order to obtain an increasing number of performing materials. A theoretical investigation based on the Density Functional Theory has been done on the reaction paths implied in the Pt-catalysed alkene hydrosilylation, a process through which the selective grafting of organic molecules to silicon can be obtained. We studied the SiH oxidative addition of SiH4 to a diphosphine molecule, the C2H4 insertion on PtH and PtSiH3 bonds, the isomerisation of the product of the C2H4 insertion, and the two following SiC and CH reductive eliminations. The set of these processes are known as the Chalk–Harrod mechanism and the modified Chalk–Harrod mechanism, respectively. The main goal of this work has been to identify the rate determining step of both mechanisms. Moreover, by employing dynamics simulations, we have reached a possible alternative product in the oxidative addition step.</abstract>
	<keywords>Density Functional Theory; Oxidative addition; Car-Parrinello molecular dynamics; Chalk–Harrod mechanism; Rate determining step</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=805>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A theoretical approach to molecular batteries: CC bonds functioning as electron shuttles</paper_heading>
	<authors>P. Belanzoni, M. Rosi, A. Sgamellotti</authors>
	<abstract>Density functional calculations have been performed on titanium, nickel, molybdenum and niobium–Schiff base complexes and titanium, nickel–porphyrinogen complexes in order to understand the behavior of these systems in redox processes. In titanium and nickel–Schiff base complexes CCσ bonds are formed upon reduction, while in titanium and nickel–porphyrinogen complexes CCσ bonds are formed upon oxidation. In both systems, the formation or the cleavage of CC bonds avoids a variation in the oxidation state of the metal and these CC bonds act not only as electron reservoirs, but also as a buffer for the oxidation state of the metal. The possibility of intramolecular electron transfer from CC bonds to MM bonds and vice versa has been investigated for molybdenum and niobium–Schiff base complexes.</abstract>
	<keywords>Density functional calculations; Molecular batteries; Electron reservoirs; Optimized geometries; Energies</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=806>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Phenylium and naphtylium cations in the interstellar medium: a density functional study on their reactivity towards D2 molecules</paper_heading>
	<authors>Marco Di Stefano, Marzio Rosi, Antonio Sgamellotti</authors>
	<abstract>The reactions of the phenylium, C6H5+, and the naphtylium, C10H7+, cations with D2 molecules are investigated at B3LYP/6-31G∗ level of theory. Our calculations indicate that the reactivity of C6H5+ and C10H7+ is dominated by the substitution of one or two hydrogen atoms with deuterium leading to isotope-exchange processes.</abstract>
	<keywords>Density functional calculations; Hydrocarbons; Interstellar chemistry; Isotopically labelled species; Thermochemistry</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=807>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Local orbitals for excited states</paper_heading>
	<authors>José-Vicente Pitarch Ruiz, Stefano Evangelisti, Daniel Maynau</authors>
	<abstract>A CAS-SCF-type algorithm based on molecular orbitals that preserve their physical nature during the iterative process have been recently proposed by our group. Our approach is based on the iterative partial diagonalization of the one-body reduced density matrix. If localized guess orbitals are used, the locality property is kept by the final orbitals. In the present contribution, the formalism has been used to obtain localized orbitals describing the n→π∗ excitations of aldehydes and dialdehydes.</abstract>
	<keywords>Local orbitals; Excited states; CAS-SCF</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=808>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallelization strategies for quantum reactive scattering codes</paper_heading>
	<authors>A. Laganà, L. Pacifici, D. Bellucci</authors>
	<abstract>In this paper various aspects of the parallelization of quantum reactive scattering codes are examined. To this end, different paradigms and models have been considered and message passing approaches compared with skeleton-based ones. The efficiency of the various models has also been investigated by measuring their speedup.</abstract>
	<keywords>Time dependent techniques; Time independent methods; Parallelization; Quantum reactive scattering</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=809>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Continuous interacting ant colony algorithm based on dense heterarchy</paper_heading>
	<authors>J. Dréo, P. Siarry</authors>
	<abstract>Ant colony algorithms are a class of metaheuristics which are inspired from the behavior of real ants. The original idea consisted in simulating the stigmergic communication, therefore these algorithms are considered as a form of adaptive memory programming. A new formalization is proposed for the design of ant colony algorithms, introducing the biological notions of heterarchy and communication channels. We are interested in the way ant colonies handle the information. According to these issues, a heterarchical algorithm called “Continuous Interacting Ant Colony” (CIAC) is designed for the optimization of multiminima continuous functions. CIAC uses two communication channels showing the properties of trail and direct communications. CIAC presents interesting emergent properties as it was shown through some analytical test functions.</abstract>
	<keywords>Continuous optimization; Ant colony algorithm; Heterarchy</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=810>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Low computational cost integrity for block ciphers</paper_heading>
	<authors>José M. Sierra, Julio C. Hernández, Narayana Jayaram, Arturo Ribagorda</authors>
	<abstract>Encryption algorithms supply confidentiality to communications between parties. However, only under certain circumstances, encryption might also supply integrity validation. For those situations, where encryption algorithms do not supply any integrity protection, additional mechanisms must be implemented (hash functions, digital signature). In this way, many solutions have been proposed in the literature, which achieve confidentiallity and integrity checking (some of them also provide authentication), however these methods usually represent a relatively high computational cost. This paper presents a new encryption mode for block cipher algorithms, which is based on the Plaintext Cipher Block Chaining (from now on, PCBC) mode. Our new mode supplies, apart from confidentiality, fast integrity checking with a minimum computational cost. This makes it eminently suitable for ensuring data integrity in GIS systems and at the same time assuring some other GIS requirements.</abstract>
	<keywords>Data integrity in GIS; Encryption computational costIntegrity of cipher messages; GIS confidentiallity; GIS integrity</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=811>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Group-oriented signature scheme with distinguished signing authorities</paper_heading>
	<authors>Chien-Lung Hsu, Tzong-Sun Wu, Tzong-Chen Wu</authors>
	<abstract>Elaborating on Shamir’s (t, n) threshold scheme and ElGamal’s digital signature scheme, we propose a group-oriented signature with distinguished signing authorities. The proposed scheme works under the following restrictions: (1) the signing document can be divided into any t smaller subdocuments in such a way that each subdocument is meaningful and will be signed as a unit by one discretionary signatory; (2) every signatory has the same knowledge domain that covers the subjects within the signing document; (3) any t out of n signatories within the group can be on behalf of the group for signing any document, and at most t−1 participants can conspire to attack Shamir’s scheme for revealing the other ones’ private keys (or shadows). In the proposed scheme, every discretionary signatory signs only a smaller part of the document, instead of the whole one. Such an approach achieves the justification of labor-division and responsibility-sharing inherent in the group works. Moreover, the bandwidth of data transmission for group signature construction can be further reduced. The proposed scheme also has the property that the discretionary signatories are anonymous to the verifier outside the group, and the size of a group signature is equivalent to that of any individual signature.</abstract>
	<keywords>Digital signature; Group-oriented signature(t, n) threshold scheme; Distinguished signing authorities</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=812>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards solution of the set-splitting problem on gel-based DNA computing</paper_heading>
	<authors>Weng-Long Chang, Minyi Guo, Michael Ho</authors>
	<abstract>Adleman wrote the first paper that demonstrated that DNA (DeoxyriboNucleic Acid) strands could be applied for dealing with solutions of the NP-complete Hamiltonian path problem (HPP). Lipton wrote the second paper that showed that the Adleman techniques could also be used to solve the NP-complete satisfiability (SAT) problem (the first NP-complete problem). Adleman and his co-authors proposed sticker for enhancing the Adleman–Lipton model. In this paper, it proves how to apply sticker in the sticker-based model to construct solution space of DNA in the set-splitting problem and how to apply DNA operations in the Adleman–Lipton model to solve that problem from the solution space of sticker.</abstract>
	<keywords>Biological computing; Molecular computing; DNA-based computing; The NP-complete problem</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=813>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A nonrepudiable threshold multi-proxy multi-signature scheme with shared verification</paper_heading>
	<authors>Shiang-Feng Tzeng, Cheng-Ying Yang, Min-Shiang Hwang</authors>
	<abstract>In this paper, we shall propose a threshold multi-proxy multi-signature scheme with shared verification. In the scheme allows the group of original signers to delegate the signing capability to the designated group of proxy signers. Furthermore, a subset of verifiers in the designated verifier group can authenticate the proxy signature. A threshold multi-proxy multi-signature scheme with the nonrepudiation property is a scheme where the proxy group cannot deny signing for the message and the verifier group can identify the proxy group for a proxy signature.</abstract>
	<keywords>Digital signature; Proxy signature; Threshold proxy signature; Proxy multi-signature scheme; Multi-proxy multi-signature scheme; Threshold multi-proxy multi-signature scheme</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=814>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance of adaptive space-sharing policies in dedicated heterogeneous cluster systems</paper_heading>
	<authors>Sivarama P. Dandamudi, Zhengao Zhou</authors>
	<abstract>In this paper, we propose a new two-level space-sharing policy for dedicated heterogeneous systems. This policy is based on the concept of basic processor unit (BPU). At a higher level, the policy computes the partition size in terms of BPUs. At the second-level, the BPUs of the first level are mapped to physical processors. The proposed policy is an adaptive space-sharing policy as this type of policy has been shown, in homogeneous systems, to provide better performance than the fixed and static policies by taking system load and user requirements into account. We compare the performance of our new policy with a previous policy. The results presented here show that the proposed policy provides substantial performance improvements at system loads of interest (i.e., medium to high system loads).</abstract>
	<keywords>Processor scheduling; Space-sharing policies; Cluster systems; Heterogeneous clusters; Performance evaluation</keywords>
	<publication_month_year>2004-06-15 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 5</volumes_issues>
</paper>
<paper no=815>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lattice BGK simulations of flow in a symmetric bifurcation</paper_heading>
	<authors>A.M. Artoli, D. Kandhai, H.C.J. Hoefsloot, A.G. Hoekstra, P.M.A. Sloot</authors>
	<abstract>Surgical planning as a treatment for vascular diseases requires fast blood flow simulations that are efficient in handling changing geometry. It is, for example, necessary to try different paths of a planned bypass and study the resulting hemodynamic flow fields before deciding the final geometrical solution. With the aid of a real time interactive simulation environment that uses an efficient flow solver, this allows flexible treatment planning. In this article, we demonstrate that the lattice Boltzmann method can be an alternative robust computational fluid dynamics technique for such kind of applications. Steady flow in a 2D symmetric bifurcation is studied and the obtained flow fields and stress tensor components are compared to those obtained by a Navier–Stokes (NS) solver. We also demonstrate that the method is fully adaptive to interactively changing geometry.</abstract>
	<keywords>Symmetric bifurcation; Changing geometry; Lattice Boltzmann; Finite volume; Hemodynamics</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=816>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Unsteady flow in a 2D elastic tube with the LBGK method</paper_heading>
	<authors>A.G. Hoekstra, Jos van’t Hoff, A.M. Artoli, P.M.A. Sloot</authors>
	<abstract>We report results of unsteady, harmonic flow simulations with the lattice BGK method in two-dimensional elastic tubes. The tubes are assumed to obey a simple constitutive equation, linearly relating the diameter of the tube to the pressure difference inside and outside the tube. First, as a benchmark, we present results of steady flow in such elastic tubes, and compare the performance of three different boundary conditions for the solid wall. Next, we present results of unsteady (harmonic) flow in the elastic tube, and validate the results by comparing them with theoretical expressions for the dispersion relation of the complex speed of traveling waves in the tube. Within the range of Womersley numbers tested the agreement between the simulations and the theory is good.</abstract>
	<keywords>Unsteady flow; Elastic tube; Moving bounbdary; Lattice Boltzmann method</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=817>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A lattice Boltzmann study of blood flow in stented aneurism</paper_heading>
	<authors>Miki Hirabayashi, Makoto Ohta, Daniel A. Rüfenacht, Bastien Chopard</authors>
	<abstract>The treatment of cerebral aneurisms with a porous stent has recently been proposed as a minimally invasive way to prevent rupture and favor coagulation mechanism inside the aneurism. The efficiency of a stent is related to several parameters which are not yet fully understood. The goal of this paper is to identify, through numerical simulations, how the stent structure and its positioning affect the hemodynamics properties of the flow inside the aneurism. We use the concept of flow reduction to characterize the stent efficiency. Our simulations are based on a lattice Boltzmann modeling of blood flow. Our results, which agree well with in vitro experiments, show how the various parameters play a role and help us to understand the phenomena</abstract>
	<keywords>Lattice Boltzmann; Hemodynamics; Blood flow; Stents; Aneurism</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=818>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulating two-dimensional thermal channel flows by means of a lattice Boltzmann method with new boundary conditions</paper_heading>
	<authors>Annunziata D’Orazio, Sauro Succi</authors>
	<abstract>Thermal boundary conditions for a doubled-populations BGK model are introduced and numerically demonstrated. The unknown thermal distribution functions at the boundary are assumed to be equilibrium distribution functions, with a counter-slip internal energy density which is determined consistently with Dirichlet and/or Neumann boundary constraints. The hydrodynamic boundary conditions are adapted to situations of engineering interest, and viscous heating effects are taken in account. The method is used to simulate channel flows; numerical results and theoretical solutions are found in satisfactory agreement for both hydrodynamic and thermal fields.</abstract>
	<keywords>Lattice Boltzmann thermal model; Doubled-populations BGK; Boundary conditions; Viscous heating; Channel flows; Heat transfer</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=819>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A non-linear lattice-Boltzmann model for ideal miscible fluids</paper_heading>
	<authors>Paulo C. Facin, Paulo C. Philippi, Luı́s O.E. dos Santos</authors>
	<abstract>This work is concerned with the construction of a lattice-Boltzmann (LB) model for ideal miscible fluids. In this particular case, the collision term in the LB equation can be modelled by only considering mutual and cross collisions between, respectively, particles of the same and of different kinds. A non-linear LB model with three distinct relaxation times intended to be used in problems with large concentration gradients is presented. The model enables the independent management of the fluid viscosities μr and μb and binary diffusivity D. It is shown that mass and momentum are, always, preserved and that consistent hydrodynamic equations are obtained at the incompressible limit. Theoretical values, obtained from Chapman–Enskog analysis, for binary diffusivity and mixture viscosity are compared with numerical values, directly obtained from LB simulations.</abstract>
	<keywords>Lattice-Boltzmann method (LBM); Binary miscible fluid; BGK collision term; Diffusion</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=820>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lattice Boltzmann simulation of the flow interference in bluff body wakes</paper_heading>
	<authors>Rodrigo Surmas, Luı́s O.E. dos Santos, Paulo C. Philippi</authors>
	<abstract>This paper presents a two-dimensional numerical simulation of fluid flow around a couple of identical circular cylinders aligned, respectively, along and orthogonal to the main-flow direction, at several distances. A lattice Boltzmann (LB) method is used. The forces resulting from fluid–solid interaction are calculated by considering the momentum exchanged between the fluid and the solid surfaces. Bluff body wake interference and its effect on drag and lift forces is studied in detail. The drag and the lift coefficients are obtained and compared with other existing results. A transition regime is obtained when the cylinders are aligned orthogonal to the main-flow direction and separated by distances smaller than about 2D.</abstract>
	<keywords>Lattice Boltzmann method (LBM); BGK collision term; Transition to turbulence; Vortex dynamics</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=821>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Numerical simulation of bubble flows by the lattice Boltzmann method</paper_heading>
	<authors>T. Inamuro, T. Ogata, F. Ogino</authors>
	<abstract>A lattice Boltzmann method for two-phase immiscible fluids with large density ratios is proposed. The difficulty in the treatment of large density ratio is resolved by using the projection method. The method can simulate two-phase fluid flows with the density ratio up to 1000. The method is applied to the simulations of a single rising bubble in liquid and many bubbles rising in a square duct. The terminal shapes and the terminal Reynolds numbers of the single bubble for various Morton and Eötvös numbers are in good agreement with available experimental data. The complicated unsteady structures of the interface and the flow field are illustrated in many bubbles rising in a square duct.</abstract>
	<keywords>Lattice Boltzmann method (LBM); Two-phase flow; Bubble flow</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=822>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Diffusion on unstructured triangular grids using Lattice Boltzmann</paper_heading>
	<authors>R.G.M. van der Sman</authors>
	<abstract>In this paper, we present a Lattice Boltzmann scheme for diffusion on unstructured triangular grids. In this formulation there is no need for interpolation, as is required in other LB schemes on irregular grids. At the end of the propagation step, the lattice gas particles arrive exactly at neighbouring lattice sites, as is the case in LB schemes on Bravais lattices. The scheme is constructed using the constraints that the moments of the equilibrium distribution equals that of the Maxwell–Boltzmann distribution. For a special choice of the relaxation parameter (ω=1) we show that our LB scheme is identical to a cell-centred Finite Volume scheme on an unstructured triangular grid. Consequences of the use of the unstructured grid on the required data structures is discussed in detail. Subsequent simulation show that diffusion on this unstructured grid is isotropic.</abstract>
	<keywords>Lattice Boltzmann; Diffusion; Unstructured grids; Finite Volume</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=823>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficiently using memory in lattice Boltzmann simulations</paper_heading>
	<authors>R. Argentini, A.F. Bakker, C.P. Lowe</authors>
	<abstract>Numerical simulation programs using the lattice Boltzmann equation are limited in the range of problems they can address by memory requirements. We describe an implementation scheme that can reduce memory usage by up to 78%. The performance of both the simple and the memory-reduced implementation are compared. Memory access is the determining factor in the speed of execution of memory intensive applications. We therefore look at the cache memory utilization in both the storage reduced version and in the simple reference implementation. From this we conclude that the memory-reduced method does not degrade the performance, in terms of execution speed, relative to a simple implementation.</abstract>
	<keywords>Computational techniques; Fluid dynamics; Lattice Boltzmann equation; Memory usage</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=824>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Hydrodynamic properties of fractal aggregates in 2D using Lattice Boltzmann simulation</paper_heading>
	<authors>Hung Phi Nguyen, Bastien Chopard, Serge Stoll</authors>
	<abstract>The Lattice Boltzmann approach for fluid dynamics is used to compute the drag force on various types of obstacles in two-dimensional flows. We study the finite size effects for the drag force on a cylinder and propose a scaling law. Then we consider the hydrodynamic behavior of fractal objects and compare the drag force they experience with that of simpler geometries. Simulation results show that, in 2D, the outer shape of the fractal plays the main role to determine the drag whereas the internal structure has little influence. Finally, the effect of fractal compacity on the hydrodynamic properties is also discussed. Since the fractal dimension and the outer radius of the aggregate are related, a dependence of the hydrodynamic properties on the fractal dimension can be established.</abstract>
	<keywords>Lattice Boltzmann; Hydrodynamic properties; Fractal aggregates</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=825>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Lattice Boltzmann modelling of droplets on chemically heterogeneous surfaces</paper_heading>
	<authors>A. Dupuis, J.M. Yeomans</authors>
	<abstract>We use a three-dimensional lattice Boltzmann model to investigate the spreading of mesoscopic droplets on homogeneous and heterogeneous surfaces. On a homogeneous substrate the base radius of the droplet grows with time as t0.28 for a range of viscosities and surface tensions. The time evolutions collapse onto a single curve as a function of a dimensionless time. On a surface comprising of alternate lyophobic and lyophilic stripes the wetting velocity is anisotropic and the equilibrium shape of the droplet reflects the wetting properties of the underlying substrate.</abstract>
	<keywords>Droplet spreading; Lattice Boltzmann model; Patterned surfaces</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=826>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evaluation of a lattice-Boltzmann method for mercury intrusion porosimetry simulations</paper_heading>
	<authors>J. Hyväluoma, P. Raiskinmäki, A. Jäsberg, A. Koponen, ... J. Timonen</authors>
	<abstract>We have simulated intrusion of a non-wetting liquid into pores of varying shape and size. Simulations were based on the lattice-Boltzmann method and the Shan–Chen multiphase model. The liquid–solid contact angle for pores with circular cross-section was found to be equal to that for pores with square cross-section, and constant even for small pore sizes if the discretised shape of the circular cross-section was taken into account. For comparison, contact angle was also determined for a liquid column descending in a capillary tube, and the results were found to be consistent. Application of the method to mercury intrusion porosimetry is discussed.</abstract>
	<keywords>Lattice-Boltzmann; Mercury intrusion porosimetry; Wetting</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=827>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A threshold signature scheme for group communications without a shared distribution center</paper_heading>
	<authors>Ting-Yi Chang, Chou-Chen Yang, Min-Shiang Hwang</authors>
	<abstract>In this paper, we shall propose a (t,n) threshold signature with (k,l) threshold-shared verification to be used in a group-oriented cryptosystem without a shared distribution center (SDC). In this scheme, any t participants can represent a group (signing group) to sign a message, and any k participants can represent another group (verifying group) to verify the signature. We need no SDC to distribute the public and private keys to all the participants in the two groups. Hence, our scheme is more practical in real-world applications and more efficient than its predecessors in terms of communication and computational complexity as well as storage.</abstract>
	<keywords>Cryptosystem; Digital signatures; Elliptic curves; Secret sharing; Threshold cryptosystem</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=828>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>VLADYMIR—a C++ matrix library for data-parallel applications</paper_heading>
	<authors>Jonas Lätt, Bastien Chopard</authors>
	<abstract>We present an object-oriented technique for the development of array-based programmes. It permits to develop applications that parallelise automatically and show an excellent scalability, even on large parallel machines. Unlike existing languages that treat array-based code at compiler level, we propose a dynamic approach through a matrix class library in C++. We will show that this model is even competitive with the compiler approach of Fortran 90. The aim of the paper is to introduce our matrix library as a conceptually easy and powerful development tool for scientific programmers, in particular in the field of cellular automata or lattice Boltzmann computations.</abstract>
	<keywords>Data-parallelism; Array-based computation; Cellular automata; Lattice Boltzmann models</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=829>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Competitive proportional resource allocation policy for computational grid</paper_heading>
	<authors>Chunlin Li, Layuan Li</authors>
	<abstract>This paper presents a competitive proportional resource allocation in computational grid. A system model is described that allows agents representing various grid resources, which owned by different real world enterprises, to coordinate their resource allocation decisions without assuming a priori cooperation. The grid task agents buy resources to complete tasks. Grid resource agents charge the task agents for the amount of resource capacity allocated. Two types of optimization problems related to grid task agent are proposed. Given grid resource agent’s pricing policy, the task agent optimization problem is to complete its job as quickly as possible when spending the least possible amount of money. Given specified amount of time to complete jobs, the task agent optimization problem is to minimize the cost accrued. This paper provides a price-directed proportional resource allocation algorithm for solving the grid task agent resource allocation problem. Experiments are made to compare the performance of the price-directed resource allocation with conventional Round-Robin allocation. The results of experiment show the price-directed allocation has better performance than usual Round-Robin allocation.</abstract>
	<keywords>Computational grid; Proportional resource allocation; Agent; Utility function; Competitive</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=830>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Creating an adaptive embedded system by applying multi-agent techniques to reconfigurable hardware</paper_heading>
	<authors>Hamid Reza Naji, B.Earl Wells, Letha Etzkorn</authors>
	<abstract>This paper describes how the multi-agent paradigm that is so prevalent in today’s distributed reactive software systems can be extended to operate in a dynamically adaptable embedded system environment. In this model, both reconfigurable hardware and the embedded system software design space can be partitioned into autonomous units of execution that are called agents, where each agent has the capacity to interact with the environment and other agents in an intelligent manner. The incorporation of agents that reside in reconfigurable hardware makes it possible to create an intelligent system that exploits the speed advantages associated with reconfigurable hardware within the framework of a unified system model that spans the entire hardware/software continuum. In addition to dynamic adaptability, this approach has the potential to greatly facilitate the flexibility, efficiency, fault tolerance, scalability, and reliability of embedded systems. This paper highlights agent-based reconfigurable embedded systems, and illustrates how they can be applied to a real-time sensor fusion type application using a conventional hardware description language.</abstract>
	<keywords>Multi-agent; Reconfigurable hardware; Sensor/data fusion</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=831>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A new approach to solve hybrid flow shop scheduling problems by artificial immune system</paper_heading>
	<authors>Orhan Engin, Alper Döyen</authors>
	<abstract>Artificial immune system (AIS) is an intelligent problem-solving technique that has been used in scheduling problems for about 10 years. AIS are computational systems inspired by theoretical immunology, observed immune functions, principles and mechanisms in order to solve problems. In this research, a computational method based on clonal selection principle and affinity maturation mechanism of the immune response is used. The n-job, k-stage hybrid flow shop problem is one of the general production scheduling problems. Hybrid flow shop (HFS) problems are NP-Hard when the objective is to minimize the makespan [Two-stage hybrid flowshop scheduling problem, Oper. Res. Soc. 39 (1988) 359]. The research deals with the criterion of makespan minimization for the HFS scheduling problems. The operating parameters of meta-heuristics have an important role on the quality of the solution. In this paper we present a generic systematic procedure which is based on a multi-step experimental design approach for determining the optimum system parameters of AIS. AIS algorithm is tested with benchmark problems. Experimental results show that the artificial immune system algorithm is an effective and efficient method for solving HFS problems.</abstract>
	<keywords>Hybrid flow shop scheduling; Artificial immune systems; Clonal selection; Experimental design</keywords>
	<publication_month_year>2004-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 6</volumes_issues>
</paper>
<paper no=832>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Iterative route planning for large-scale modular transportation simulations</paper_heading>
	<authors>Bryan Raney, Kai Nagel</authors>
	<abstract>Multi-agent transportation simulations represent travelers as individual “agents”, who make independent decisions about their actions. We are implementing such a simulation for all of Switzerland, which is composed of modules that model those decisions for each agent, such as: (i) Route planner: Generates travel plans. (ii) Micro-simulation: Executes plans simultaneously; computes agent interactions, leading to congestion. (iii) Feedback: Iterates the above modules, resolving inter-dependencies. We discuss the operation of these modules, and focus on improvements made to the feedback system, such as an agent “memory” that allows agents to choose among previously used routes based on their past performance.</abstract>
	<keywords>Traffic simulation; Transportation planning; Route planning</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=833>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evolving neural network using real coded genetic algorithm (GA) for multispectral image classification</paper_heading>
	<authors>Zhengjun Liu, Aixia Liu, Changyao Wang, Zheng Niu</authors>
	<abstract>This paper investigates the effectiveness of the genetic algorithm (GA) evolved neural network classifier and its application to the land cover classification of remotely sensed multispectral imagery. First, the key issues of the algorithm and the general procedures are described in detail. Our methodology adopts a real coded GA strategy and hybrid with a back propagation (BP) algorithm. The genetic operators are carefully designed to optimize the neural network, avoiding premature convergence and permutation problems. Second, a SPOT-4 XS imagery is employed to evaluate its accuracy. Traditional classification algorithms, such as maximum likelihood classifier, back propagation neural network classifier, are also involved for a comparison purpose. Based on an evaluation of the user’s accuracy and kappa statistic of different classifiers, the superiority of applying the discussed genetic algorithm-based classifier for simple land cover classification using multispectral imagery data is established. Thirdly, a more complicate experiment on CBERS (China–Brazil Earth Resources Satellite) data and discussion also demonstrates that carefully designed genetic algorithm-based neural network outperforms than gradient descent-based neural network. This has been supported by the analysis of the changes of connection weights and biases of the neural network. Finally, some concluding remarks and suggestions are also presented.</abstract>
	<keywords>Genetic algorithm; Land cover classification; Neural network; Remote sensing</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=834>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A modular eigen subspace scheme for high-dimensional data classification</paper_heading>
	<authors>Yang-Lang Chang, Chin-Chuan Han, Fan-Di Jou, Kuo-Chin Fan, ... Jeng-Horng Chang</authors>
	<abstract>In this paper, a novel filter-based greedy modular subspace (GMS) technique is proposed to improve the accuracy of high-dimensional data classification. The proposed approach initially divides the whole set of high-dimensional features into several arbitrary number of highly correlated subgroups by performing a greedy correlation matrix reordering transformation for each class. These GMS can be treated as not only a preprocess of GMS filter-based classifiers but also a unique feature extractor to generate a particular feature subspaces for each different class presented in high-dimensional data. The similarity measures are next calculated by projecting the samples into different modular feature subspaces. Finally, a GMS filter-based architecture based on the mean absolute errors criterion is adopted to build a non-linear multi-class classifier. The proposed GMS filter-based classification scheme is developed to find non-linear boundaries of different classes for high-dimensional data. It not only significantly improves the classification accuracy but also dramatically reduces the computational complexity of feature extraction compared with the conventional principal components analysis. Experimental results demonstrate that the proposed GMS feature extraction method suits the GMS filter-based classifier best as a classification preprocess. It significantly improves the precision of high-dimensional data classification.</abstract>
	<keywords>Greedy modular subspaces; High-dimensional data classification; Mean absolute errors; Principal components analysis</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=835>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An almost linear-time algorithm for trapezoidation of GIS polygons</paper_heading>
	<authors>Gian Paolo Lorenzetto, Amitava Datta</authors>
	<abstract>The decomposition of planar polygons into triangles is a well studied area of computer graphics with particular relevance to geographic information systems (GIS). Trapezoidation is often performed as a first step to triangulation. There is a complex linear-time algorithm [Discrete Comput. Geom. 6 (1991) 485] for the decomposition of a simple polygon into triangles. However, it is extremely complicated and in practice O(n log n) algorithms are used. Our motivation in trapezoidation of large GIS polygons is the fast display of such polygons. It is much faster to display simple shapes like triangles or trapezoids on raster graphics devices, compared to complex polygons. Hence, quite often complex polygons are decomposed into triangles or trapezoids for displaying. Since triangulation is usually more difficult compared to trapezoidation, we are interested in trapezoidation of GIS polygons for faster display. We present a very simple algorithm for the trapezoidation of simple polygons without holes. Our algorithm runs in O(n) time in practice with a very small hidden constant. We have extensively tested our algorithm for polygons in a GIS database. Our algorithm is easy to implement compared to existing algorithms and runs extremely fast even for polygons with thousands of vertices.</abstract>
	<keywords>Simple polygon; Triangulation; Trapezoidation; Raster display; Linear-time algorithm; Sweeplines</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=836>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Spatio-temporal information integration in XML</paper_heading>
	<authors>Bo Huang, Shanzhen Yi, Weng Tat Chan</authors>
	<abstract>Location and time are interconnected and play an important role in various database applications. This paper explores how these two types of information are integrated in XML. In addition to spatial literal types (e.g. Point, LineString and Polygon) and temporal literal types (TimeInterval and Instant), a parameterized type called TimeSeries has been defined using XML schema to represent the histories of both spatial and non-spatial properties of an object. An associated query language, XML-STQ, based upon XQuery is also provided in support of spatio-temporal queries on object histories. This query language is illustrated with some typical query examples.</abstract>
	<keywords>XML; XML schema; XML-STQ; Xquery; Spatial–temporal types; ODMG; Object model</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=837>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>TIN meets CAD—extending the TIN concept in GIS</paper_heading>
	<authors>Rebecca O.C. Tse, Christopher Gold</authors>
	<abstract>Extending the current “2.5D” terrain model is a necessary development in GIS. Existing 2D surface information must be extended to 3D, while trying to preserve the topological integrity. Computer-Aided Design (CAD) on the other hand is often fully 3D, which may be excessive for our “extended 2D” mapping needs. We have used the “boundary representation” (b-rep) design from CAD systems, based on manifold models, to address these problems. Triangulated Irregular Network (TIN) models of the terrain surface are well known in Geographic Information System (GIS), but are unable to represent cliffs, caves or holes, which are required to represent complex buildings. The simple TIN b-rep structures may be extended using formal Euler Operators to add simple CAD functionality while guaranteeing the connectivity required for terrain modelling. We believe this provides a simple and reliable extension that is sufficient for many applications.</abstract>
	<keywords>Euler Operators; TIN concept; GIS</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=838>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Land-use adjustment with a modified soil loss evaluation method supported by GIS</paper_heading>
	<authors>T.H. Li, J.R. Ni, W.X. Ju</authors>
	<abstract>Land-use structure information is of significance in evaluation of soil loss owing to the contributions of various land-use patterns with different relative importance to soil and water conservation. In previous studies, a land-use structure characteristic index (SI), defined as the sum of products between each weighted land-use type and the percentage of the impacted area in the whole test region, was proposed to reflect the resulting impacts of human factors and serve as an indirect measure of soil loss variation. In this paper, the SI was modified with consideration of spatial distribution of land-use in terms of the minimal distance between a specific land patch and a river, either the main stem or one of the primary tributaries. A topographic factor was also introduced to correct the SI. Soil loss was evaluated with support of GIS through a case study at Zhifanggou, a small catchment in the Yanhe watershed in the middle part of the Loess plateau. Comparison was made between the soil losses before and after the adjustment of land-use types since 1987 according to the governmental ordinance aiming at ecological rehabilitation. It was found that the land-use adjustment in the last 12 years has been in a right direction toward minimizing soil loss, but the difference of the actual SI in 1999 and the predicted SI corresponding to the potential optimal land-use structure implies that more efforts should be made in land-use conversion in the coming years.</abstract>
	<keywords>Soil loss; Land-use; GIS</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=839>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Learning to detect texture objects by artificial immune approaches</paper_heading>
	<authors>Hong Zheng, Jingxin Zhang, Saeid Nahavandi</authors>
	<abstract>This paper introduces a novel method to detect texture objects from satellite images. First, a hierarchical strategy is developed to extract texture objects according to their roughness. Then, an artificial immune approach is presented to automatically generate segmentation thresholds and texture filters, which are used in the hierarchical strategy. In this approach, texture objects are regarded as antigens, and texture object filters and segmentation thresholds are regarded as antibodies. The clonal selection algorithm inspired by human immune system is employed to evolve antibodies. The population of antibodies is iteratively evaluated according to a statistical performance index corresponding to object detection ability, and evolves into the optimal antibody using the evolution principles of the clonal selection. Experimental results of texture object detection on satellite images are presented to illustrate the merit and feasibility of the proposed method.</abstract>
	<keywords>Texture object detection; Artificial immune system; Clonal selection</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=840>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fast rectifying airborne infrared scanning image based on GPS and INS</paper_heading>
	<authors>You Hongjian, Shao Yun, Li Shukai</authors>
	<abstract>Airborne infrared scanning image is often distorted due to the change of the airplane’s attitude and position. Therefore, some ground control points (GCPs) must be used to rectify the raw infrared image by traditional approach. A new method is tested in this paper to rectify scanning image fast based on global positioning system (GPS) and inertial navigation system (INS). First, the positions of some pixels within the same scanning line are calculated, and then these pixels are used as GCPs to rectify the whole scanning line. The gray level interpolation and strips mosaic are also discussed in this paper. The data of flight tests processed shows this method is feasible.</abstract>
	<keywords>GPS; INS; Rectify image; Mosaic</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=841>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Assessment of the effectiveness of support vector machines for hyperspectral data</paper_heading>
	<authors>Mahesh Pal, P.M. Mather</authors>
	<abstract>Support vector machines (SVMs) have recently been introduced into machine learning for pattern recognition. In this paper, a multi-class SVM is used for classification of DAIS hyperspectral remotely sensed data. Results show that the SVM performs better than maximum likelihood, univariate decision tree and backpropagation neural network classifiers, even with small training data sets, and is almost unaffected by the Hughes phenomenon [IEEE Trans. Inform. Theory IT-14 (1968) 55].</abstract>
	<keywords>Support vector machines; Hughes phenomenon; DAIS data; Classification accuracy</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=842>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the reconstruction of three-dimensional complex geological objects using Delaunay triangulation</paper_heading>
	<authors>Yong Xue, Min Sun, Ainai Ma</authors>
	<abstract>In the past four decades, huge amount of geologic data has been accumulated. Now it is time to integrate these geologic data with other sources of data, such as the digital elevation model (DEM) and remote sensing data, to do various analyses, 3D modelling, and real-time interactive visualisation to extract more meaningful results. Reconstruction of 3D geological objects is very important for geological research and mining. Traditional geologic maps that show structures and materials at the earth’s surface no longer are sufficient for storing, displaying, and transmitting geoscience information. Fully three-dimensional geologic maps are needed to provide users with geologic information of the quality, accuracy, and detail necessary to solve problems related to natural hazard mitigation, resource management, mineral and petroleum exploration, contaminant dispersion, and other issues. Although some data models and algorithms have been developed, there are few works on reconstruction of real 3D complex geological objects. In this paper, we apply Delaunay triangulation and tetrahedron for 3D reconstruction of geological structures with sudden changes. Since this technique is mainly used for 3D reconstruction of medical objects, follows that we provide a new field of application.</abstract>
	<keywords>3D geological objects reconstruction; Delaunay triangulation algorithm; Tetrahedron mesh algorithm</keywords>
	<publication_month_year>2004-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 7</volumes_issues>
</paper>
<paper no=843>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Functional networks for B-spline surface reconstruction</paper_heading>
	<authors>A. Iglesias, G. Echevarría, A. Gálvez</authors>
	<abstract>Recently, a new extension of the standard neural networks, the so-called functional networks, has been described [E. Castillo, Functional networks, Neural Process. Lett. 7 (1998) 151–159]. This approach has been successfully applied to the reconstruction of a surface from a given set of 3D data points assumed to lie on unknown Bézier [A. Iglesias, A. Gálvez, Applying functional networks to CAGD: the tensor-product surface problem, in: D. Plemenos (Ed.), Proceedings of the International Conference on Computer Graphics and Artificial Intelligence, 3IA’2000, 2000, pp. 105–115; A. Iglesias, A. Gálvez, A new artificial intelligence paradigm for computer-aided geometric design, in: Artificial Intelligence and Symbolic Computation, J.A. Campbell, E. Roanes-Lozano (Eds.), Lectures Notes in Artificial Intelligence, Berlin, Heidelberg, Springer-Verlag, vol. 1930, 2001, pp. 200–213] and B-spline tensor-product surfaces [A. Iglesias, A. Gálvez, Applying functional networks to fit data points from B-spline surfaces, in: H.H.S. Ip, N. Magnenat-Thalmann, R.W.H. Lau, T.S. Chua (Eds.), Proceedings of the Computer Graphics International, CGI’2001, IEEE Computer Society Press, Los Alamitos, CA, 2001, pp. 329–332]. In both cases the sets of data were fitted using Bézier surfaces. However, in general, the Bézier scheme is no longer used for practical applications. In this paper, the use of B-spline surfaces (by far the most common family of surfaces in surface modeling and industry) for the surface reconstruction problem is proposed instead. The performance of this method is discussed by means of several illustrative examples. A careful analysis of the errors makes it possible to determine the number of B-spline surface fitting control points that best fit the data points. This analysis also includes the use of two sets of data (the training and the testing data) to check for overfitting, which does not occur here.</abstract>
	<keywords>Neural networks; Functional networks; CAGD; Surface reconstruction; B-spline surfaces; Bézier surfaces; Artificial intelligence; Functional equations</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=844>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computer graphics for water modeling and rendering: a survey</paper_heading>
	<authors>A. Iglesias</authors>
	<abstract>A key topic in computer graphics is the realistic representation of natural phenomena. Among the natural objects, one of the most interesting (and most difficult to deal with) is water. Its inherent complexity, far beyond that of most artificial objects, represents an irresistible challenge for the computer graphics world. Thus, during the last two decades we have witnessed an increasing number of papers addressing this problem from several points of view. However, the computer graphics community still lacks a survey classifying the vast literature on this topic, which is certainly unorganized and dispersed and hence, difficult to follow. This paper aims to fill this gap by offering a historical survey on the most relevant computer graphics techniques developed during the 1980s and 1990s for realistic modeling, rendering and animation of water.</abstract>
	<keywords>Water waves; Water modeling; Water rendering; Water animation; Bump mapping; Particle systems; Height fields; Environment maps; Solid textures; Navier–Stokes equations; Fluid dynamics; Caustics</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=845>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An octree-based multiresolution hybrid framework</paper_heading>
	<authors>Imma Boada</authors>
	<abstract>The Hybrid Octree (HO) is a new approach to deal with hybrid scenes, (i.e. scenes composed of surface and volume data). The HO codes into a single model a compressed approximation of an isosurface extracted from a volume dataset and a compressed representation of the scalar data. The HO definition and its construction algorithm are provided in the paper. We also describe the capabilities of the HO to efficiently manipulate surface and volume data independently and its facilities to composite and visualize surface and volume data using texture hardware. Special emphasis is given to the HO facilities to support multiresolution. Some examples are presented and the goodness of the model is discussed.</abstract>
	<keywords>Hybrid visualizations; Surface-volume data integration; Hierarchical data structures; Multiresolution; 3D texture-based volume rendering</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=846>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Surface reconstruction from scan paths</paper_heading>
	<authors>Claus-Peter Alberts</authors>
	<abstract>Given a finite set of points sampled from a surface, a particular important topic of surface reconstruction is to find a triangular mesh connecting the sampling points and approximating the surface. We introduce a new solution for this problem which takes into consideration hidden structural information in form of scan paths. The scan paths, which often are available in sampling data sets, allow us to cope with creases or ridges more reliably than algorithms assuming unstructured point clouds. Moreover, the consideration of scan paths leads to a better reconstruction of surface boundaries than other heuristics.</abstract>
	<keywords>Geometric modelling; Surface reconstruction; Boundary reconstruction; Sharp bends; Creases; Scan paths; Data reduction</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=847>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Light mesh: soft shadows as interpolation of visibility</paper_heading>
	<authors>Victor A. Debelov, Igor Sevastianov</authors>
	<abstract>The light meshes (LM) method is a modification of the recursive ray tracing algorithm (RRTA). In this paper we consider an original procedure of shadow calculations, which allows soft shadows to be obtained. The light meshes method can be integrated easily into existing implementations of RRTA as a supplementary technique or plug-ins. In comparison with conventional RRTA the suggested technique allows us to: (a) build soft shadows, (b) speed up a process of shadow calculations, (c) simulate main effects of diffuse interreflections, (d) omit the ambient term. Obviously, visual results of the LM method differ slightly from those calculated with the help of the basic RRTA. A light mesh is a point set in the 3D space of a scene. In practice, it is approximated by a finite set of points, as a rule, by evenly spaced grids. At each mesh point we compute illuminance, which characterizes some kind of “light field intensity” in the scene space. Then an important part of resulting illumination of a scene object point is obtained by the interpolation of the stored values of nearby mesh points. Experiments show that the approach proposed reduces computational time with respect to the ray tracing algorithm if a scene contains a lot of light sources and/or an image has a large resolution.</abstract>
	<keywords>Light mesh; Ray tracing; Soft shadow; Visibility interpolation</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=848>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Analytical C2 smooth blending surfaces</paper_heading>
	<authors>Jian J. Zhang, Lihua You</authors>
	<abstract>Two factors are important in the generation of blending surfaces for interactive graphical and CAD applications, computational speed and the degree of smoothness. Most surface-blending methods blend surfaces with tangent continuity. However, curvature continuity has recently become increasingly important in various applications. In this paper, we present a method that is able to achieve curvature continuity based on the use of partial differential equations (PDE). The blending surfaces are generated as the solution to a sixth-order PDE with one vector-valued parameter. To achieve interactive performance, we propose an effective analytical method for the resolution of this sixth-order PDE.</abstract>
	<keywords>Surface blending; Curvature continuity; Sixth-order partial differential equation; Analytical solution</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=849>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An automatic algorithm for approximating boundary of bitmap characters</paper_heading>
	<authors>M. Sarfraz, M.A. Khan</authors>
	<abstract>In this paper, an automatic and efficient algorithm for outline capture of character images, stored as bitmap, is presented. This method is well suited for characters of non-Roman languages like Arabic, Japanese, Urdu, Persian, etc. Contemporary word processing systems store shapes of the characters in terms of their outlines, and outlines are expressed as cubic Bezier curves. The process of capturing outlines includes steps: detection of boundary, finding Corner Points and Break Points and fitting the curve. The work done in this paper, fully automate the above process and produces optimal results.</abstract>
	<keywords>Digital character images; Significant points; Boundary points; Corner points; Break points; Reparameterization; Cubic Bezier</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=850>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficiently using connectivity information between triangles in a mesh for real-time rendering</paper_heading>
	<authors>Ó. Belmonte, I. Remolar, J. Ribelles, M. Chover, M. Fernández</authors>
	<abstract>Triangle meshes are the most popular standard model used to represent polygonal surfaces. Drawing these meshes as a set of independent triangles involves sending a vast amount of information to the graphics system. Taking advantage of the connectivity information between the triangles in a mesh dramatically diminishes the amount of information the graphics system must handle. Multiresolution Triangle Strips (MTS) represent a triangle mesh as a collection of multiresolution triangles strips. These strips are the basis of both the storage and the rendering stage. The coherence between the extraction of two levels of detail is used in the model in order to decrease the visualisation time.</abstract>
	<keywords>Multiresolution modelling; Level of detail; Interactive visualisation</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=851>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Normal vector compression of 3D mesh model based on clustering and relative indexing</paper_heading>
	<authors>Deok-Soo Kim, Youngsong Cho, Hyun Kim</authors>
	<abstract>As the transmission of 3D shape models through Internet becomes more important, the compression issue of shape models gets more critical. While the compressions of topology and geometry have been explored significantly, the same issue for normal vectors has not yet been studied as much as it deserves. Presented in this paper is an approach to compress the normal vectors of a 3D mesh model using the concept of clustering and relative indexing. The model is assumed to be manifold triangular mesh model with normal vectors associated with vertices. The proposed scheme clusters the normal vectors of given model and the representative normal vector of each cluster is referred to via a mixed use of relative as well as absolute indexing concepts. It turns out that the proposed approach achieves a significant compression ratio (less than 10% of the original VRML model files) without a serious sacrifice of the visual quality.</abstract>
	<keywords>Normal vector compression; Clustering; Relative indexing</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=852>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Bandwidth reduction for remote navigation systems through view prediction and progressive transmission</paper_heading>
	<authors>Pere-Pau Vázquez, Mateu Sbert</authors>
	<abstract>Remote navigation systems, such as exploration of virtual 3D worlds or remote surgery, usually require higher bandwidth than the Internet connection commonly available at home. In this paper, we explore a set of techniques to reduce the bandwidth required by these applications. Our system consists of a client PC equipped with a graphics card, and a remote high-end server. The server hosts the remote environment and does the actual rendering of the scenes for several clients, and the new image is passed to them. This scheme is suitable when the data has a copyright or when its size may exceed the rendering capabilities of the client. The general scheme is the following: each time the position changes, the new view is predicted by both the client and the server and the difference information between the predicted view and the correct one is sent to the client. To reduce bandwidth we can improve the prediction method, and the transmission system. We present here two groups of techniques: First, a set of lossless methods which achieve reductions of up to a 9:1 ratio. These are a combination of a two-level forward warping, that takes advantage of spatial coherence, and a masking method, which allows to transmit only the information that really needs to be updated. Second, a set of lossy methods suitable for very low bandwidth environments which involve both progressive transmission and image reuse. They consider relevant parameters such as the number of pixels, the amount of information they provide, and their colour deviation in order to create a strategy for prioritizing the information transmission. This system allows to improve up to an additional 4:1 ratio. The quality of the generated images is very high, and often indistinguishable from the correct ones.</abstract>
	<keywords>Bandwidth reduction; Remote navigation; Image-based rendering; Information theory</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=853>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Locally constrained synthetic LoDs generation for natural terrain meshes</paper_heading>
	<authors>M. Perez, M. Fernandez, P. Morillo, I. Coma</authors>
	<abstract>Terrain representation is a basic topic in the field of interactive graphics. The amount of data required for a good quality of the terrain offers an important challenge to developers of such systems. For users of these applications, the accuracy of geographical data is generally less important than its natural visual appearance. This makes it possible to maintain a limited geographical database for the system and to extend it generating synthetic data. The evaluation of the intrinsic properties of the terrain (i.e. fractal dimension, roughness, etc.) may be used as the basis for generating extra data accomplishing the same patterns discovered in the actual information. However, it is also interesting to point out that in most natural landscapes, it is usual to have human or natural changes in the basic properties of some areas, i.e. a road or a river. This fact can make it more difficult for synthetic data generation to be free of visual artifacts within these areas. In this paper, we combine fractal and wavelet theories to provide extra data which keeps the natural properties of actual information available. New levels of detail for the terrain are obtained by applying an inverse Wavelet Transform to a set of values randomly generated, thus maintaining the coherence of statistical properties with the original geographical data. Combined with this approach, the use of energy reduction masks has been added in order to avoid undesired visual artifacts in those special areas for which the general terrain properties are no longer valid.</abstract>
	<keywords>Fractal; Wavelets; Terrain mesh; Synthetic extra resolution generation; Locally constrained extra resolution</keywords>
	<publication_month_year>2004-11-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 20, Issue 8</volumes_issues>
</paper>
<paper no=854>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Incorporation of middleware and grid technologies to enhance usability in Computational Chemistry applications</paper_heading>
	<authors>Jerry P. Greenberg, Steve Mock, Karan Bhatia, Mason Katz, ... Kim K. Baldridge</authors>
	<abstract>High performance computing, storage, visualization, and database infrastructures are increasing in complexity as scientists move towards grid-based computing. This evolution of computing infrastructure has the effect of pushing breakthrough computational capabilities beyond the reach of domain scientists. In this work, we discuss a workflow management system that allows portal construction that is fully integrated with emerging grid standards but can be dynamically reconfigured. By defining an XML schema to describe both resources, application codes and interfaces, we will enable a “pluggable” event-driven model where grid-enabled services can be composed to form elaborate pipelines of simulation, and visual analysis.</abstract>
	<keywords>XML schema; pluggable;  grid-based computing; grid-enabled</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=855>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Executing and observing CFD applications on the Grid</paper_heading>
	<authors>Jan Wendler, Florian Schintke</authors>
	<abstract>We present the FlowGrid system, that allows Computational Fluid Dynamics (CFD) simulations to be executed in Grid environments. Using this system, users can observe online the progress of their simulation by looking at intermediate results, that are visualized in the graphical user interface. Several Grid centers across Europe currently use and validate the system with their CFD computations and build a ‘CFD Virtual Organization’ to share their resources and balance their processing load. We first describe the overall FlowGrid architecture, highlight its special features and present the system along a typical job execution. The Grid infrastructure, i.e. FlowServe, is presented in detail, a description of the accounting system is given and experiences with the FlowGrid testbed are provided. Finally, we provide evidence that the results can be used as a generic CFD Grid service.</abstract>
	<keywords>Computational fluid dynamics; Grid computing; FlowGrid</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=856>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Application of Grid-enabled technologies for solving optimization problems in data-driven reservoir studies</paper_heading>
	<authors>Manish Parashar, Hector Klie, Umit Catalyurek, Tahsin Kurc, ... Mary F. Wheeler</authors>
	<abstract>This paper presents the use of numerical simulations coupled with optimization techniques in oil reservoir modeling and production optimization. We describe three main components of an autonomic oil production management framework. The framework implements a dynamic, data-driven approach and enables Grid-based large scale optimization formulations in reservoir modeling.</abstract>
	<keywords>Grid computing; Optimization algorithms; Distributed database</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=857>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Application of grid computing to parameter sweeps and optimizations in molecular modeling</paper_heading>
	<authors>Wibke Sudholt, Kim K. Baldridge, David Abramson, Colin Enticott, ... Duy Nguyen</authors>
	<abstract>In science and engineering in general and in computational chemistry in particular, parameter sweeps and optimizations are of high importance. Such parametric modeling jobs are embarrassingly parallel and thus well suited for grid computing. The Nimrod toolkit significantly simplifies the utilization of computational grids for this kind of research by hiding the complex grid middleware, automating job distribution, and providing easy-to-use user interfaces. Here, we present examples for the usage of Nimrod in molecular modeling. In detail, we discuss the parameterization of a group difference pseudopotential (GDP). Other applications are protein–ligand docking and a high-throughput workflow infrastructure for computational chemistry.</abstract>
	<keywords>Grid computing; Parameter sweep; Parameter optimization; Computational chemistry; Pseudopotential</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=858>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>“Applications on demand” as the exploitation of the Migrating Desktop</paper_heading>
	<authors>Mirosław Kupczyk, Rafał Lichwała, Norbert Meyer, Bartosz Palak, ... Paweł Wolniewicz</authors>
	<abstract>The Migrating Desktop is a ready-to-use GUI framework for making use of grid applications and putting into practice the “applications on demand” concept. On-demand computing, contrary to the traditional approach of assigning resources to applications, refers to the concept of pooling system resources and dynamically allocating them to meet shifting demands. We introduce a ready-to-use framework for the integration of different HPC systems into a comfortable working environment with support for “applications on demand”. This work is done under the EU CrossGrid project IST-2001-32243.</abstract>
	<keywords>Migrating Desktop; Grid; Roaming Access Server; Mobile user; GUI</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=859>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Cambridge CFD grid for large-scale distributed CFD applications</paper_heading>
	<authors>X. Yang, M. Hayes, K. Jenkins, S. Cant</authors>
	<abstract>The Cambridge CFD (computational fluid dynamics) Grid is a distributed problem-solving environment for large-scale CFD applications set up between the Cambridge eScience Centre and the CFD Laboratory in the Engineering Department at the University of Cambridge. A Web portal, the Cambridge CFD Web Portal (CamCFDWP) has been developed to provide transparent integration of CFD applications to non-computer scientist end users. In addition to the basic services provided of authentication, job submission and file transfer, the CamCFDWP makes use of XML (extensible markup language) techniques which make it possible to easily share datasets between different groups of users. A Web service interface has recently been implemented for a CFD database which could be integrated in the CamCFDWP in the near future. We also review how this Web service can be made secure using SSL, XML signatures and XML encryption.</abstract>
	<keywords>Grid; Portal; Web services; XML; Security</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=860>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Rule-based visualization in the Discover computational steering collaboratory</paper_heading>
	<authors>Hua Liu, Lian Jiang, Manish Parashar, Deborah Silver</authors>
	<abstract>In this paper, we introduce the concept of rule-based visualization for a computational steering collaboratory and show how these rules can be used to steer the behaviors of the visualization subsystem. Rules define high-level policies and are used to autonomically select and tune the visualization routines based on application requirements and available computing/network resources. Such an autonomic management of the visualization subsystem can significantly improve the effectiveness of computational steering collaboratories in wide-area Grid environments.</abstract>
	<keywords>Rule-based visualization; Computational steering; Feature tracking</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=861>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Enhancing wildland fire prediction on cluster systems applying evolutionary optimization techniques</paper_heading>
	<authors>Baker Abdalhaq, Ana Cortés, Tomàs Margalef, Emilio Luque</authors>
	<abstract>Classical prediction fire schemes do not match the real fire propagation, basically, because of the complexity of the physical models involved, the need for a great amount of computation and the difficulties of providing accurate input parameters. We describe an enhanced prediction scheme, which uses recent fire history and optimization techniques to predict near future propagation. The proposed method takes advantage of the computational power offered by distributed systems to accelerate the optimization process at real time.</abstract>
	<keywords>Cluster computing; Fire prediction; Optimization convergence</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=862>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An Atmospheric Sciences Workflow and its implementation with Web Services</paper_heading>
	<authors>David Abramson, Jagan Kommineni, John L. McGregor, Jack Katzfey</authors>
	<abstract>Computational and data Grids couple geographically distributed resources such as high performance computers, workstations, clusters, and scientific instruments. Grid Workflows consist of a number of components, including: computational models, distributed files, scientific instruments and special hardware platforms. In this paper, we describe an interesting grid workflow in atmospheric sciences and show how it can be implemented using Web Services. An interesting attribute of our implementation technique is that the application codes can be adapted to work on the Grid without source modification.</abstract>
	<keywords>Grid Workflow; Web Services; Atmospheric Science; Legacy code</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=863>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Workflow composer and service registry for grid applications</paper_heading>
	<authors>Marian Bubak, Tomasz Gubała, Michał Kapałka, Maciej Malawski, Katarzyna Rycerz</authors>
	<abstract>Automatic composition of workflows from Web and Grid services is an important challenge in today’s distributed applications. The system presented in this paper supports the user in composing an application workflow from existing Grid services. The flow composition system builds workflows on an abstract level with semantic and syntactic descriptions of services available on the Grid. Two main modules of the system are the flow composer and the distributed Grid service registry. We present motivation, the concept of the overall system architecture and the results of a feasibility study.</abstract>
	<keywords>Grid workflow; Workflow composition; Distributed registry; Ontologies; Grid programming</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=864>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Federate migration in HLA-based simulation</paper_heading>
	<authors>Wentong Cai, Zijing Yuan, Malcolm Yoke Hean Low, Stephen J Turner</authors>
	<abstract>The High Level Architecture (HLA) promotes simulation interoperability and reusability, making it an ideal candidate to model large-scale systems. However, a large-scale simulation running in a distributed environment is often affected by the imbalance of load level at different computing hosts. Migrating simulation components from heavily-loaded hosts to less-loaded ones can solve the problem. Protocols to migrate HLA simulation federates have been introduced by various research projects. However, existing protocols achieve migration by using either federation wide synchronization or a third party host, such as FTP servers, to handle the saving and restoring of migration states. We have previously developed a framework to execute HLA-based simulations in the Grid environment with migration support as a prominent design feature. Based on the framework, a federate migration protocol that bypasses the shortcomings identified above has been developed and better migration performance is achieved. To eliminate message loss during the migration process, a counter mechanism is employed. Studies also show that federate join time contributes significantly to the migration overhead. Therefore, our protocol overlaps the simulation execution of the federate to be migrated and the join federation operation performed by the restarting federate at the migration destination. In this paper, the development of our protocol is described and experimental results with comparison to the approach using federation wide synchronization are discussed.</abstract>
	<keywords>HLA-based simulation; Federate migration; Load management; Grid computing</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=865>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using Web services to integrate heterogeneous simulations in a grid environment</paper_heading>
	<authors>J. Mark Pullen, Ryan Brunton, Don Brutzman, David Drake, ... Andreas Tolk</authors>
	<abstract>The distributed information technologies collectively known as Web services recently have demonstrated powerful capabilities for scalable interoperation of heterogeneous software across a wide variety of networked platforms. This approach supports a rapid integration cycle and shows promise for ultimately supporting automatic composability of services using discovery via registries. This paper presents a rationale for extending Web services to distributed simulation environments, together with a description and examples of the integration methodology used to develop significant prototype implementations, and argues for combining the power of Grid computing with Web services to further expand this demanding computation and database access environment.</abstract>
	<keywords>Web services; Grid computing; Distributed simulation; Composability</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=866>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Novel mediator architectures for Grid information systems</paper_heading>
	<authors>Alexander Wöhrer, Peter Brezany, A. Min Tjoa</authors>
	<abstract>Virtualization is one of the key features of the Grid. As data-intensive applications gain on importance, it becomes crucial for the success of the Grid to provide transparent access to distributed/heterogeneous data sources as well. In this paper, we describe our concepts to provide a Virtual Data Source (VDS) – a logically single homogeneous data source – on the Grid. Various data sources like relational/XML databases and comma separated value files can be combined via a Mapping Schema. By allowing to invoke user defined functions during the mediation a flexible way of resolving heterogeneities is provided. To show the feasibility of the developed concepts and elaborate the basis for a dynamic version they have been seamlessly integrated into OGSA-DAI to provide a non-proprietary, centralized and easy to integrate solution.</abstract>
	<keywords>Data integration; Grid computing; User defined functions; Mapping schema</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=867>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>File-based replica management</paper_heading>
	<authors>Peter Kunszt, Erwin Laure, Heinz Stockinger, Kurt Stockinger</authors>
	<abstract>Data replication is one of the best known strategies to achieve high levels of availability and fault tolerance, as well as minimal access times for large, distributed user communities using a world-wide Data Grid. In certain scientific application domains, the data volume can reach the order of several petabytes; in these domains, data replication and access optimization play an important role in the manageability and usability of the Grid. In this paper, we present the design and implementation of a replica management Grid middleware that was developed within the EDG project [European DataGrid Project (EDG), http://www.eu-egee.org] and is designed to be extensible so that user communities can adjust its detailed behavior according to their QoS requirements.</abstract>
	<keywords>Data Grid; Replica management; Replica access optimization</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=868>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards new load-balancing schemes for structured peer-to-peer grids</paper_heading>
	<authors>Carles Pairot, Pedro García, Antonio F. Gómez Skarmeta, Rubén Mondéjar</authors>
	<abstract>In this paper, we introduce the concept of a structured peer-to-peer grid and present our contribution to this new world by means of Dermi, a wide-area decentralized object middleware. In addition, we focus on the design and implementation of a load-balancing facility by using the functionalities provided by this middleware. We present two different approaches to achieve load-balancing in our system: a completely decentralized solution by using the anycall abstraction, and a distributed interceptor-based one. Each of them can be used in a wide variety of scenarios, depending on our needs.</abstract>
	<keywords>Structured peer-to-peer grid; Peer-to-peer middleware; Load-balancing</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=869>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Grid load balancing using intelligent agents</paper_heading>
	<authors>Junwei Cao, Daniel P. Spooner, Stephen A. Jarvis, Graham R. Nudd</authors>
	<abstract>Scalable management and scheduling of dynamic grid resources requires new technologies to build the next generation intelligent grid environments. This work demonstrates that AI techniques can be utilised to achieve effective workload and resource management. A combination of intelligent agents and multi-agent approaches is applied to both local grid resource scheduling and global grid load balancing. Each agent is a representative of a local grid resource and utilises predictive application performance data with iterative heuristic algorithms to engineer local load balancing across multiple hosts. At a higher level, agents cooperate with each other to balance workload using a peer-to-peer service advertisement and discovery mechanism.</abstract>
	<keywords>Load balancing; Grid computing; Intelligent agents; Genetic algorithm; Service discovery</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=870>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Adaptive grid job scheduling with genetic algorithms</paper_heading>
	<authors>Yang Gao, Hongqiang Rong, Joshua Zhexue Huang</authors>
	<abstract>This paper proposes two models for predicting the completion time of jobs in a service Grid. The single service model predicts the completion time of a job in a Grid that provides only one type of service. The multiple services model predicts the completion time of a job that runs in a Grid which offers multiple types of services. We have developed two algorithms that use the predictive models to schedule jobs at both system level and application level. In application-level scheduling, genetic algorithms are used to minimize the average completion time of jobs through optimal job allocation on each node. The experimental results have shown that the scheduling system using the adaptive scheduling algorithms can allocate service jobs efficiently and effectively.</abstract>
	<keywords>Service Grid; Adaptive job scheduling; Prediction; Genetic algorithms</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=871>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A taxonomy of grid monitoring systems</paper_heading>
	<authors>Serafeim Zanikolas, Rizos Sakellariou</authors>
	<abstract>Monitoring is the act of collecting information concerning the characteristics and status of resources of interest. Monitoring grid resources is a lively research area given the challenges and manifold applications. The aim of this paper is to advance the understanding of grid monitoring by introducing the involved concepts, requirements, phases, and related standardisation activities, including Global Grid Forum’s Grid Monitoring Architecture. Based on a refinement of the latter, the paper proposes a taxonomy of grid monitoring systems, which is employed to classify a wide range of projects and frameworks. The value of the offered taxonomy lies in that it captures a given system’s scope, scalability, generality and flexibility. The paper concludes with, among others, a discussion of the considered systems, as well as directions for future research.</abstract>
	<keywords>Grid monitoring; Grid Information Services; Grid Monitoring Architecture</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=872>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Extended resource space model</paper_heading>
	<authors>Hai Zhuge, Erlin Yao, Yunpeng Xing, Jie Liu</authors>
	<abstract>The resource space model (RSM) is a semantic data model based on orthogonal classification semantics for efficiently managing various resources in the future interconnection environment. This paper extends the RSM in theory by formalizing the resource space, investigating its characteristics from the perspective of set theory, defining the resource space schema and developing its normal forms. The topological space properties of the resource space are presented based on the definition of a distance in the space and the construction of a quotient space structure. The proposed theory ensures the RSM to correctly and efficiently specify and manage resources.</abstract>
	<keywords>Normal forms; Resource space model; Resource space schema; Topological space</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=873>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Active and logistical networking for grid computing: the e-Toile architecture</paper_heading>
	<authors>Alessandro Bassi, Micah Beck, Jean-Patrick Gelas, Laurent Lefèvre, ... Pascale Vicat-Blanc Primet</authors>
	<abstract>While active networks provide new solutions for the deployment of dynamic services in the network, exposing network processing resources, logistical networking focuses on exposing storage resources inside networks, optimizing the global scheduling of data transport, data storage and computation. In this paper, we show how active and logistical environments working together can improve grid middleware, and provide new and innovative high-level services for grid applications. We have experimented with this approach by combining the Internet Backplane Protocol suite with the Tamanoir Active Node environment. Our target architecture is the French e-Toile Grid infrastructure based on a high performance backbone (Vraiment Trés Haut Débit, VTHD).</abstract>
	<keywords>Active networks; Logistical storage; Gride-Toile</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=874>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>P2P/Grid-based overlay architecture to support VoIP services in large-scale IP networks</paper_heading>
	<authors>Wei Yu, Sriram Chellappan, Dong Xuan</authors>
	<abstract>Communication services such as, Voice over IP (VoIP), over the Internet have gained much attention in recent years, as next generation Internet applications are requiring the integration of voice and data in the single IP infrastructure. In this paper, we propose a Peer-to-Peer (P2P)/Grid-based architecture to efficiently provide VoIP services in large-scale IP networks. Particularly, our technologies include: (1) a multi-overlay architecture that leverages the resources/capabilities of individual VoIP components. (2) We design and develop several models and protocols for realizing VoIP services in our architecture. We conduct extensive performance evaluations on different schemes proposed. The evaluation results show that the load-aware weight-based call routing scheme can achieve much better performance than static selection schemes in terms of average call routing delay. The experimental results also demonstrate that the P2P + Hierarchy model for conferencing applications can achieve better performance than all other models in terms of minimizing the network bandwidth overhead.</abstract>
	<keywords>Communication services; Grid computing; P2P-based computing; Overlay; VoIP</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=875>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards OGSA compatibility in the H2O metacomputing framework</paper_heading>
	<authors>Gunther Stuer, Vaidy Sunderam, Jan Broeckhove</authors>
	<abstract>Lately, grid research has focused its attention on interoperability and standards, such as Grid Services, in order to facilitate resource virtualization, and to accommodate the intrinsic heterogeneity of resources in distributed environments. To ensure interoperability with other grid solutions, it is important that new and emerging metacomputing frameworks conform to these standards. In particular, the H2O-system offers several benefits, including lightweight operation, user-configurability, and selectable security levels; with OGSA compliance, its applicability would be enhanced even further. In this contribution, a framework is presented which will augment the H2O-system with the functionality to produce and publish WSDL- and GSDL-documents for arbitrary third-party pluglets, and thereby enhance OGSA compatibility.</abstract>
	<keywords>OGSA; H2O; Distributed computing; Grid</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=876>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GHolo: a multiparadigm model oriented to development of grid systems</paper_heading>
	<authors>Jorge Luis Victória Barbosa, Cristiano André da Costa, Adenauer Corrêa Yamin, Cláudio Fernando Resin Geyer</authors>
	<abstract>Multiparadigm approach integrates programming language paradigms. We propose Holoparadigm (Holo) as a multiparadigm model oriented to development of grid systems. Holo uses a logic blackboard (called history) to implement a coordination mechanism. The programs are organized in levels using abstract entities called beings. First, we describe the principal concepts of the Holoparadigm. After, the principles of a language based on the Holoparadigm are presented. Besides, we propose the Grid Holo (GHolo), a platform to support the multi-domain heterogeneous distributed computing of programs developed in Holo. GHolo is based on object mobility and blackboards. This distributed model can be fully implemented on Java platform.</abstract>
	<keywords>Multiparadigm; Mobility; Blackboard; Grid systems</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=877>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>JavaSymphony, a programming model for the Grid</paper_heading>
	<authors>Alexandru Jugravu, Thomas Fahringer</authors>
	<abstract>In previous work, JavaSymphony has been introduced as a high-level programming model for performance-oriented distributed and parallel Java programs. We have extended JavaSymphony to simplify the development of Grid applications, allowing the programmer to control parallelism, load balancing, and locality at a high level of abstraction. In this paper, we introduce new features to support the widely popular workflow paradigm. The overall idea is to provide a high-level programming paradigm that shields the programmer from low level implementation details such as RMI, sockets, and threads. Experiments are presented to demonstrate the usefulness of JavaSymphony as a programming paradigm for the Grid.</abstract>
	<keywords>Java-based Grid programming paradigms; Workflow scheduling</keywords>
	<publication_month_year>2005-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 1</volumes_issues>
</paper>
<paper no=878>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The INFN-Grid Testbed</paper_heading>
	<authors>R. Alfieri, R. Barbera, P. Belluomo, A. Cavalli, ... G. Vita Finzi</authors>
	<abstract>he Italian INFN-Grid Project is committed to set-up, run and manage an unprecedented nation-wide Grid infrastructure. The implementation and use of this INFN-Grid Testbed is presented and discussed. Particular care and attention are devoted to those activities, relevant for the management of the Testbed, carried out by the INFN within international Grid Projects.</abstract>
	<keywords>INFN-Grid; Grid computing; Testbed; Virtual Organizations</keywords>
	<publication_month_year>2005-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 2</volumes_issues>
</paper>
<paper no=879>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The Astrophysics Simulation Collaboratory Portal: a framework for effective distributed research</paper_heading>
	<authors>Ruxandra Bondarescu, Gabrielle Allen, Gregory Daues, Ian Kelley, ... Malcolm Tobias</authors>
	<abstract>We describe the Astrophysics Simulation Collaboratory (ASC) Portal, a collaborative environment in which distributed projects can perform research. The ASC project seeks to provide a web-based problem solving framework for the astrophysics community to harness Computational Grids. To facilitate collaboration amongst distributed researchers within a virtual organization, the ASC Portal supplies specialized tools for the management of large-scale numerical simulations and the resources on which they are performed. The ASC virtual organization uses the Cactus framework for studying numerical relativity and astrophysics phenomena. We describe the architecture of the ASC Portal and present its components with emphasis on elements related to the Cactus framework.</abstract>
	<keywords>Grid computing; Globus; Cactus; Astrophysics Simulation Collaboratory Portal; GridLab</keywords>
	<publication_month_year>2005-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 2</volumes_issues>
</paper>
<paper no=880>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Heuristic scheduling for bag-of-tasks applications in combination with QoS in the computational grid</paper_heading>
	<authors>Chuliang Weng, Xinda Lu</authors>
	<abstract>The computational grid provides a promising platform for the deployment of various high-performance computing applications. One issue in implementing computational grid environments is how to effectively utilize various resources in the system, such as CPU cycle, memory, communication network, and data storage. A heuristic Qsufferage is presented to schedule the bag-of-tasks application in the grid environment. The algorithm considers the location of each task’s input data, while makespan and response ratio are chosen as metrics for performance evaluation. The result of the experiment shows that Qsufferage algorithm can obtain better performance compared to the other four existing algorithms.</abstract>
	<keywords>Computational grid; Mapping strategy; Heuristic algorithm</keywords>
	<publication_month_year>2005-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 2</volumes_issues>
</paper>
<paper no=881>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Decentralised application placement</paper_heading>
	<authors>Colin Low</authors>
	<abstract>The broadening of the Grid architecture to include commercial as well as scientific workloads raises the possibility of commercial datacentres allocating computational resources on demand. The Utility Datacentre provides a large pool of computational resources that are allocated to service applications on request. When a service application consists of several components, these may be allocated physical computational resources which are distributed across the datacentre. The quality of placement determines how well or badly the application consumes finite global resources, such as bandwidth in the network backbone. This paper examines the placement problem, develops metrics for placement quality, and provides a decentralised approach to improving placement quality based on rounds of resource trading.</abstract>
	<keywords>Grid; Utility computing; Resource allocation; Resource optimisation</keywords>
	<publication_month_year>2005-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 2</volumes_issues>
</paper>
<paper no=882>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A scalable authorization approach for the Globus grid system</paper_heading>
	<authors>Jiageng Li, David Cordes</authors>
	<abstract>Grid computing has received widespread attention in recent years as a significant new research field. To date, there has been only limited work on the problem of grid system authorization. In this paper, we address the authorization problem in grid system environments and propose a solution for authorization within Globus system. Our authorization approach is based on distributed authorization servers and extensions to Globus’ Metacomputing Directory Service (MDS). The goal is to provide a scalable authorization approach that is able to meet the requirements of a dynamic grid environment.</abstract>
	<keywords>Globus; Grid system; Security; Authorization</keywords>
	<publication_month_year>2005-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 2</volumes_issues>
</paper>
<paper no=883>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Enhancing grid capabilities: IBP over IPv6</paper_heading>
	<authors>Alessandro Bassi, Micah Beck, Julien Laganier, Gabriella Paolini</authors>
	<abstract>Grid computing is considered as the ultimate solution for the scientific computing community. The main accent was first put on the computational side, as the other factors, such as network and storage, were considered as given. Logistical Networking started to fill the gap in the research of storage aspects of grid, and is now widely accepted and known as a mechanism to manage distributed storage resources, in an Internet-style way. In this paper we would like to show the advantages that the adoption of IPv6 can provide to the Internet Backplane Protocol (IBP), that can be considered the foundation stone of Logistical Networking, and therefore to grid systems. On top of it, IBP allows mixed IPv4/IPv6 environments to coexist, providing a very elegant way of moving data between grids based on different networking protocols.</abstract>
	<keywords>IPv6; Grid computing; Logistical networking; IBP</keywords>
	<publication_month_year>2005-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 2</volumes_issues>
</paper>
<paper no=884>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Designing and evaluating an active grid architecture</paper_heading>
	<authors>F. Bouhafs, J.P. Gelas, L. Lefèvre, M. Maimour, ... B. Tourancheau</authors>
	<abstract>Today’s computational grids are using the standard IP routing functionality, that has basically remained unchanged for 2 decades, considering the network as a pure communication infrastructure. With the grid’s distributed system point of view, one might consider to extend the commodity Internet’s basic functionalities. Higher value functionalities can thus be offered to computational grids. In this paper, we report on our early experiences in building application-aware components and in defining an active grid architecture that would bring the usage of computational grid to a higher level than it is now (mainly batch submission of jobs). To illustrate the potential of this approach, we first present how such application-aware components could be built and then some experiments on deploying enhanced communication services for the grid. We will show how reliable multicast and QoS mechanisms could deploy specific services based on the grid application needs.</abstract>
	<keywords>Active grid architecture; Computational grids; QoS; Active multicast</keywords>
	<publication_month_year>2005-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 2</volumes_issues>
</paper>
<paper no=885>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>P2P contracts: a framework for resource and service exchange</paper_heading>
	<authors>Dipak Ghosal, Benjamin K. Poon, Keith Kong</authors>
	<abstract>A crucial aspect of Peer-to-Peer (P2P) systems is that of providing incentives for users to contribute their resources to the system. Without such incentives, empirical data show that a majority of the participants act as free riders. As a result, a substantial amount of resource goes untapped, and, frequently, P2P systems devolve into client–server systems with attendant issues of performance under high load. We propose to address the free rider problem by introducing the notion of a P2P contract. In it, peers are made aware of the benefits they receive from the system as a function of their contributions. In this paper, we first describe a utility-based framework to determine the components of the contract and formulate the associated resource allocation problem. We consider the resource allocation problem for a flash crowd scenario and show how the contract mechanism implemented using a centralized server can be used to quickly create pseudoservers that can serve out the requests. We then study a decentralized implementation of the P2P contract scheme in which each node implements the contract based on local demand. We show that in such a system, other than contributing storage and bandwidth to serve out requests, it is also important that peer nodes function as application-level routers to connect pools of available pseudoservers. We study the performance of the distributed implementation with respect to the various parameters including the terms of the contract and the triggers to create pseudoservers and routers.</abstract>
	<keywords>P2P networks; Incentives; P2P contracts; Resource exchange; Flash crowds; File sharing</keywords>
	<publication_month_year>2005-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 3</volumes_issues>
</paper>
<paper no=886>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A distributed JXTA-based architecture for searching and retrieving solar data</paper_heading>
	<authors>A. Sanna, C. Zunino, L. Ciminiera</authors>
	<abstract>This paper describes a hybrid distributed architecture, based on the JXTA technology, for searching and discovering data in a federation of solar observation archives. This work has been developed within the European project EGSO (European Grid of Solar Observations) with the intent of studying and analyzing a set of technologies and showing the brokerage capabilities of a highly controlled distributed network topology. Three roles have been identified: providers that contain data and metadata, brokers that manage providers and user queries, and consumers/clients. JXTA is used to develop all network system functionalities, while a native XML database is used to store resource descriptions.</abstract>
	<keywords>Archives federation; Highly controlled distributed topologies; JXTA; Resource sharing</keywords>
	<publication_month_year>2005-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 3</volumes_issues>
</paper>
<paper no=887>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DINCast: a hop efficient dynamic multicast infrastructure for P2P computing</paper_heading>
	<authors>H.Q. Guo, L.H. Ngoh, W.C. Wong, J.G. Tan</authors>
	<abstract>Efficient communications among computers participating in peer-to-peer (P2P) computing remains a problem when they are geographically distributed across multiple network domains. In this paper, we propose DINCast to support group communications required in P2P computing. DINCast is built on top of an existing P2P multicast tree and results in a hop efficient dynamic multicast infrastructure. We show that DINCast is able to reduce communications delay compared with the original tree-based multicast. DINCast is also advantageous since it can achieve load balance and data persistency.</abstract>
	<keywords>DINCast; Tree-based multicast; DIN-Loop; Hop count; Peer-to-peer computing</keywords>
	<publication_month_year>2005-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 3</volumes_issues>
</paper>
<paper no=888>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The JXTA performance model and evaluation</paper_heading>
	<authors>Emir Halepovic, Ralph Deters</authors>
	<abstract>Project JXTA is an open-source effort to formulate and implement the core peer-to-peer (p2p) networking and collaboration protocols. A JXTA peer network is a complex overlay, constructed on top of the physical network, with its own identification scheme and routing. This paper reviews the performance of the JXTA networks using benchmarking, based on the proposed performance model. The two major versions of the JXTA protocol implementations (Versions 1.0 and 2.0) are surveyed and discussed.</abstract>
	<keywords>JXTA; Peer-to-peer; Performance; Platform</keywords>
	<publication_month_year>2005-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 3</volumes_issues>
</paper>
<paper no=889>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Authenticity and availability in PIPE networks</paper_heading>
	<authors>Brian F. Cooper, Mayank Bawa, Neil Daswani, Sergio Marti, Hector Garcia-Molin</authors>
	<abstract>We describe a system, which we call a peer-to-peer information preservation and exchange (PIPE) network, for protecting digital data collections from failure. A significant challenge in such networks is ensuring that documents are replicated and accessible despite malicious sites which may delete data, refuse to serve data, or serve an altered version of the data. We enumerate the services of PIPE networks, discuss a threat model for malicious sites, and propose basic solutions for managing these malicious sites. The basic solutions are inefficient, but demonstrate that a secure system can be built. We also sketch ways to improve efficiency.</abstract>
	<keywords>Digital preservation; Malicious sites; Failures</keywords>
	<publication_month_year>2005-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 3</volumes_issues>
</paper>
<paper no=890>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Message-based cellular peer-to-peer grids: foundations for secure federation and autonomic services</paper_heading>
	<authors>Geoffrey Fox, Sang Lim, Shrideep Pallickara, Marlon Pierce</authors>
	<abstract>We examine the creation of peer-to-peer Grids in which autonomous Grid components (Gridlets) may be federated into dynamic Grid collections. We examine two important aspects of these systems: federated security and autonomic considerations. As we discuss, both may be implemented using a messaging system as a substrate. Messaging systems provide the means of bridging Gridlet realms, organizing access control areas, and providing autonomic building blocks like discovery, reliability, resilience, and robustness of the peer-to-peer Grid.</abstract>
	<keywords>Grid federation; Publish/subscribe systems; Reliable invocation and messaging; Access control</keywords>
	<publication_month_year>2005-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 3</volumes_issues>
</paper>
<paper no=891>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computing on large-scale distributed systems: XtremWeb architecture, programming models, security, tests and convergence with grid</paper_heading>
	<authors>Franck Cappello, Samir Djilali, Gilles Fedak, Thomas Herault, ... Oleg Lodygensky</authors>
	<abstract>Global Computing systems belong to the class of large-scale distributed systems. Their properties high computational, storage and communication performance potentials, high resilience make them attractive in academia and industry as computing infrastructures in complement to more classical infrastructures such as clusters or supercomputers. However, generalizing the use of these systems in a multi-user and multi-parallel programming context involves finding solutions and providing mechanisms for many issues such as programming bag of tasks and message passing parallel applications, securing the application, the system itself and the computing nodes, deploying the systems for harnessing resources managed in different ways. In this paper, we present our research, often influenced by user demands, towards a Computational peer-to-peer system called XtremWeb. We describe (a) the architecture of the system and its motivations, (b) the parallel programming paradigms available in XtremWeb and how they are implemented, (c) the deployment issues and what mechanisms are used to harness simultaneously uncoordinated set of resources, and resources managed by batch schedulers and (d) the security issue and how we address, inside XtremWeb, the protection of the computing resources. We present two multi-parametric applications to be used in production: Aires belonging to the high energy physics (HEP) Auger project and a protein conformation predictor using a molecular dynamic simulator. To evaluate the performance and volatility tolerance, we present experiment results for bag of tasks applications and message passing applications. We show that the system can tolerate massive failure and we discuss the performance of the node protection mechanism. Based on the XtremWeb project developments and evolutions, we will discuss the convergence between Global Computing systems and Grid.</abstract>
	<keywords>Xtrem; WebLarge-scale distributed systems; Global Computing systems</keywords>
	<publication_month_year>2005-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 3</volumes_issues>
</paper>
<paper no=892>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The DataTAG transatlantic testbed</paper_heading>
	<authors>O. Martin, J.P. Martin-Flatin, E. Martelli, P. Moroni, ... D. Nae</authors>
	<abstract>Wide area network testbeds allow researchers and engineers to test out new equipment, protocols and services in real-life situations, without jeopardizing the stability and reliability of production networks. The Data TransAtlantic Grid (DataTAG) testbed, deployed in 2002 between CERN, Geneva, Switzerland and StarLight, Chicago, IL, USA, is probably the largest testbed built to date. Jointly managed by CERN and Caltech, it is funded by the European Commission, the U.S. Department of Energy and the U.S. National Science Foundation. The main objectives of this testbed are to improve the Grid community's understanding of the networking issues posed by data-intensive Grid applications over transoceanic gigabit networks, design and develop new Grid middleware services, and improve the interoperability of European and U.S. Grid applications in High-Energy and Nuclear Physics. In this paper, we give an overview of this testbed, describe its various topologies over time, and summarize the main lessons learned after two years of operation.</abstract>
	<keywords>Gigabit wide area networks; Grid networking; Testbed</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=893>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Native 10 Gigabit Ethernet experiments over long distances</paper_heading>
	<authors>Catalin Meirosu, Piotr Golonka, Andreas Hirstius, Stefan Stancu, ... Cees de Laat</authors>
	<abstract>The current solutions for transmitting data over Wide Area Networks (WANs) are expensive and require protocol translation at layer 1. The IEEE recently standardized the 10 Gigabit Ethernet (10 GE) WAN PHY as a native gateway from the Local Area Networks (LAN) to the WAN. This opened a debate as to whether Ethernet is now a valid alternative to Synchronous Optical Network/Synchronous Digital Hierarchy (SONET/SDH) for WANs. In this article, we report on the experience gathered while building the first trans-European native 10 Gigabit Ethernet testbed based on WAN PHY. We describe and analyze network tests with a 1700 km Ethernet network. Our work validates this approach and indicates that Ethernet can offer a large bandwidth to long-distance bulk data transfers at a trans-European level.</abstract>
	<keywords>10 Gigabit Ethernet; SONET/SD; HWAN PHY; Long-distance TCP</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=894>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance of 1 and 10 Gigabit Ethernet cards with server quality motherboards</paper_heading>
	<authors>Richard Hughes-Jones, Peter Clarke, Steven Dallison</authors>
	<abstract>System administrators often assume that just by plugging in a Gigabit Ethernet Interface, the system will deliver line rate performance; sadly this is not often the case. The behaviour of various 1 and 10 Gigabit Ethernet Network Interface cards (NICs) has been investigated using server quality motherboards. The latency, throughput, and the activity on the PCI buses and Gigabit Ethernet links were chosen as performance indicators. The tests were performed using two PCs connected back-to-back and sending UDP/IP frames from one to the other. This paper shows the importance of having a good combination of memory and peripheral bus chipset, Ethernet Interface, CPU power, good driver, and operating system designs and proper configuration to achieve and sustain gigabit transfer rates. With these considerations taken into account, and suitably designed hardware, transfers can operate at gigabit and multi-gigabit speeds. Some recommendations are given for high performance data servers.</abstract>
	<keywords>Gigabit networking; Gigabit Ethernet; Network Interface card; UDP; PCI bus; PCI-X bus</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=895>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Exploring practical limitations of TCP over transatlantic networks</paper_heading>
	<authors>Antony Antony, Johan Blom, Cees de Laat, Jason Lee</authors>
	<abstract>Tomorrow’s large physics and astronomy projects will require to transport tremendous amounts of data over long distances in near real time. Traditional TCP implementations have severe problems in reaching the necessary performance. In the recent past, researchers have shown that TCP implementations can be scaled to achieve multi-gigabit per second speeds over high-bandwidth high-delay networks. The ability of TCP to scale to high speeds opens possibilities for very large data transfers over vast distances. We analyze here whether TCP can fulfill this task and what problems we are faced with. We also examine TCP in the context of dedicated links (Lambdas).</abstract>
	<keywords>TCP; TCP response behavior; HSTCP; Grid networking</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=896>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Teraflows over Gigabit WANs with UDT</paper_heading>
	<authors>Robert L. Grossman, Yunhong Gu, Xinwei Hong, Antony Antony, ... Cees de Laat</authors>
	<abstract>The TCP transport protocol is currently inefficient for high speed data transfers over long distance networks with high bandwidth delay products (BDP). The challenge is to develop a protocol which is fast over networks with high bandwidth delay products, fair to other high volume data streams, and friendly to TCP-based flows. We describe here a UDP-based application level transport protocol named UDT (UDP-based Data Transfer) with these properties and which is designed to support distributed data-intensive computing applications. UDT can utilize high bandwidth efficiently over wide area networks with high bandwidth delay products. Unlike TCP, UDT is fair to flows independently of their round trip times (RTT). In addition, UDT is friendly to concurrent TCP flows, which means it can be deployed not only on experimental research networks but also on production networks. To ensure these properties, UDT employs a novel congestion control approach that combines rate-based and window-based control mechanisms. In this paper, we describe the congestion control algorithms used by UDT and provide some experimental results demonstrating that UDT is fast, fair, and friendly.</abstract>
	<keywords>UDT; Bandwidth delay product (BDP); Grid networking; Transport protocol; High speed WANs</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=897>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experiments with equivalent differentiated services in a grid context</paper_heading>
	<authors>Pascale Vicat-Blanc Primet, Franois Echantillac, Mathieu Goutelle</authors>
	<abstract>Grids generally rely on a complex interconnection of Internet Protocol (IP) domains that offer heterogeneous services and unpredictable performance characteristics, particularly at the local area network/wide area network boundary. The total lack of end-to-end resource control in IP networks is responsible for performance problems that may affect the whole Grid environment. An end-to-end service differentiation architecture that controls heterogeneous communication performance is thus needed. We propose the Equivalent Differentiated Services (EDS) architecture, based on a layer-4 service differentiation solution exploiting a new layer-3 relative DiffServ model. In this paper, we present the EDS packet forwarding principles, the router mechanisms and two adaptive packet marking algorithms. As a proof of concept, we have implemented the EDS architecture in Linux and performed experiments on a transoceanic testbed.</abstract>
	<keywords>Grid networking; End-to-end performance; Service differentiation; Quality of service; Adaptive packet marking; EDS</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=898>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On advance reservation of heterogeneous network paths</paper_heading>
	<authors>Chiara Curti, Tiziana Ferrari, Leon Gommans, S. van Oudenaarde, ... Cristina Vistoli</authors>
	<abstract>The availability of information about properties and status of resources is essential for Grid resource brokers. However, while abstractions of computing and storage resources already exist, the notion of Grid network resource is far from being understood today. As a result, the integration of advanced network services is still difficult when a Grid system spans large-scale heterogeneous network infrastructures. In this paper, we propose a single definition of a Grid network resource abstraction for multiple types of network connectivity. This abstraction was successfully implemented and tested in a network resource management prototype supporting a variety of network technologies.</abstract>
	<keywords>Advance reservation; Network resource management; AAA; MPLS; Differentiated Services; Light Path; Grid networking; GARA</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=899>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Dynamic paths in multi-domain optical networks for grids</paper_heading>
	<authors>S. van Oudenaarde, Z. Hendrikse, F. Dijkstra, L. Gommans, ... R. J Meijer</authors>
	<abstract>Many Grid applications require high bandwidth end-to-end connections between Grid resources in different domains. Fiber optic networks, owned by different providers, have to cooperate in a coordinated manner in order to provide an end-to-end connection. Currently, multi-domain optical network solutions require paper-based long-term contracts between administrative domains. This paper describes a solution for dynamically creating optical connections between different autonomous domains. This was implemented in the form of a Grid Service following the Open Grid Service Architecture. In our prototype, each switch belongs to a different network domain. Our Grid Service uses a toolkit based on the Generic Authorization, Authentication, and Accounting framework. This toolkit authorizes the use of optical infrastructure elements based on specific policies that are active within each domain. To complete our multi-domain authorization architecture, a Broker Service was also implemented. Our Broker Service interacts with the Grid Service instances to provide Grid application with a simplified way to set up end-to-end connections on demand.</abstract>
	<keywords>AAA; Multi-domain; Grid Service; Web Service; Bandwidth on Demand; Lightpath Switching; OGSA; OGSI</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=900>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>From gridmap-file to VOMS: managing authorization in a Grid environment</paper_heading>
	<authors>R. Alfieri, R. Cecchini, V. Ciaschini, L. dell’Agnello, ... F. Spataro</authors>
	<abstract>Grids are potentially composed of several thousands of users from different institutions sharing their computing resources (or using resources provided by third parties). Controlling access to these resources is a difficult problem, as it depends on the policies of the organizations the users belong to and of the resource owners. Moreover, a simple authorization implementation, based on a direct user registration on the resources, is not applicable to a large scale environment. In this paper, we describe the solution to this problem developed in the framework of the European DataGrid [M. Draoli, G. Mascari, R. Piccinelli, Project Presentation, DataGrid-11-NOT-0103-_1] and DataTAG [http://www.datatag.org/] projects: the Virtual Organization Membership Service (VOMS) [R. Alfieri, et al., Managing Dynamic User Communities in a Grid of Autonomous Resources, TUBT005, in: Proceedings of the CHEP 2003, 2003]. VOMS allows a fine grained control of the use of the resources both to the users’ organizations and to the resource owners.</abstract>
	<keywords>Grids; Authorization; Attributes</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=901>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GridICE: a monitoring service for Grid systems</paper_heading>
	<authors>Sergio Andreozzi, Natascia De Bortoli, Sergio Fantinel, Antonia Ghiselli, ... Maria Cristina Vistoli</authors>
	<abstract>Grid systems follow a new paradigm of distributed computing that enables the coordination of resources and services that are not subject to centralized control, can dynamically join and leave virtual pools, and are assigned to users by means of an explicit assignment functionality. The monitoring of a Grid is a multi-institutional and Virtual Organization (VO)-oriented service. It must deal with the dynamics, diversity, and geographical distribution of the resources available to Virtual Organizations, and the various levels of abstraction for modeling them. This paper presents the requirements, architecture and implementation of GridICE, a monitoring service for Grid systems. The suitability of this tool in real-life scenarios is analyzed and discussed.</abstract>
	<keywords>Grid computing; Distributed systems; Grid monitoring</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=902>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Real-time rendering of 3D medical data sets</paper_heading>
	<authors>Kai Xie, Jie Yang, Y.M. Zhu</authors>
	<abstract>Interactive exploration of three dimension (3D) medical data sets is required by many applications, but the huge amount of computational time and storage space needed for rendering do not allow the visualization of large medical data sets by now. In this paper we present a new algorithm for rendering large medical data sets at interactive frame rates on standard PC hardware. The input data is converted into a compressed hierarchical wavelet representation in a preprocessing step. During rendering, the wavelet representation is decompressed on-the-fly and rendered using hardware texture mapping. The level of detail used for rendering is adapted to the local frequency spectrum of the data and its position relative to the viewer. Using a prototype implementation of the algorithm we were able to perform an interactive walkthrough of large medical data sets on a single of-the-shelf PC.</abstract>
	<keywords>Compression algorithms; Level of detail; Volume rendering; Wavelet</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=903>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Validating key constraints over XML document using XPath and structure checking</paper_heading>
	<authors>Yunfeng Liu, Dongqing Yang, Shiwei Tang, Tengjiao Wang, Jun Gao</authors>
	<abstract>Although key definitions are adopted by XML Schema, validating hierarchical key constraints over XML document faces many difficult problems. The paper begins by proposing a systematic architecture for efficient XML key validation and incremental validation using XPath and structure checking. We propose a key value document generation algorithm and present how a key value document and its schema can be designed to check whether predefined key constraints are satisfied. Then we present a novel method that can support incremental validation of XML key constraints by incremental maintenance of key value document.</abstract>
	<keywords>XML; SchemaKey constraint; Xpath; Incremental validation</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=904>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Achieving high availability and performance computing with an HA-OSCAR cluster</paper_heading>
	<authors>Chokchai Box Leangsuksun, Lixin Shen, Tong Liu, Stephen L. Scott</authors>
	<abstract>High availability (HA) computing has long gained much attention in enterprise and mission critical systems. HA goals are to maximize the uptime, thus undoubtedly complementing high-performance computing (HPC) objectives. HA-OSCAR is a project that aims to improve HA in commercial-off-the-shelf (COTS)-based HPC environments. In this paper, we introduce a multi-head HPC cluster architecture. Server redundancy is an initial key aspect aiming toward downtime reduction. Two HA-OSCAR types, active–standby and active–active, are studied. We evaluate system dependability for given two models. Stochastic Reward Nets (SRN) are used to model the system availability. We describe our SRN modeling using Stochastic Petri Net Package, and compute several interesting results that characterize HA-OSCAR availability.</abstract>
	<keywords>Cluster computing; Petri net modeling; Stochastic Reward Nets; High availability; Dependability</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=905>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A greedy strategy for detecting negative cost cycles in networks</paper_heading>
	<authors>K. Subramani, L. Kovalchick</authors>
	<abstract>In this paper, we develop a greedy strategy for the problem of checking whether a network (directed graph) with positive and negative costs on its edges has a negative cost cycle. We call our approach the Vertex Contraction algorithm; it is the first known greedy strategy for this problem. As per the literature, all known approaches to the problem of detecting negative cost cycles are based on dynamic programming or scaling. It is well known that the negative cost cycle detection problem is equivalent to the problem of checking whether a system of linear difference constraints is feasible. Our algorithm exploits this equivalence and uses polyhedral projection on the polyhedron of linear difference constraints, corresponding to the input network, to detect the presence of negative cost cycles. We use a variant of the Fourier–Motzkin elimination procedure to effect polyhedral projection and contrast the performance of our algorithm with the “standard” Bellman–Ford algorithm for the same problem. We observed that the Vertex Contraction algorithm performed an order of magnitude better than the Bellman–Ford algorithm on a range of randomly generated inputs, thereby conclusively demonstrating the superiority of our approach. From the perspective of asymptotic analysis, the Bellman–Ford algorithm is superior to our algorithm, in that it runs in time O(m⋅n) in the worst case, on a network with m edges and n vertices, whereas the worst-case complexity of our algorithm is O(n3).</abstract>
	<keywords>Vertex Contraction; Bellman–Ford; Relaxation; Negative cost cycle detection</keywords>
	<publication_month_year>2005-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 4</volumes_issues>
</paper>
<paper no=906>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Supporting the application of Situated Cellular Agents in non-uniform spaces</paper_heading>
	<authors>Stefania Bandini, Sara Manzoni, Carla Simone</authors>
	<abstract>The paper presents Situated Cellular Agents (SCA), systems of reactive agents that are heterogeneous and populate a structured environment. The structure of this environment is defined as a non-uniform network of sites in which agents are situated. The behavior of SCA agents (i.e. change of state and position) is influenced by states and types of agents that are situated in adjacent and at-a-distance sites. After an outline of SCA approach and its main applications, the paper introduces an ongoing project to develop a tool to support the development and execution of SCA applications.</abstract>
	<keywords>Multi Agent Systems; MAS-based modelling; Structured environment; Development environment</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=907>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computation properties of spatial dynamics simulation by probabilistic cellular automata</paper_heading>
	<authors>Olga L. Bandman</authors>
	<abstract>Accuracy, stability and computation complexity of fine-grained parallel simulation of spatial dynamics by probabilistic cellular automata (CA), are assessed and experimentally studied. Under investigation are probabilistic CA constructed as a composition of an ordinary CA with a function given in real numbers. The accuracy problem is reduced to approximation error assessment of the transformation of a real spatial function into a Boolean array and addition of cellular arrays with different cell state alphabets: real and Boolean. Some techniques for determining simulation parameters which provide a given accuracy are given. Stability is shown to be dependent only on real function, the CA component of the dynamics having no effect on it. Computation complexity of simulation process is also assessed. Some experimental results supporting the theoretical conclusions are presented.</abstract>
	<keywords>Fine-grained parallelism; Cellular automaton; Spatial dynamics simulation</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=908>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Authentication primitives for secure protocol specifications</paper_heading>
	<authors>C. Bodei, P. Degano, R. Focardi, C. Priami</authors>
	<abstract>We use two authentication primitives proposed recently as a linguistic support for enforcing authentication. They offer a way of abstracting from various specifications of authentication and of obtaining idealized protocols “secure by construction”. Consequently, they help in proving that a cryptographic protocol correctly implements its corresponding abstract version; when the implementation is incorrect, suggestions on how to fix it may come from reasoning on the abstract specification.</abstract>
	<keywords>Cryptographic process calculi; Security; Authentication</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=909>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Wait-free computing: an introductory lecture</paper_heading>
	<authors>Michel Raynal</authors>
	<abstract>This paper is a short introduction to wait-free computing. “Wait-free” means that the progress of a process depends only on it, regardless of the other processes (that can progress slowly or even crash). To illustrate wait-free computing, the paper considers the design of two concurrent objects, namely, a renaming object and a snapshot object. A renaming object allows the processes to acquire new names from a smaller name space despite possible process crashes. A snapshot object provides the processes with an array-like data structure (with one entry per process) offering two operations. The write operation allows a process to update its own entry. The snapshot operation allows a process to read all the entries in such a way that the reading of the whole array appears as it is was an atomic operation. A renaming protocol by Moir and Anderson and a snapshot protocol by Afek et al. are used to illustrate the beauty and subtleties of wait-free computing.</abstract>
	<keywords>Atomic register; Concurrency; Fault-tolerance; Non-blocking synchronization; Process crash; Renaming problem; Shared memory system; Snapshot problem; Wait-free computation</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=910>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A static analysis for Bulk Synchronous Parallel ML to avoid parallel nesting</paper_heading>
	<authors>F. Gava, F. Loulergue</authors>
	<abstract>The BSMLlib library is a library for Bulk Synchronous Parallel (BSP) programming with the functional language Objective Caml. It is based on an extension of the λ-calculus by parallel operations on a data structure named parallel vector, which is given by intention. In order to have an execution that follows the BSP model, and to have a simple cost model, nesting of parallel vectors is not allowed. The novelty of this paper is a type system which prevents such nesting. This system is correct w.r.t. the dynamic semantics.</abstract>
	<keywords>Bulk Synchronous Parallelism; Functional programming; Polymorphic type system</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=911>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel computing for globally optimal decision making on cluster systems</paper_heading>
	<authors>V.P. Gergel, R.G. Strongin</authors>
	<abstract>This paper presents a new scheme for parallel computations on cluster systems for time-consuming problems of globally optimal decision making. This uniform scheme (without any centralized control processor) is based on the idea of multidimensional problem reduction. Using same new multiple mappings (of the Peano curve type), a multidimensional problem is reduced to a family of univariate problems which can be solved in parallel in such a way that each of these processors shares the information obtained by the other processors.</abstract>
	<keywords>Global optimization; Space filling curves; Parallel computations</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=912>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Retargetable code generation for application-specific processors</paper_heading>
	<authors>A. Doroshenko, D. Ragozin</authors>
	<abstract>An approach of intelligent retargetable compiler is introduced to overcome the gap between hardware and software development and to increase performance of embedded systems. It focuses on knowledgeable treatment of code generation where knowledge about target microprocessor architecture and human-level heuristics are integrated into compiler expert system. Structure of an experimental compiler supporting code generation for DSPs and VLIW-DSPs is described. A technique to detect optimal instruction set architecture for program execution is presented. Results of code generation experiments are presented for DSPstone benchmarks.</abstract>
	<keywords>Instruction level parallelism; Retargetable compiler; DSP; VLIW; Processor architecture</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=913>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Modelling and implementing asynchronous timed multimedia frameworks using coordination principles</paper_heading>
	<authors>George A. Papadopoulos</authors>
	<abstract>This paper combines work done in the areas of Artificial Intelligence, Multimedia Systems and Coordination Programming to derive a framework for Distributed Multimedia Systems based on asynchronous timed computations expressed in a certain coordination formalism. More to the point, we propose the development of multimedia programming frameworks based on the declarative logic programming setting and in particular the framework of object-oriented timed concurrent constraint programming (OO-TCCP). The real-time extensions that have been proposed for the concurrent constraint programming framework are coupled with the object-oriented and inheritance mechanisms that have been developed for logic programs yielding an integrated declarative environment for multimedia objects modelling, composition and synchronisation. Furthermore, we show how the framework can be implemented in the general-purpose coordination language MANIFOLD without the need for using special architectures or real-time languages.</abstract>
	<keywords>Multimedia programming; Timed concurrent constraint programming; Timed asynchronous languages; Coordination models; Distributed computing</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=914>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Adapting Java RMI for grid computing</paper_heading>
	<authors>Martin Alt, Sergei Gorlatch</authors>
	<abstract>Computational grids allow the users to run their applications on remote high-performance servers available via Internet. Java is often used to develop portable grid applications, with programs being sequences (compositions) of remote method calls. We demonstrate an inherent inefficiency of the standard remote method invocation (RMI) mechanism of Java for implementing compositions of remote calls. We propose a new, optimised RMI mechanism, called future-based RMI, that substantially reduces the unnecessary communication overhead of method compositions in a grid environment. We present an analytical model for estimating the performance improvements achieved by our mechanism and report experimental results for two case studies on a grid testbed including a high-performance shared-memory server which is accessed from a client located 500 km away.</abstract>
	<keywords>Grid computing; Network enabled servers; Java RMI; Performance modeling; Futures</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=915>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A Channel Memory based fault tolerance for MPI applications</paper_heading>
	<authors>A. Selikhov, C. Germain</authors>
	<abstract>Fault tolerant message passing environments protect parallel applications against node failures. Very large scale computing systems, ranging from large clusters to worldwide Global Computing systems, require a high level of fault tolerance in order to efficiently run parallel applications. The Channel Memory approach provides the infrastructure for scalable tolerance to simultaneous faults. Along with a specially designed checkpointing system and recovery protocol, this approach has resulted in the MPICH-V architecture. In this paper, we describe CMDE – a stand-alone distributed program system based on MPICH-V architecture and implementing an approach to tolerate faults of Channel Memories.</abstract>
	<keywords>Channel Memory; Message passing interface; Fault tolerance; Global Computing; Grid</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=916>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An efficient incremental marshaling framework for distributed systems</paper_heading>
	<authors>K. Popov, V. Vlassov, P. Brand, S. Haridi</authors>
	<abstract>We present an efficient and incremental (un)marshaling framework designed for distributed applications. A marshaler/ unmarshaler pair converts arbitrary structured data between its host and network representations. This technology can also be used for persistent storage. Our framework simplifies the design of efficient and flexible marshalers. The network latency is reduced by concurrent execution of (un)marshaling and network operations. The framework is actually used in Mozart, a distributed programming system that implements Oz, a multi-paradigm concurrent language.</abstract>
	<keywords>Distributed computing; Marshaling; Concurrency; Latency; Throughput</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=917>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>DRAxML@home: a distributed program for computation of large phylogenetic trees</paper_heading>
	<authors>A. Stamatakis, M. Lindermeier, M. Ott, T. Ludwig, H. Meier</authors>
	<abstract>Inference of large phylogenetic trees using statistical methods is computationally extremely expensive. Thus, progress is primarily achieved via algorithmic innovation rather than by brute-force allocation of available computational ressources. We describe simple heuristics which yield accurate trees for synthetic (simulated) as well as real data and significantly improve execution time. The heuristics are implemented in a sequential program (RAxML) and a novel non-deterministic distributed algorithm (DRAxML@home). We implemented an MPI-based and a http-based distributed prototype of this algorithm and used DRAxML@home to infer trees comprising 1000 and 2025 organisms on LINUX PC clusters.</abstract>
	<keywords>High performance bioinformatics; Distributed computing; Phylogenetic tree inference; Maximum-likelihood method</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=918>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Gene transcript clustering: a comparison of parallel approaches</paper_heading>
	<authors>Todd E. Scheetz, Nishank Trivedi, Kevin T. Pedretti, Terry A. Braun, Thomas L. Casavant</authors>
	<abstract>One of the fundamental components of large-scale gene discovery projects is that of clustering of expressed sequence tags (ESTs) from complementary DNA (cDNA) clone libraries. Clustering is used to create non-redundant catalogs and indices of these sequences. In particular, clustering of ESTs is frequently used to estimate the number of genes derived from cDNA-based gene discovery efforts. This paper presents a novel parallel extension to an EST clustering program, UIcluster4, that incorporates alternative splicing information and a new parallelization strategy. The results are compared to other parallelized EST clustering systems in terms of overall processing time and in accuracy of the resulting clustering.</abstract>
	<keywords>Parallel Algorithms; Performance measurement; Genome Analysism; RNA clustering; Bioinformatics</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=919>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A cost-optimal parallel implementation of a tridiagonal system solver using skeletons</paper_heading>
	<authors>Holger Bischof, Sergei Gorlatch</authors>
	<abstract>We address the task of systematically designing efficient programs for parallel machines. Our approach starts with a sequential algorithm and proceeds by expressing it in terms of standard, pre-implemented parallel components called skeletons. We demonstrate the skeleton-based design process using a tridiagonal system solver as our example application. We develop a cost-optimal parallel version of our application and implement it in message passing interface (MPI). The performance of our solution is demonstrated experimentally on a Cray T3E machine.</abstract>
	<keywords>Tridiagonal systems solver; Skeletons; Cost optimality; Parallel programming</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=920>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Development of efficient computational kernels and linear algebra routines for out-of-order superscalar processors</paper_heading>
	<authors>O. Bessonov, D. Fougère, B. Roux</authors>
	<abstract>We present methods for developing high performance computational kernels and dense linear algebra routines. The microarchitecture of AMD processors is analyzed with the goal to achieve peak computational rates. Approaches for implementing matrix multiplication algorithms are suggested for hierarchical memory computers. Block versions of matrix multiplication and LU-decomposition algorithms are considered. The obtained performance results for AMD processors are discussed in comparison with other approaches.</abstract>
	<keywords>Instruction level parallelism; Out-of-order processor; Cache memory; Performance measurement; LINPACK benchmark</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=921>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Supercomputer simulation of self-gravitating media</paper_heading>
	<authors>E.A. Kuksheva, V.E. Malyshkin, S.A. Nikitin, A.V. Snytnikov, ... V.A. Vshivkov</authors>
	<abstract>A numerical 3D-model for investigation of non-stationary processes in a gravitating system with gas has been created. The model is based on the solution of the Poisson equation for gravitational field, the Vlasov–Liouville equation for solids and equations of gas dynamics. For solution of the Poisson equation at each timestep an efficient iterational solver is created with extrapolation of the evolutionary prosesses under study. It provides fast convergence at high precision. Parallelisation technique and load balancing strategy are discussed.</abstract>
	<keywords>Gravitational instability; PIC method; Parellel computing</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=922>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Partitioning finite element meshes using space-filling curves</paper_heading>
	<authors>Stefan Schamberger, Jens-Michael Wierum</authors>
	<abstract>Using space-filling curves to partition unstructured finite element meshes is a widely applied strategy when it comes to distributing load among several computation nodes. Compared to more elaborated graph partitioning packages, this geometric approach is relatively easy to implement and very fast. However, results are not expected to be as good as those of the latter. In this paper we present results of our experiments comparing the quality of partitionings computed with different types of space-filling curves to those generated with the graph partitioning package Metis.</abstract>
	<keywords>Distributed Finite Element Method; Graph partitioning; Space-filling curves</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=923>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The instruction register file micro-architecture</paper_heading>
	<authors>Bernard Goossens, David Defour</authors>
	<abstract>In this paper, we address the issue of feeding future superscalar processor cores with enough instructions. Hardware techniques targeting an increase in the instruction fetch bandwidth have been proposed such as the trace cache microarchitecture. We present a microarchitecture solution based on a register file holding basic blocks of instructions. This solution places the instruction memory hierarchy out of the cycle determining path. We call our approach, instruction register file (IRF). We estimate our approach with a SimpleScalar based simulator run on the Mediabench benchmark suite and compare to the trace cache performance on the same benchmarks. We show that on this benchmark suite, an IRF-based processor fetching up to three basic blocks per cycle outperforms a trace-cache-based processor fetching 16 instructions long traces by 25% on the average.</abstract>
	<keywords>Trace cache; Instruction memory; Register file; Fetch stage</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=924>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Dynamic replication algorithms for the multi-tier Data Grid</paper_heading>
	<authors>Ming Tang, Bu-Sung Lee, Chai-Kiat Yeo, Xueyan Tang</authors>
	<abstract>Data replication is a common method used to improve the performance of data access in distributed systems. In this paper, two dynamic replication algorithms, Simple Bottom-Up (SBU) and Aggregate Bottom-Up (ABU), are proposed for the multi-tier Data Grid. A multi-tier Data Grid simulator called DRepSim is developed for studying the performances of the dynamic replication algorithms. The simulation results show that both algorithms can reduce the average response time of data access greatly compared to the static replication method. ABU can achieve great performance improvements for all access patterns even if the available storage size of the replication server is very small. Comparing the two algorithms to Fast Spread dynamic replication strategy, ABU proves to be superior. As for SBU, although the average response time of Fast Spread is better in most cases, Fast Spread’s replication frequency is too high to be applicable in the real world.</abstract>
	<keywords>Replication; Data Grid; Distributed system; Simulation; Performance</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=925>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>JoiN: The implementation of a Java-based massively parallel grid</paper_heading>
	<authors>Eduardo Javier Huerta Yero, Fabiano de Oliveira Lucchese, Francisco Sérgio Sambatti, Miriam von Zuben, Marco Aurélio Amaral Henriques</authors>
	<abstract>This paper presents JoiN, a Java-based software platform to construct massively parallel grids capable of executing large parallel applications. The system is designed to be scalable by allowing computers in the grid to be separated in independent sets (called groups) which are managed independently and collaborate using a logical interconnection topology. JoiN provides advanced fault tolerance capabilities that allow it to withstand failures both in computers executing parallel tasks and in computers managing the groups. The parallel applications executing in the system are formally specified using a rigorously defined application model. JoiN uses a dynamic, flexible scheduling algorithm that adapts to changes in resource availability and replicates parallel tasks for fault tolerance. The platform provides an authentication/access control mechanism based on roles which is embedded in the inner parts of the system. The software architecture is based on the concept of services, which are independent pieces of software that can be combined in several ways, providing the flexibility needed to adapt to particular environments. JoiN has been successfully used to implement and execute several parallel applications, such as DNA sequencing, Monte Carlo simulations and a version of the Traveling Salesman Problem.</abstract>
	<keywords>Grids; Parallel computing; Internet computing</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=926>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Monte Carlo grid for financial risk management</paper_heading>
	<authors>Shu Tezuka, Hiroki Murata, Shuji Tanaka, Shoji Yumae</authors>
	<abstract>Due to reduced profitability, increased price competition, and strengthened regulation, financial institutions in all countries are now upgrading their financial analytics based on Monte Carlo simulation. In this article, we propose three key technologies, i.e., data protection, integrity, and deadline scheduling, which are indispensable to build a secure PC-grid for financial risk management. We constructed a PC-grid by scavenging unused CPU cycles of about 50 PCs under real office environment, and obtained the 80 times speed-up, namely, for 100,000 Monte Carlo scenarios, 95 h computation on a single server is reduced to 70 min. Finally, we discuss future research directions.</abstract>
	<keywords>Monte Carlo methods; Financial risk management; Basel II; Value at risk (VaR); Grid computing</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=927>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Cooperative control of multicast-based streaming on-demand systems</paper_heading>
	<authors>Giancarlo Fortino, Carlo Mastroianni, Wilma Russo</authors>
	<abstract>This paper proposes the COoperative COntrol Protocol (COCOP), which enables a synchronous cooperative group to interactively control an on-demand server which multicasts time-dependent data streams. Multicast-based streaming on-demand systems such as video on-demand systems, web casters, and networks of real/virtual sensors can beneficially exploit COCOP to provide cooperative control sessions as a mainstream service. In order to improve efficiency and scalability, the protocol relies on a reliable multicast transport layer which can be based either on the IP-multicast or on Application Layer Multicast. Performance evaluation of COCOP was carried out on multicast trees by using a discrete-event simulation framework.</abstract>
	<keywords>Multicasting; Cooperative playback systems; Synchronous group coordination; Discrete event simulation</keywords>
	<publication_month_year>2005-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 5</volumes_issues>
</paper>
<paper no=928>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Future trends in distributed applications and problem-solving environments</paper_heading>
	<authors>José C. Cunha, Omer F. Rana, Pedro D. Medeiros</authors>
	<abstract>As grid computing technologies and infrastructures are being developed, suitable abstractions, methods, and tools will become necessary to enable application development, and software development of the components of grid computing environments. Grid computing will enable distributed applications with large numbers of involved components with dynamic interactions. This requires new approaches to understand and manage structure and behaviour, and the diversity of interactions among system components. This paper discusses emerging trends in distributed applications on large-scale and dynamic grid computing infrastructures. These trends allow us to identify the need to develop suitable software models, methods and tools for grid computing environments, in order to help specify, compose, and develop dynamic distributed large-scale applications.</abstract>
	<keywords>Distributed applications; Grid software environments</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=929>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Building Problem-Solving Environments with Application Web Service toolkits</paper_heading>
	<authors>Choonhan Youn, Marlon E. Pierce, Geoffrey C. Fox</authors>
	<abstract>Application portals, or Problem-Solving Environments (PSEs), provide user environments that simplify access and integrate various distributed computational services for scientists working on particular classes of problems. Specific application portals are typically built on common sets of core services, so reusability of these services is a key problem in PSE development. In this paper we address the reusability problem by presenting a set of core services built using the Web services model and application metadata services that can be used to build science application front ends out of these core services and the management of multiple versions of services.</abstract>
	<keywords>Web services; Application toolkit; Computing portal; Portal interoperability; Web service negotiation</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=930>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GSiB: PSE infrastructure for dynamic service-oriented Grid applications</paper_heading>
	<authors>Yan Huang</authors>
	<abstract>This paper describes the Grid-Service-in-a-Box (GSiB) system—a visual environment for both service providers and service clients who wish to deploy, manage, and use services in a Grid environment, without having to be experts in Grid and Web service technologies. GSiB provides a variety of easy-to-use graphical user interfaces for service providers to deploy, undeploy, update, compose, and monitor services. In addition, service clients can generate service-composite applications, and track and monitor submitted jobs. This paper describes these interfaces and several implementation issues are addressed. An example service-based application is presented to illustrate how a workflow can be used to generate and execute an application on Grid infrastructure.</abstract>
	<keywords>Grid system; Problem-solving environment; Web service; Workflow</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=931>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Novel runtime systems support for adaptive compositional modeling in PSEs</paper_heading>
	<authors>Srinidhi Varadarajan, Naren Ramakrishna</authors>
	<abstract>Complex problem solving environments (PSEs) rely on runtime systems support for adaptive composition and control of scientific computations. This paper presents a PSE runtime support solution through a novel combination of two technologies—Weaves, a source language independent parallel compositional framework that operates through reverse-analysis of compiled object files, and runtime recommender systems that aid in dynamic knowledge-based application composition. Domain-specific adaptivity is realized by recommendation of code modules, supported by a sophisticated checkpointing framework for runtime control. A core set of “adaptivity schemas” are provided as templates for adaptive composition of large-scale scientific computations. Implementation issues, motivating application contexts, and preliminary results are described.</abstract>
	<keywords>Problem solving environments; Checkpointing; Recommender systems; Application composition systems; Adaptivity</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=932>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>From virtualized resources to virtual computing grids: the In-VIGO system</paper_heading>
	<authors>Sumalatha Adabala, Vineet Chadha, Puneet Chawla, Renato Figueiredo, ... Xiaomin Zhu</authors>
	<abstract>This paper describes the architecture of the first implementation of the In-VIGO grid-computing system. The architecture is designed to support computational tools for engineering and science research In Virtual Information Grid Organizations (as opposed to in vivo or in vitro experimental research). A novel aspect of In-VIGO is the extensive use of virtualization technology, emerging standards for grid-computing and other Internet middleware. In the context of In-VIGO, virtualization denotes the ability of resources to support multiplexing, manifolding and polymorphism (i.e. to simultaneously appear as multiple resources with possibly different functionalities). Virtualization technologies are available or emerging for all the resources needed to construct virtual grids which would ideally inherit the above mentioned properties. In particular, these technologies enable the creation of dynamic pools of virtual resources that can be aggregated on-demand for application-specific user-specific grid-computing. This change in paradigm from building grids out of physical resources to constructing virtual grids has many advantages but also requires new thinking on how to architect, manage and optimize the necessary middleware. This paper reviews the motivation for In-VIGO approach, discusses the technologies used, describes an early architecture for In-VIGO that represents a first step towards the end goal of building virtual information grids, and reports on first experiences with the In-VIGO software under development.</abstract>
	<keywords>Virtual machines; Grid-computing; Middleware; Virtual data; Network computing</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=933>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automating metadata Web service deployment for problem solving environments</paper_heading>
	<authors>Ozgur Balsoy, Ying Jin, Galip Aydin, Marlon Pierce, Geoffrey Fox</authors>
	<abstract>XML-based metadata information services are a crucial core service needed by problem solving environments built over emerging service-based, globally scaled distributed systems, as envisioned by the Open Grid Services Architecture and the Semantic Web. Developing user interfaces and service bindings for manipulating instances of particular schemas is thus extremely important and needs to be made as simple as possible. In this paper we describe procedures for automating the creation of Web service environments that can be used to simplify the creation and deployment of schema-based metadata services.</abstract>
	<keywords>Metadata authoring; Problem solving environments; XML; Web services</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=934>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Implementation and utilisation of a Grid-enabled problem solving environment in Matlab</paper_heading>
	<authors>M. Hakki Eres, Graeme E. Pound, Zhouan Jiao, Jasmin L. Wason, ... Simon J. Cox</authors>
	<abstract>In many areas of design search and optimisation one needs to utilise computational fluid dynamics (CFD) methods in order to obtain a numerical solution of the flow field in and/or around a proposed design. From this solution measures of quality for the design may be calculated, which are then used by the optimisation methods. In large models the processing time for the CFD computations can very well be many orders of magnitude larger than for the optimisation methods themselves; and the overall optimisation process usually demands a combination of computational and database resources; therefore this class of problems is well suited to Grid computing. The Geodise toolkit is a suite of tools for Grid-enabled parametric geometry generation, meshing, CFD analysis, design optimisation and search, databasing, Grid computing, and notification within the Matlab environment. These Grid services are presented to the design engineer as Matlab functions that conform to the usual syntax of Matlab. The use of the Geodise toolkit in Matlab introduces a flexible and Grid-enabled problem solving environment (PSE) for design search and optimisation. This PSE is illustrated here with two exemplar problems.</abstract>
	<keywords>Engineering design optimisation; Computational fluid dynamics; Problem solving environment; Matlab software; Grid computing</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=935>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The design of a distributed MATLAB-based environment for computing pseudospectra</paper_heading>
	<authors>C. Bekas, E. Kokiopoulou, E. Gallopoulos</authors>
	<abstract>It has been documented in the literature that the pseudospectrum of a matrix is a powerful concept that broadens our understanding of phenomena based on matrix computations. When the matrix A is non-normal, however, the computation of the pseudospectrum becomes a very expensive computational task. Thus, the use of high performance computing resources becomes key to obtaining useful answers in acceptable amounts of time. In this work we describe the design and implementation of an environment that integrates a suite of state-of-the-art algorithms running on a cluster of workstations to enable the matrix pseudospectrum become a practical tool for scientists and engineers. The user interacts with the environment via the graphical user interface PPsGUI. The environment is constructed on top of CMTM, an existing environment that enables distributed computation via an MPI API for MATLAB.</abstract>
	<keywords>Eigenvalues; Pseudospectrum; Problem solving environments; Parallel MATLAB; MPI; CMTM; Grid computing</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=936>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A problem solving environment for multidisciplinary coupled simulations in computational grids</paper_heading>
	<authors>Andreas Schreiber, Thijs Metsch, Hans-Peter Kersken</authors>
	<abstract>This paper describes a software environment for doing multidisciplinary coupled simulations in computational grids using a combination of various existing software tools. The environment consists of the software integration system TENT for setting up, steering and monitoring the simulation, the code coupling library MpCCI for the numerical coupling of simulation codes, and the MPI implementation MPICH-G2 and the Globus Toolkit for running the coupled codes in distributed environments.</abstract>
	<keywords>Problem solving environment; Multidisciplinary coupling; Grid computing</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=937>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On a data-driven environment for multiphysics applications</paper_heading>
	<authors>J. Michopoulos, P. Tsompanopoulou, E. Houstis, C. Farhat, ... A. Joshi</authors>
	<abstract>The combination of the recent advances in computational and distributed sensor network technologies provide a unique opportunity for focused efforts on high confidence modelling and simulation of multiphysics systems. Responding to this opportunity, we present in this paper the architecture of a data-driven environment for multiphysics applications (DDEMA) as a multidisciplinary problem solving environment (MPSE). The goal of this environment is to support the automated identification and efficient prediction of the behavioral response of multiphysics continuous interacting systems. The design takes into consideration heterogeneous and distributed information technologies, coupled multiphysics sciences, and sensor originating data to drive and to steer adaptive modelling and simulation of the underlying systemic behavior. The design objectives and proposed software architecture are described in the context of two multidisciplinary applications related to material structure design of supersonic platforms and fire/material/environment interaction monitoring, assessment and management. These applications of DDEMA will be distributed over a highly heterogeneous networks that extend from light and ubiquitous resources (thin portable devices/clients) to heavy GRID-based computational infrastructure.</abstract>
	<keywords>Data-driven; Multiphysics; Multidisciplinary problem solving environment; Distributed computing; Distributed sensors; Heterogeneous networks</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=938>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using Python for large scale linear algebra applications</paper_heading>
	<authors>Oliver Bröker, Oscar Chinellato, Roman Geus</authors>
	<abstract>Software used in scientific computing is traditionally developed using compiled languages for the sake of maximal performance. However, for most applications, the time-critical portion of the code that requires the efficiency of a compiled language, is confined to a small set of well-defined functions. Implementing the remaining part of the application using an interactive and interpreted high-level language offers many advantages without a big performance degradation tradeoff. This paper describes the Pythonic approach, a mixed language approach combining the Python programming language with near operating-system level languages. We demonstrate the effectiveness of the Pythonic approach by showing a few small examples and fragments of two large scale linear algebra applications. Essential advantages of the Pythonic mixed language approach is the combination of flexible, readable, shorter, and most importantly less error-prone syntax with performance similar to pure Fortran or C/C++ implementations.</abstract>
	<keywords>Python; C/C++; Fortran; Linear algebra; Multigrid; Eigenvalue solver</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=939>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Biological sequence alignment on the computational grid using the GrADS framework</paper_heading>
	<authors>Asim YarKhan, Jack J. Dongarra</authors>
	<abstract>In spite of the existence of several grid middleware projects, developing and executing programs on the computational grid remains a user intensive process. The goal of the Grid Application Development Software (GrADS) project is to make the grid simpler to use despite the dynamically changing status of grid resources. Protein and genome sequence alignment is a basic operation in bioinformatics, and it requires large data sets and tends to be highly compute intensive. In this paper, we present work done to grid-enable a biological sequence alignment package (FASTA) and to run it under the GrADS framework. We discuss the advantages of using GrADS framework for FASTA.</abstract>
	<keywords>GrADS project; Grid scheduling; Biological sequence alignment</keywords>
	<publication_month_year>2005-06-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 6</volumes_issues>
</paper>
<paper no=940>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An adaptive grid implementation of DNA sequence alignment</paper_heading>
	<authors>Chunxi Chen, Bertil Schmidt</authors>
	<abstract>In this paper we have described a dynamic programming algorithm to compute k non-intersecting near-optimal alignments in linear space. In order to reduce its runtime significantly, we are using a hierarchical grid system as the computing platform. Static and dynamic load balancing approaches are investigated in order to achieve efficiently mapping onto this type of architecture, which has characteristics such as: (1) the resources in the grid systems have different computational power; (2) the resources usually are connected by networks with widely varying performance characteristics. At last, a new dynamic load balancing approach named scheduler–worker parallel paradigm is proposed and evaluated.</abstract>
	<keywords>Sequence alignment; Grid computing; MPI; Dynamic programming</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=941>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Normalized workflow net (NWF-net): Its definition and properties</paper_heading>
	<authors>Shuyou Li, Binheng Song</authors>
	<abstract>Workflow net, based on Petri nets’ characteristics, is powerful and used widely. However, the workflow net itself cannot guarantee the soundness for its defined models; and what is more, the coming analysis on the workflow net models are difficult. In this paper, based on the structure of the classical workflow net, it introduces a new kind of net for modeling business processes, named normalized workflow net (NWF-net). NWF-net, which can be represented by a context-free language, can effectively avoid those structure problems occurred in the classical workflow net by removing some possible unnatural patterns. NWF-net satisfies all the required properties needed for workflow process modeling while keeping a strongly modeling capability, and the computational performance analysis, such as the expected completion time, becomes simple.</abstract>
	<keywords>NWF-net; Petri net; Soundness; Context-free language</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=942>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PYR: a Cellular Automata model for pyroclastic flows and application to the 1991 Mt. Pinatubo eruption</paper_heading>
	<authors>Gino Mirocle Crisci, Salvatore Di Gregorio, Rocco Rongo, William Spataro</authors>
	<abstract>PYR is a first Cellular Automata model for simulating pyroclastic flows, which are generated by collapsing volcanic columns. It was developed according to an empirical method for modelling macroscopic phenomena and was applied to the 1991 eruption of Mt. Pinatubo in the Philippines Islands. The model is very simple, but it is able to capture the three-dimensional evolution of the phenomenon in so far as it spreads. Results of the simulations are satisfying enough, if the comparison between real and simulated event is performed, considering the area involved by the event and the thickness of the deposit.</abstract>
	<keywords>Cellular Automata; Simulation; Pyroclastic flows; Mt. Pinatubo</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=943>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Correspondence between mutation and selection pressure and the genetic code degeneracy in the gene evolution</paper_heading>
	<authors>M. Dudkiewicz, P. Mackiewicz, A. Nowicka, M. Kowalczuk, ... S. Cebrat</authors>
	<abstract>There are three main elements deciding about the effect of mutations on the protein coding sequences—the type of the substitution of nucleotide, the selection for the function of the gene product and the nature of the genetic code itself. Selection used to be considered as the only directional process among the evolutionary mechanisms. In fact the mutational pressure is also “directional” which means that the rates of particular nucleotide substitutions tend to produce a DNA molecule with a specific nucleotide composition. Using Monte Carlo simulations we have shown that the genetic code plays the central role in buffering the effect of mutations and that all three elements are optimised in the generation of the genetic diversity in such a way that deleterious effects of mutations are substantially reduced.</abstract>
	<keywords>Mutation; Selection; Evolution; Genetic code; Monte Carlo simulation</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=944>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Self-organizing multivariate constrained meta-modeling technique for passive microwave and RF components</paper_heading>
	<authors>Tom Dhaene, Jan De Geest</authors>
	<abstract>A self-organizing algorithm is developed for multivariate constrained modeling of general passive components. The algorithm builds compact, analytical circuit models and represents the scattering parameters of the passive components as a function of its geometrical parameters and as a function of the frequency. Multiple constraints, or relationships between the geometrical parameters, may exist. The model generation algorithm combines iterative sampling and modeling techniques. It groups a number of full-wave electromagnetic simulations in one multivariate analytic model. The modeling accuracy level is user-defined. The analytical circuit models can easily be implemented and used in commercial circuit simulators. The model extraction provides EM-accuracy and generality at traditional circuit simulation speed.</abstract>
	<keywords>Meta-modeling; Multivariate systems; Adaptive sampling; Electronic design automation; Circuit modelling; Electromagnetic modeling</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=945>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the construction of a reduced rank square-root Kalman filter for efficient uncertainty propagation</paper_heading>
	<authors>Dimitri Treebushny, Henrik Madsen</authors>
	<abstract>The Kalman filter is a sequential estimation procedure that combines a stochastic dynamical model with observations in order to update the model state and the associated uncertainty. In the situation where no measurements are available the filter works as an uncertainty propagator. The most computationally demanding part of the Kalman filter is to propagate the covariance through the dynamical system, which may be completely infeasible in high-dimensional models. The reduced rank square-root (RRSQRT) filter is a special formulation of the Kalman filter for large-scale applications. In this formulation, the covariance matrix of the model state is expressed in a limited number of modes M. In the classical implementation of the RRSQRT filter the computational costs of the truncation step grow very fast with the number of modes (>M3). In this work, a new approach based on the Lanzcos algorithm is formulated. It provides a more cost-efficient scheme and includes a precision coefficient that can be tuned for specific applications depending on the trade-off between precision and computational load.</abstract>
	<keywords>Kalman filter; Reduced rank square-root; RRSQRT; Lanczos algorithm</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=946>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Vector quantization: a weighted version for time-series forecasting</paper_heading>
	<authors>A. Lendasse, D. Francois, V. Wertz, M. Verleysen</authors>
	<abstract>Nonlinear time-series prediction offers potential performance increases compared to linear models. Nevertheless, the enhanced complexity and computation time often prohibits an efficient use of nonlinear tools. In this paper, we present a simple nonlinear procedure for time-series forecasting, based on the use of vector quantization techniques; the values to predict are considered as missing data, and the vector quantization methods are shown to be compatible with such missing data. This method offers an alternative to more complex prediction tools, while maintaining reasonable complexity and computation time.</abstract>
	<keywords>Time-series prediction; Vector quantization; Missing data; Radial-Basis Function Networks</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=947>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Very large Bayesian multinets for text classification</paper_heading>
	<authors>Mieczysław A. Kłopotek</authors>
	<abstract>This paper presents a newly developed algorithm learning very large tree-like Bayesian networks from data and exploits it to create a Bayesian multinet (BMN) classifier for natural language text documents. Results of empirical evaluation of this BMN classifier are presented. The study suggests that tree-like Bayesian networks are able to handle a classification task in 100 000 variables with sufficient speed and accuracy.</abstract>
	<keywords>Bayesian; Multinets; Classification</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=948>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Controlling a complex electromechanical process on the basis of a neurofuzzy approach</paper_heading>
	<authors>Rodolfo E. Haber, J.R. Alique, A. Alique, R.H. Haber</authors>
	<abstract>This paper shows the viability of implementing a control strategy based on the internal-model control paradigm, which is a useful synergy of a dynamic ANN trained from real-life data and used to predict process output and a fuzzy-logic control (FLC) that enhances the control system's overall performance. A force control problem involving a complex electromechanical system, represented here by the machining process, is considered as a case study. The main goal is to control a single-output variable, cutting force, by changing a single-input variable, feed rate. The proposed neurofuzzy-control (NFC) scheme consists of a dynamic model using ANNs to estimate process output, and a fuzzy-logic controller (FLC) with the same static gain as the inverse model to determine the control inputs (feed rate) necessary to keep the cutting force constant. Four approaches, the fuzzy-logic controller (FLC), the direct inverse controller based on ANNs (DIC-NN), the internal-model controller (IMC-NN) and a neurofuzzy controller (NFC), are simulated and their performances are assessed in terms of several performance measurements. The results demonstrate that the NFC strategy provides better disturbance rejection than the IMC-NN and the FLC for the cases analyzed.</abstract>
	<keywords>Fuzzy control; Neural networks; Internal-model control; Machining process</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=949>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Neural networks for event extraction from time series: a back propagation algorithm approach</paper_heading>
	<authors>D. Gao, Y. Kinouchi, K. Ito, X. Zhao</authors>
	<abstract>This paper presents a relatively new event detection method using neural networks for time series analysis. Such method can capture homeostatic dynamics of the system under the influence of exogenous event. The results show that financial time series include both predictable deterministic and unpredictable random components. Neural networks can identify the properties of homeostatic dynamics and model the dynamic relation between endogenous and exogenous variables in financial time series input–output system. We explore the signaling mechanisms that transfer information in such dynamic system and investigate the impact of the number of model inputs and the number of hidden layer neurons on financial analysis.</abstract>
	<keywords>Neural network; Time series; Back propagation</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=950>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Real-time interactive motion transitions by a uniform posture map</paper_heading>
	<authors>Jin Ok Kim, Bum Ro Lee, Chin Hyun Chung</authors>
	<abstract>Motion transition is a useful method for reusing existing motion data. It generates a seamless intermediate motion with two short motion sequences. In this paper, the Uniform Posture Map (UPM) is proposed to perform motion transitions. The UPM is organized through the quantization of various postures with an unsupervised learning algorithm; it places the output neurons with similar postures in adjacent positions. Using this property, an intermediate posture of two applied postures is generated; the generating posture is used as a key-frame to make an interpolating motion. The UPM has lower computational costs in comparison with other motion transition algorithms. It provides a control parameter; an animator can not only control the motion simply by adjusting this parameter, but can also produce animation interactively. The UPM prevents the generating of invalid output neurons to present unrealistic postures in the learning phase; thus, it makes more realistic motion curves. Finally, it contributes to the making of more natural motions. The motion transition algorithm proposed in this paper can be applied to various fields such as real-time 3D games, virtual reality applications, and web 3D applications.</abstract>
	<keywords>Kohonen self-organizing map; Motion edit; Motion transition; Character animation; UPM (uniform posture map)</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=951>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Nonlinear model for ECG R–R interval variation using genetic programming approach</paper_heading>
	<authors>Yun Seok Chang, Kwang Suk Park, Bo Yeon Kim</authors>
	<abstract>This paper proposes a nonlinear system modeling method, which predicts characteristics of the ECG R–R interval variation. For determining model equation, we adopted a genetic programming method in which the chromosome represents the model equation consisting of time-delayed variables, constants, and four arithmetic operators, and determines the fitness function. By genetic programming, sequences of regressive nonlinear equations are produced and evolved until the finding of the optimal model equation, which could simulate the spectral, statistical and nonlinear behavior of the given R–R interval dynamics. Experimental results showed that the evolutionary approach could find the equation which simulates the spectral and chaotic dynamics of the given signal. Therefore, the proposed evolutionary approach is useful for the system identification of the nonlinear biological system.</abstract>
	<keywords>Genetic programming; Modeling nonlinear system; Nonlinear equation</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=952>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computerized recognition of Alzheimer disease-EEG using genetic algorithms and neural network</paper_heading>
	<authors>Hyun Taek Kim, Bo Yeon Kim, Eun Hye Park, Jong Woo Kim, ... Sunyoung Cho</authors>
	<abstract>We propose an automatic recognition method of Alzheimer's disease (AD) with single channel EEG recording using combined the genetic algorithms (GA) and the artificial neural network (ANN). Five min of the resting spontaneous EEG and the ERP in an auditory oddball task were recorded at P4 site in 16 early AD patients and 16 age-matched normal subjects. EEG and ERP were analyzed to compute their 28 statistical and 2 nonlinear features as well as 88 spectral features and 10 ERP features, to make a feature pool for each 30-s segment of the recording data. The combined GA/ANN was applied to find the minimal set of the dominant features from the feature pool that are most efficient to classify into two groups automatically. The effective 35 features were found and used as inputs of the artificial neural network. The recognition rate of ANN fed by these input was 81.9% for untrained data set. These results suggest that the combined GA/ANN approach may be useful for early detection of AD and that single channel EEG data might be enough to recognize AD. This approach could be extended to a reliable classification system using EEG recording that can discriminate between groups.</abstract>
	<keywords>Alzheimer's disease; Genetic algorithms; Artificial neural network; Electroencephalogram (EEG)</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=953>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Learning methods for radial basis function networks</paper_heading>
	<authors>Roman Neruda, Petra Kudová</authors>
	<abstract>RBF networks represent a vital alternative to the widely used multilayer perceptron neural networks. In this paper we present and examine several learning methods for RBF networks and their combinations. A gradient-based learning, the three-step algorithm with unsupervised part, and an evolutionary algorithms are introduced, and their performance compared on benchmark problems from the Proben1 database. The results show that the three-step learning is usually the fastest, while the gradient learning achieves better precision. The best results can be achieved by employing hybrid approaches that combine presented methods.</abstract>
	<keywords>Radial basis function networks; Hybrid supervised learning; Genetic algorithms; Benchmarking</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=954>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>BioSim—a biomedical character-based problem solving environment</paper_heading>
	<authors>Yang Cai, Ingo Snel, Betty Cheng, B. Suman Bharathi, ... Judith Klein-Seetharaman</authors>
	<abstract>Understanding and solving biomedical problems requires insight into the complex interactions between the components of biomedical systems by domain and non-domain experts. This is challenging because of the enormous amount of data and knowledge in this domain. Therefore, non-traditional educational tools have been developed such as a biological storytelling system, animations of biomedical processes and concepts, and interactive virtual laboratories. The next-generation problem solving tools need to be more interactive to include users with any background, while remaining sufficiently flexible to target open research problems at any level of abstraction, from the conformational changes of a protein to the interaction of the various biochemical pathways in our body. Here, we present an interactive and visual problem solving environment for the biomedical domain. We designed a biological world model, in which users can explore biological interactions by role-playing “characters” such as cells and molecules or as an observer in a “shielded vessel”, both with the option of networked collaboration between simultaneous users. The system architecture of these “characters” contains four main components: (1) bio-behavior is modeled using cellular automata; (2) bio-morphing uses vision-based shape tracking techniques to learn from recordings of real biological dynamics; (3) bio-sensing is based on molecular principles of recognition to identify objects, environmental conditions and progression in a process; (4) bio-dynamics implements mathematical models of cell growth and fluid-dynamic properties of biological solutions. The principles are implemented in a simple world model of the human vascular system and a biomedical problem that involves an infection by Neisseria meningitides where the biological characters are white and red blood cells and Neisseria cells. Our case studies show that the problem solving environment can inspire user's strategic, creative and innovative thinking.</abstract>
	<keywords>Scientific visualization; Biological discovery; Game design; Problem solving; Artificial life; Education</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=955>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Virtual environment trajectory analysis: a basis for navigational assistance and scene adaptivity</paper_heading>
	<authors>Corina Sas, Gregory O’Hare, Ronan Reilly</authors>
	<abstract>This paper describes the analysis and clustering of motion trajectories obtained while users navigate within a virtual environment (VE). It presents a neural network simulation that produces a set of five clusters which help to differentiate users on the basis of efficient and inefficient navigational strategies. The accuracy of classification carried out with a self-organising map algorithm was tested and improved to in excess of 85% by using learning vector quantisation. This paper considers how such user classifications could be utilised in the delivery of intelligent navigational support and the dynamic reconfiguration of scenes within such VEs. We explore how such intelligent assistance and system adaptivity could be delivered within a Multi-Agent Systems (MAS) context.</abstract>
	<keywords>Spatial behaviour; Trajectory classification; Adaptive VEs</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=956>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Bringing combined interaction to a problem solving environment for vascular reconstruction</paper_heading>
	<authors>E.V. Zudilova, P.M.A. Sloot</authors>
	<abstract>The paper addresses the problem of how to make a human–computer interaction user-friendlier within a problem solving environment (PSE). Pre-operative planning of a vascular reconstruction procedure is a case study for this research. The Virtual Radiology Explorer (VRE) presented in the paper has being deployed for the virtual reality (VR) and desktop projection modalities. Both projection modalities have been compared in respect to the interaction capabilities provided to the potential users of the VRE. The heuristic usability evaluation of the VRE as well as the first results of user profiling show that the combination of VR and desktop within the same interaction–visualisation medium could help to satisfy the wider range of the VRE users. A Personal Space Station (PSS) is a possible solution for the implementation of this concept.</abstract>
	<keywords>Vascular reconstruction; PSE; Projection modality; User profiling</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=957>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>User hints: a framework for interactive optimization</paper_heading>
	<authors>Hugo A.D. do Nascimento, Peter Eades</authors>
	<abstract>Innovative improvements in the area of human–computer interaction and user interfaces have enabled intuitive and effective applications for a variety of problems. On the other hand, there has also been the realization that several real-world optimization problems still cannot be totally automated. Very often, user interaction is necessary for refining the optimization problem, managing the computational resources available, or validating or adjusting a computer-generated solution. This paper presents an interactive framework called user hints for having humans help optimization methods to solve difficult problems. In the framework users play a dynamic and important role by providing hints. Hints are actions that help to insert domain knowledge, to escape from local minima, to reduce the space of solutions to be explored, or to avoid ambiguity when there is more than one optimal solution. User hints are given in an intuitive way through a graphical interface. Visualization tools are also included in order to inform the user about the state of the optimization process. We discuss applications of the user hints framework to the graph drawing and the map labeling problems. An evaluation of some user hints systems indicates that optimization processes can benefit from human interaction.</abstract>
	<keywords>Interactive optimisation; Information visualization; Graph drawing; Map labeling</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=958>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Oh behave! Agent-based behavioral representations in problem solving environments</paper_heading>
	<authors>M. North, C. Macal, P. Campbell</authors>
	<abstract>The development of deregulated electricity systems around the world has produced the need for simulation systems that are capable of addressing the complexities that arise in the new markets. Agent-based models allow the use of complex adaptive systems approaches that are capable of producing tools or problem solving environments that can address the behavior of each of the participants within the electricity market. The agents in the tools are allowed to establish their own objectives and apply their own decision rules. They can be developed to learn from their previous experiences and change their behavior when future opportunities arise. In this paper, we will argue that the same type of agent-based technology that is used to produce “realistic” agent behavior in agent-based simulation tools at Argonne National Laboratory can also be used to embed these tools in problem solving environments.</abstract>
	<keywords>Problem solving environments; Agent-based modelling; Behavioral representations</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=959>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Visualisation of fuzzy systems: requirements, techniques and framework</paper_heading>
	<authors>Binh Pham, Ross Brown</authors>
	<abstract>Complex fuzzy systems exist in many applications and effective visualisation is required to gain insights into the nature and working of these systems, especially in the implication of imprecision, its propagation and impacts on the quality and reliability of the outcomes. This paper presents a holistic approach towards the design of a visualisation system for fuzzy systems. We firstly analyse the requirements for such a visualisation system by articulating fundamental ontologies that underpin the structure and operations of fuzzy systems. A software framework using a multi-agent approach is then presented with the aim to facilitate the organisation and flow of complex tasks, their inter-relationships and their interactions with users. Finally, we discuss visualisation techniques for fuzzy data and fuzzy rules, and introduce methods to extend and improve some existing techniques.</abstract>
	<keywords>Fuzzy data; Ontologies; Framework; Visualisation</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=960>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A component-oriented software authoring system for exploratory visualization</paper_heading>
	<authors>Masahiro Takatsuka</authors>
	<abstract>This paper discusses the benefits of a component-oriented visual software authoring system that is based on open standards and provides the seamless integration of various software tools in a unified environment. It employs a visual component assembly paradigm for ease of construction, Java™ and JavaBeans™ component architecture for the open environment, and recursive development methods, all of which allow us to rapidly construct and share applications from independently developed components. Moreover, it is highly interactive and fully configurable at run-time in order to support exploratory visualization. This versatility has the potential to improve the integration of independently developed analysis tools and the dissemination of research findings.</abstract>
	<keywords>Component-oriented; Visual programming; Interactive; Exploratory; Visualization</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=961>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A simple model generation system for computer graphics</paper_heading>
	<authors>Minh Tran, Amitava Datta, Nick Lowe</authors>
	<abstract>Most 3D objects in computer graphics are represented as polygonal mesh models. Though techniques like image-based rendering are gaining popularity, a vast majority of applications in computer graphics and animation use such polygonal meshes for representing and rendering 3D objects. High quality mesh models are usually generated through 3D laser scanning techniques. However, even the inexpensive laser scanners cost tens of thousands of dollars and it is difficult for researchers in computer graphics to buy such systems just for model acquisition. In this paper, we describe a simple model acquisition system built from web cams or digital cameras. This low-cost system gives researchers an opportunity to capture and experiment with reasonably good quality 3D models. Our system uses standard techniques from computer vision and computational geometry to build 3D models.</abstract>
	<keywords>3D reconstruction; Object modeling; Delauney triangulation; Intensity-based matching</keywords>
	<publication_month_year>2005-07-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 7</volumes_issues>
</paper>
<paper no=962>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A super-peer model for resource discovery services in large-scale Grids</paper_heading>
	<authors>Carlo Mastroianni, Domenico Talia, Oreste Verta</authors>
	<abstract>As deployed Grids increase from tens to thousands of nodes, peer-to-peer (P2P) techniques and protocols can be used to implement scalable services and applications. The super-peer model is a novel approach that helps the convergence of P2P models and Grid environments and can be used to deploy a P2P information service in Grids. A super-peer serves a single physical organization in a Grid, and manages metadata associated to the resources provided by the nodes of that organization. Super-peers connect to each other to form a peer network at a higher level. This paper examines how the super-peer model can handle membership management and resource discovery services in a multi-organizational Grid. A simulation analysis evaluates the performance of a resource discovery protocol; simulation results can be used to tune protocol parameters in order to increase search efficiency.</abstract>
	<keywords>peer-to-peer (P2P);  Grid; P2P model; multi-organizational Grid </keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=963>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Large-scale nonlinear optimization in circuit tuning</paper_heading>
	<authors>Andreas Wächter, Chandu Visweswariah, Andrew R. Conn</authors>
	<abstract>Circuit tuning is an important task in the design of custom digital integrated circuits such as high-performance microprocessors. The goal is to improve certain aspects of the circuit, such as speed, area, or power, by optimally choosing the widths of the transistors. This task can be formulated as a large-scale nonlinear, nonconvex optimization problem, where function values and derivatives are obtained by simulation of individual gates. This application offers an excellent example of a nonlinear optimization problem, for which it is very desirable to increase the size of the problems that can be solved in a reasonable amount of time. In this paper we describe the mathematical formulation of this problem and the implementation of a circuit tuning tool. We demonstrate how the integration of a novel state-of-the-art interior point algorithm for nonlinear programming led to considerable improvement in efficiency and robustness. Particularly, as will be demonstrated with numerical results, the new approach has great potential for parallel and distributed computing.</abstract>
	<keywords>Large-scale nonlinear nonconvex programming; Circuit tuning; Transistor sizing; Interior point method; Filter method; Line search</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=964>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Computation of optical modes in axisymmetric open cavity resonators</paper_heading>
	<authors>O. Chinellato, P. Arbenz, M. Streiff, A. Witzig</authors>
	<abstract>The computation of optical modes inside axisymmetric cavity resonators with a general spatial permittivity profile is a formidable computational task. In order to avoid spurious modes the vector Helmholtz equations are discretised by a mixed finite element approach. We formulate the method for first and second order Nédélec edge and Lagrange nodal elements. We discuss how to accurately compute the element matrices and solve the resulting large sparse complex symmetric eigenvalue problems. We validate our approach by three numerical examples that contain varying material parameters and absorbing boundary conditions (ABC).</abstract>
	<keywords>Optical mode computation; Numerical device simulation; Axisymmetric cavity; Body of revolution; Semiconductor laser; Finite element method; Large sparse complex symmetric eigenproblem; Jacobi–Davidson algorithm; Nédélecedge elements</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=965>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Parallel iterative solvers for sparse linear systems in circuit simulation</paper_heading>
	<authors>A. Basermann, U. Jaekel, M. Nordhausen, K. Hachiya</authors>
	<abstract>For the solution of sparse linear systems from circuit simulation whose coefficient matrices include a few dense rows and columns, a parallel iterative algorithm with distributed Schur complement preconditioning is presented. The parallel efficiency of the solver is increased by transforming the equation system into a problem without dense rows and columns as well as by exploitation of parallel graph partitioning methods. The costs of local, incomplete LU decompositions are decreased by fill-in reducing reordering methods of the matrix and a threshold strategy for the factorization. The efficiency of the parallel solver is demonstrated with real circuit simulation problems on PC clusters.</abstract>
	<keywords>Parallel iterative solver; Circuit simulation; System reduction; Preconditioning; Partitioning; Reordering</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=966>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Reactions in complex biologically relevant systems: challenges for computational approaches</paper_heading>
	<authors>Markus Meuwly</authors>
	<abstract>The ever increasing power of computational architectures allows to investigate reactive phenomena in complex biomolecular systems and environments. Results from such calculations can be compared with experiment and give important insight into microscopic aspects of chemical reactions. We discuss two examples of biologically relevant reaction mechanisms: double proton transfer in a DNA–base pair analogue for which detailed experimental information is available and ligand (re)binding in myoglobin (Mb). For the double proton transfer in the DNA–base pair analogue ab initio molecular dynamics simulations provide direct information on the infrared spectrum that has also been observed experimentally. In the case of ligand rebinding in MbNO, we discuss results from reactive molecular dynamics simulations to investigate the rebinding probability after photodissociation of the NO from myoglobin and compare the findings with experiment. Both reaction types pose different challenges and we highlight and address the computational difficulties in each case. In particular, we explore and discuss the possibilities and limitations of current computational methods to understand reactive processes in systems where several degrees of freedom are important.</abstract>
	<keywords>Ab initio molecular dynamics; Proton transfer; Rebinding in myoglobin; Biological systems</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=967>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Monitoring cache behavior on parallel SMP architectures and related programming tools</paper_heading>
	<authors>Thomas Brandes, Helmut Schwamborn, Michael Gerndt, Jürgen Jeitner, ... Hans-Christian Hoppe</authors>
	<abstract>This paper describes the ideas and developments of the project EP-CACHE. Within this project new methods and tools are developed to improve the analysis and the optimization of programs for cache architectures, especially for SMP clusters. The tool set comprises the semi-automatic instrumentation of user programs, the monitoring of the cache behavior, the visualization of the measured data, and optimization techniques for improving the user program for better cache usage. As current hardware performance counters do not give sufficient user relevant information, new hardware monitors are designed that provide more detailed information about the cache utilization related to the data structures and code blocks in the user program. The expense of the hardware and software realization will be assessed to minimize the risk of a real implementation of the investigated monitors. The usefulness of the hardware monitors is evaluated by a cache simulator.</abstract>
	<keywords>Hardware cache monitoring; Performance analysis; Cache optimizations; Parallel programming tools; SMP cluster</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=968>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Routing direction determination in regular networks based on configurable circuits</paper_heading>
	<authors>Andreas C. Döring</authors>
	<abstract>We consider the problem of finding short paths in a regular network such as a k-ary n-cube. This problem is a basic aspect of routing and has to be implemented with a very high performance for cluster and parallel computer networks. To achieve this, a scalable reconfigurable circuit is proposed that covers many popular topologies at acceptable cost. As a demonstration, the application to a modestly complex topology is shown in detail (“Multi-Mesh” introduced by Das et al. [D. Das, M. De, B.P. Sinha, A network topology with multiple meshes, IEEE Transactions on Computers 48 (5) (1999) 536–551]). The achieved flexibility is much higher than that of previously reported reconfigurable circuits for the same purpose such as interval routing [R.B. Tan, J. van Leeuwen, Compact routing methods: a survey, in: Proceedings of the Colloquium on Structural Information and Communication Complexity (SICC94), School of Computer Science, Carleton University, Ottawa, 1995, pp. 99–109] or bit-pattern-associative routing [D.H. Summerville, J.G. Delgado-Frias, S. Vassiliadis, A flexible bit-pattern-associative router for interconnection networks, IEEE Transactions on Parallel and Distributed Systems 7 (5) (1996) 477–485]. The proposed circuit extends the domain of application-specific reconfigurable circuits beyond the areas of signal processing and cryptography, where most work is currently done.</abstract>
	<keywords>Routing; Multicomputer networks; Reconfigurable computing</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=969>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using automatic differentiation to compute derivatives for a quantum-chemical computer program</paper_heading>
	<authors>Rainer Steiger, Christian H. Bischof, Bruno Lang, Walter Thiel</authors>
	<abstract>The ADIFOR 2.0 tool for Automatic Differentiation of Fortran programs has been used to generate analytic gradient code for all semiempirical SCF methods available in the MNDO97 program. The correctness and accuracy of the new code have been verified. Its performance has been compared with that of hand-coded analytic derivative routines and with numerical differentiation. From a quantum-chemical point of view, the major advance of this work is the development of previously unavailable analytic gradient code for the recently proposed OM1 and OM2 methods.</abstract>
	<keywords>MNDO97; Analytic gradient; Automatic Differentiation; ADIFOR</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=970>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient and accurate derivatives for a software process chain in airfoil shape optimization</paper_heading>
	<authors>C.H. Bischof, H.M. Bücker, B. Lang, A. Rasch, E. Slusanschi</authors>
	<abstract>When using a Newton-based numerical algorithm to optimize the shape of an airfoil with respect to certain design parameters, a crucial ingredient is the derivative of the objective function with respect to the design parameters. In large-scale aerodynamics, this objective function is an output of a computational fluid dynamics program written in a high-level programming language such as Fortran or C. Numerical differentiation is commonly used to approximate derivatives but is subject to truncation and subtractive cancellation errors. For a particular two-dimensional airfoil, we instead apply automatic differentiation to compute accurate derivatives of the lift and drag coefficients with respect to geometric shape parameters. In automatic differentiation, a given program is transformed into another program capable of computing the original function together with its derivatives. In the problem at hand, the objective function consists of a sequence of programs: a MATLAB program followed by two Fortran 77 programs. It is shown how automatic differentiation is applied to a sequence of programs while keeping the computational complexity within reasonable limits. The derivatives computed by automatic differentiation are compared with approximations based on divided differences.</abstract>
	<keywords>Automatic differentiation; Forward mode; Seeding; Concatenated programs; Shape optimization</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=971>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Generating efficient derivative code with TAF: Adjoint and tangent linear Euler flow around an airfoil</paper_heading>
	<authors>R. Giering, T. Kaminski, T. Slawig</authors>
	<abstract>FastOpt’s new automatic differentiation tool TAF is applied to the two-dimensional Navier–Stokes solver NSC2KE. For a configuration that simulates the Euler flow around an NACA airfoil, TAF has generated the tangent linear and adjoint models as well as the second derivative (Hessian) code. Owing to TAF’s capability of generating efficient adjoints of iterative solvers, the derivative code has a high performance: running both the solver and its adjoint requires 3.4 times as long as running the solver only. Further examples of highly efficient tangent linear, adjoint, and Hessian codes for large and complex three-dimensional Fortran 77-90 climate models are listed. These examples suggest that the performance of the NSC2KE adjoint may well be generalised to more complex three-dimensional CFD codes. We also sketch how TAF can improve the adjoint’s performance by exploiting self-adjointness, which is a common feature of CFD codes.</abstract>
	<keywords>Computational fluid dynamics; Adjoint; HessianAutomatic differentiation; Navier–Stokes; Shape optimisation</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=972>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An efficient exact adjoint of the parallel MIT General Circulation Model, generated via automatic differentiation</paper_heading>
	<authors>Patrick Heimbach, Chris Hill, Ralf Giering</authors>
	<abstract>We describe computational aspects of automatic differentiation applied to global ocean circulation modeling and state estimation. The task of minimizing a cost function measuring the ocean simulation versus observation misfit is achieved through efficient calculation of the cost gradient w.r.t. a set of controls via the adjoint technique. The adjoint code of the parallel MIT general circulation model is generated using TAMC or its successor TAF. To achieve a tractable problem in both CPU and memory requirements, in the light of control flow reversal, the adjoint code relies heavily on the balancing of storing versus recomputation via the checkpointing method. Further savings are achieved by exploiting self-adjointness of part of the computation. To retain scalability of domain decomposition-based parallelism, hand-written adjoint routines are provided. These complement routines of the parallel support package to perform corresponding operations in reverse mode. The unique feature of the TAF tool which enables the dumping of the adjoint state and restart the adjoint integration is exploited to overcome batch execution limitations on HPC machines for large-scale ocean and climate simulations. Strategies to test the correctness of the adjoint-generated gradient are presented. The size of a typical adjoint application is illustrated for the case of the global ocean state estimation problem undertaken by the SIO-JPL-MIT Consortium “Estimating the Circulation and Climate of the Ocean” (ECCO). Results are given by way of example.</abstract>
	<keywords>Ocean/climate modeling; Ocean state estimation/data assimilation; Automatic differentiation; Adjoint/reverse mode of AD; Parallel/high performance computing</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=973>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Automatic differentiation and nonlinear controller design by exact linearization</paper_heading>
	<authors>Klaus Röbenack</authors>
	<abstract>Various modern algorithms for controller design are based on differential-geometric concepts. A method of particular importance is called exact linearization via feedback. In this case, the implementation of the controller requires the computation of Lie derivatives, which have been computed symbolically. This can be very time consuming. We present a new computation method relying on automatic differentiation.</abstract>
	<keywords>Nonlinear control; Feedback linearization; Taylor series</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=974>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Reducing reverse-mode memory requirements by using profile-driven checkpointing</paper_heading>
	<authors>Mike Fagan, Alan Carle</authors>
	<abstract>Reverse-mode derivative calculations have favorable time cost for many problems. Unfortunately “real world” reverse-mode computations frequently experience prohibitive space costs. To mitigate this space cost, users resort to checkpointing techniques to recompute, rather than save, the necessary values. Injudicious checkpointing, however, can destroy the favorable time performance that made reverse mode attractive in the first place. Consequently, reverse-mode users must spend significant amounts of development time analyzing and developing checkpointing schemes that complement their reverse-mode computation code. In this paper, we describe a particular instance of this checkpointing problem: we were using reverse-mode code generated by Adifor 3.0 to compute derivatives of a large computational fluid dynamics code. Our effort labored under the additional constraint that development time was minimal (as always, it was needed yesterday). Our solution was to use profiling to narrowly focus our checkpoint analysis. This profiling approach worked well for our problem. Furthermore, the profiling idea is sufficiently general that it should work well for other problems. This paper details both our results on our specific problem and guidelines for applying the profiling technique to other checkpoint-based reverse-mode development problems.</abstract>
	<keywords>Automatic differentiation; Reverse mode; Profiling</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=975>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An automatic differentiation platform: Odyssée</paper_heading>
	<authors>Christèle Faure</authors>
	<abstract>Numerous automatic differentiation strategies can be imagined to produce all kind of derivative programs under a wide range of complexity constraints, but there is no way to prototype and evaluate them on real size applications with reasonable effort. Since the development of an automatic differentiation platform is prohibitively expensive, using an existing platform to share investments between the different research teams is a good solution. Odyssée is an open automatic differentiation tool that enables the development of program analysis as well as program transformation for automatic differentiation. It has been used to differentiate large size industrial programs (300 000 lines of Fortran 77) and to prototype diverse new automatic differentiation algorithms. Its source is now freely available, a cooperative research project can therefore be based on it without financial or contractual constraint.</abstract>
	<keywords>Automatic differentiation; Computational differentiation; Odyssée; Open platform; Source transformation tool</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=976>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>“To be recorded” analysis in reverse-mode automatic differentiation</paper_heading>
	<authors>Laurent Hascoët, Uwe Naumann, Valérie Pascual</authors>
	<abstract>The automatic generation of adjoints of mathematical models that are implemented as computer programs is receiving increased attention in the scientific and engineering communities. Reverse-mode automatic differentiation is of particular interest for large-scale optimization problems. It allows the computation of gradients at a small constant multiple of the cost for evaluating the objective function itself, independent of the number of input parameters. Source-to-source transformation tools apply simple differentiation rules to generate adjoint codes based on the adjoint version of every statement. In order to guarantee correctness, certain values that are computed and overwritten in the original program must be made available in the adjoint program. For their determination we introduce a static data-flow analysis called “to be recorded” analysis. Possible overestimation of this set must be kept minimal to get efficient adjoint codes. This efficiency is essential for the applicability of source-to-source transformation tools to real-world applications.</abstract>
	<keywords>Automatic differentiation; Reverse modeActive variables; Memory requirement; Data-flow analysis</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=977>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Looking for narrow interfaces in automatic differentiation using graph drawing</paper_heading>
	<authors>H.Martin Bücker</authors>
	<abstract>Automatic differentiation is a powerful technique for evaluating derivatives of functions given in the form of a high-level programming language such as Fortran, C, or C++. This technique is superior, in terms of accuracy, to numerical differentiation because it avoids the truncation error involved in divided difference approximations. In automatic differentiation, the program is treated as a potentially very long composition of elementary functions to which the chain rule of differential calculus is applied over and over again. Because of the associativity of the chain rule, there is room for different strategies computing the same numerical results but whose computational cost may vary significantly. Several strategies exploiting high-level structure of the underlying computer code are known to reduce computational cost as opposed to blindly applying automatic differentiation. An example includes “interface contraction” where one takes advantage of the fact that the number of variables passed between subroutines is small compared with the number of propagated directional derivatives. Unfortunately, these so-called narrow interfaces are not immediately available. The present study investigates the use of the VCG graph drawing tool to recognize narrow interfaces in the computational graph, a certain directed acyclic graph used to represent data dependences of variables in the underlying computer code.</abstract>
	<keywords>Automatic differentiation; Graph drawing; Computational graph; Interface contraction; Local preaccumulation</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=978>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Making automatic differentiation truly automatic: coupling PETSc with ADIC</paper_heading>
	<authors>Paul Hovland, Boyana Norris, Barry Smith</authors>
	<abstract>Despite its name, automatic differentiation (AD) is often far from an automatic process. Often one must specify independent and dependent variables, indicate the derivative quantities to be computed, and perhaps even provide information about the structure of the Jacobians or Hessians being computed. When AD is used in conjunction with a toolkit with well-defined interfaces, however, many of these issues can be dealt with automatically. We describe recent research into coupling the ADIC automatic differentiation tool with PETSc, a toolkit for the parallel numerical solution of PDEs. This research leverages the interfaces and objects of PETSc to make the AD process very nearly invisible.</abstract>
	<keywords>Automatic; Differentiation; ADIC; PETSc; Derivatives</keywords>
	<publication_month_year>2005-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 21, Issue 8</volumes_issues>
</paper>
<paper no=979>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Comments on an access control model in semantic grid</paper_heading>
	<authors>Libin Wang, Kefei Chen</authors>
	<abstract>In this note, we show that there are some significant design weaknesses in the access control model that proposed to address the convergence and consistence problems in semantic grid by Bu and Xu in a recent paper.</abstract>
	<keywords>Access control; GridSecurity model; Security policy</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=980>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Workflow-based Grid applications</paper_heading>
	<authors>Falk Neubauer, Andreas Hoheisel, Joachim Geiler</authors>
	<abstract>Standardization, usability, and business models are the main success factors for next generation Grid computing systems. This paper focuses on standards and usability within the context of the Fraunhofer Resource Grid that implements a simple, Petri net-based graph model for orchestrating Grid Services and legacy command line applications on OGSI-compatible middleware. The design and prototype implementation of a Grid workflow management system is presented, including a set of description languages required for orchestrating abstract workflows and for mapping them onto real Grid infrastructure, as well as a workflow enactment service that executes loosely coupled Grid jobs on the Grid.</abstract>
	<keywords>Grid computing; Grid Services; Workflow management; Petri nets</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=981>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Web object-based storage management in proxy caches</paper_heading>
	<authors>Abdolreza Abhari, Sivarama P. Dandamudi, Shikharesh Majumdar</authors>
	<abstract>Proxy caches are essential to improve the performance of the World Wide Web and to enhance user perceived latency. Appropriate cache management strategies are crucial to achieve these goals. In our previous work, we have introduced Web object-based caching policies. A Web object consists of the main HTML page and all of its constituent embedded files. Our studies have shown that these policies improve proxy cache performance substantially. In this paper, we propose a new Web object-based policy to manage the storage system of a proxy cache. We propose two techniques to improve the storage system performance. The first technique is concerned with prefetching the related files belonging to a Web object, from the disk to main memory. This prefetching improves performance as most of the files can be provided from the main memory rather than from the proxy disk. The second technique stores the Web object members in contiguous disk blocks in order to reduce the disk access time. We used trace-driven simulations to study the performance improvements one can obtain with these two techniques. Our results show that the first technique by itself provides up to 50% reduction in hit latency, which is the delay involved in providing a hit document by the proxy. An additional 5% improvement can be obtained by incorporating the second technique.</abstract>
	<keywords>Web proxy caching; Prefetching; Proxy cache performance evaluation; Cache replacement policy; Trace-driven simulation</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=982>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Dynamic mapping of cooperating tasks to nodes in a distributed system</paper_heading>
	<authors>Christian Rehn</authors>
	<abstract>Networks of workstations (NOWs) are a low-cost and widespread platform for parallel computing. This paper focuses on the dynamic task-scheduling problem in NOW environments. The aim is to minimize the completion time of parallel programs by distributing cooperating concurrent tasks to homogeneous networked nodes. Cooperation dependencies as well as creation and termination dependencies between tasks are taken into account. An event lattice model is introduced to describe past, actual and future behavior of a parallel program in execution. Based on this model an algorithm is presented to dynamically assign tasks to the nodes of a dedicated distributed system. Crucial for the efficiency of this approach is a top-down construction of all operating system entities involved in distributed resource management, particularly the close cooperation of the compiler and runtime system, which allows the creation of event lattice clippings at runtime.</abstract>
	<keywords>Distributed system; Cooperative system; Global scheduling; Task mapping; Task assignment</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=983>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using wait-free synchronization in the design of distributed applications</paper_heading>
	<authors>Joseph S. Berrios, Manuel E. Bermudez</authors>
	<abstract>Wait-free synchronization has been recognized in the literature as an effective concurrent programming technique. The concurrent programming community, however, has been slow to adopt this technique. This paper addresses the practical application of wait-free synchronization in the design of distributed applications. In this paper, we present an implementation of a server that uses wait-free synchronization. The resulting code is more easily seen to be fault tolerant. The performance analysis of the wait-free synchronization server outperformed a server that uses traditional locking techniques. This practical demonstration of the benefits of wait-free synchronization should help foster its adoption in the development of distributed applications.</abstract>
	<keywords>Concurrency; Non-blocking algorithms</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=984>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulation tools to study a distributed shared memory for clusters of symmetric multiprocessors</paper_heading>
	<authors>Darshan D. Thaker, Vipin Chaudhary</authors>
	<abstract>Distributed shared memory (DSM) systems have become popular as a means of utilizing clusters of computers for solving large applications. We have developed a high-performance DSM, Strings. In addition, to improve the performance of our DSM, a memory hierarchy simulator has been developed that allows us to compare various techniques very quickly and with much less effort. This paper describes our simulator, DSMSim. We show that the simulator's performance closely matches the real system and demonstrate potential performance gains of up to 60% after adding optimization features to the simulator. The simulator also accepts the same code as the software distributed shared memory.</abstract>
	<keywords>Distributed shared memory (DSM); memory hierarchy simulator; DSMSim; optimization features</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=985>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The use of configurable computing for computational kernels in scientific simulations</paper_heading>
	<authors>Mark Jones, Zahi Nakad, Paul Plassmann, Yanhua Yi</authors>
	<abstract>In many scientific simulation codes, the bulk of the floating-point arithmetic required is done by a small number of compact computational kernels. In this paper, we explore the potential use of configurable computers to instantiate the hardware required for such kernels and, thus, improve their performance. We present algorithms and analysis for two such kernels: fast, problem-specific multipliers and the efficient evaluation of Taylor series. A novel aspect of the algorithm for Taylor series evaluation is that it takes advantage of the variable precision arithmetic available to a configurable computer. Experimental results obtained on a Xilinx field-programmable gate array (FPGA) are presented for the proposed algorithms.</abstract>
	<keywords>Configurable computing; FPGA; Variable precision arithmetic; Polynomial evaluation; Taylor series approximation</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=986>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Bioinformatic searches using a single-chip shared-memory multiprocessor</paper_heading>
	<authors>Scott F. Smith, James F. Frenzel</authors>
	<abstract>A single-chip shared-memory multiprocessor architecture is introduced which is particularly well suited to common bioinformatic computing tasks. The architecture uses asynchronous bus interfaces to create an integrated circuit design methodology allowing for scaling of the multiprocessor with very little design effort. A key aspect of this design methodology is that it is not necessary to expend significant design resources and chip area on the clock tree. An analysis of the Smith–Waterman alignment algorithm running on this architecture shows that the performance penalty due to increased bus latency compared to a fully synchronous architecture is negligible.</abstract>
	<keywords>Bioinformatics; Multiprocessor; Scalable; Asynchronous interface</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=987>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A middleware architecture to facilitate distributed programming: DAROC: Data-Activated Replicated Object Communications</paper_heading>
	<authors>Brian M. Stack, Gene Hsiao, Stephen F. Jenks</authors>
	<abstract>Programming distributed computer systems is difficult because of complexities in addressing remote entities, message handling, and program coupling. As systems grow, scalability becomes critical, as bottlenecks can serialize portions of the system. When these distributed system aspects are exposed to programmers, code size and complexity grow, as does the fragility of the system. This paper describes a distributed software architecture and middleware implementation that combines object-based blackboard-style communications with data-driven and periodic application scheduling to greatly simplify distributed programming while achieving scalable performance. Data-Activated Replication Object Communications (DAROC) allows programmers to treat shared objects as local variables while providing implicit communications.</abstract>
	<keywords>Middleware; Distributed system; Data-activated; Blackboard</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=988>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Software design for finite difference schemes based on index notation</paper_heading>
	<authors>Krister Åhlander, Kurt Otto</authors>
	<abstract>A formulation of finite difference schemes based on the index notation of tensor algebra is advocated. Finite difference operators on regular grids may be described as sparse, banded, “tensors”. Especially for higher space dimensions, it is claimed that a band tensor formulation better corresponds to the inherent problem structure than does conventional matrix notation. Tensor algebra is commonly expressed using index notation. The standard index notation is extended with the notion of index offsets, thereby allowing the common traversal of band tensor diagonals. The transition from mathematical index notation to implementation is presented. It is emphasized that efficient band tensor computations must exploit the particular problem structure, which calls for a combination of general index notation software with special-purpose band tensor routines.</abstract>
	<keywords>Index notation; Finite difference scheme; Tensor bandedness; Preconditioner; Fast transform</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=989>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GrAL—the grid algorithms library</paper_heading>
	<authors>Guntram Berti</authors>
	<abstract>Dedicated library support for mesh-level geometry components, central to numerical PDE solution, is scarce. We claim that the situation is due to the inadequacy of traditional design techniques for complex and variable data representations typical for meshes. As a solution, we introduce an approach based on generic programming, implemented in the C++ library GrAL, whose algorithms are able to run on any mesh representation. We present the core design of GrAL and highlight some of its generic components. Finally, we discuss some practical issues of generic libraries, in particular efficiency and usability.</abstract>
	<keywords>Mesh algorithms; Mesh data structures; PDE solution software; Generic programming; Software reuse</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=990>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using B SP and Python to simplify parallel programming</paper_heading>
	<authors>Konrad Hinsen, Hans Petter Langtangen, Ola Skavhaug, Åsmund Ødegård</authors>
	<abstract>Scientific computing is usually associated with compiled languages for maximum efficiency. However, in a typical application program, only a small part of the code is time-critical and requires the efficiency of a compiled language. It is often advantageous to use interpreted high-level languages for the remaining tasks, adopting a mixed-language approach. This will be demonstrated for Python, an interpreted object-oriented high-level language that is well suited for scientific computing. Particular attention is paid to high-level parallel programming using Python and the BSP model. We explain the basics of BSP and how it differs from other parallel programming tools like MPI. Thereafter we present an application of Python and BSP for solving a partial differential equation from computational science, utilizing high-level design of libraries and mixed-language (Python–C or Python–Fortran) programming.</abstract>
	<keywords>BSP; Python; Parallel programming</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=991>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using CORBA middleware in finite element software</paper_heading>
	<authors>J. Lindemann, O. Dahlblom, G. Sandberg</authors>
	<abstract>Distributed middleware technologies, such as CORBA can enable finite element software to be used in a more flexible way. Adding functionality is possible without the need for recompiling client code. Applications and libraries can expose their functionality to other applications in a language neutral way, enabling a more direct and easy transfer of data, without the need for intermediate input and output files. The CORBA software components can be easily configured and distributed transparently over the network. A sample structural mechanics code, implemented in C++ is used to illustrate these concepts. Some future directions, such as placing CORBA enabled finite element software on HPC centres are also discussed.</abstract>
	<keywords>CORBA; Finite element software; C++</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=992>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>OpenMP versus MPI for PDE solvers based on regular sparse numerical operators</paper_heading>
	<authors>Markus Nordén, Sverker Holmgren, Michael Thuné</authors>
	<abstract>wo parallel programming models represented by OpenMP and MPI are compared for PDE solvers based on regular sparse numerical operators. As a typical representative of such an operator, a finite difference approximation of the Euler equations for fluid flow is considered. The comparison of programming models is made with regard to uniform memory access (UMA), non-uniform memory access (NUMA), and self-optimizing NUMA (NUMA-opt) computer architectures. By NUMA-opt, we mean NUMA systems extended with self-optimization algorithms, in order to reduce the non-uniformity of the memory access time. The main conclusions of the study are: (1) that OpenMP is a viable alternative to MPI on UMA and NUMA-opt architectures; (2) that OpenMP is not competitive on NUMA platforms, unless special care is taken to get an initial data placement that matches the algorithm; (3) that for OpenMP to be competitive in the NUMA-opt case, it is not necessary to extend the OpenMP model with additional data distribution directives, nor to include user-level access to the page migration library.</abstract>
	<keywords>OpenMP; MPI; UMA; NUMA; Optimization; PDE; EulerStencil</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=993>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A component-based architecture for parallel multi-physics PDE simulation</paper_heading>
	<authors>Steven G. Parker</authors>
	<abstract>We describe the Uintah Computational Framework (UCF), a set of software components and libraries that facilitate the simulation of partial differential equations on structured adaptive mesh refinement grids using hundreds to thousands of processors. The UCF uses a non-traditional approach to achieving parallelism, employing an abstract taskgraph representation to describe computation and communication. This representation has a number of advantages that affect the performance of the resulting simulation. We demonstrate performance of the system on a solid mechanics algorithm, two different computational fluid-dynamics (CFD) algorithms, as well as coupled CFD/mechanics algorithms. We show performance of the UCF using up to 2000 processors.</abstract>
	<keywords>Structured AMR; Components; Scientific computing</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=994>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Hardware-oriented numerics and concepts for PDE software</paper_heading>
	<authors>S. Turek, Chr. Becker, S. Kilian</authors>
	<abstract>Processor technology is still dramatically advancing and promises further enormous improvements in processing data for the next decade. On the other hand, much lower advances in moving data are expected such that the efficiency of many numerical software tools for partial differential equations (PDEs) are restricted by the cost for memory access. We demonstrate how data locality and pipelining can achieve a significant percentage of the available huge computing power, and we explain the influence of processor technology on recent and future numerical PDE simulation tools. Exemplarily, we describe hardware-oriented concepts for adaptive error control, multigrid/domain decomposition schemes and incompressible flow solvers and discuss their numerical and computational characteristics.</abstract>
	<keywords>Numerics for PDEs; Multigrid; FEM; Adaptivity; Incompressible CFD</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=995>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Conceptual interfaces in hypre</paper_heading>
	<authors>Robert D. Falgout, Jim E. Jones, Ulrike Meier Yang</authors>
	<abstract>The hypre software library is being developed with the aim of providing scalable solvers for the solution of large, sparse linear systems on massively parallel computers. To this end, the notion of conceptual interfaces was introduced. These interfaces give applications users a more natural means for describing their linear systems, and provide access to methods such as geometric multigrid which require additional information beyond just the matrix. This paper discusses the design of the conceptual interfaces in hypre and illustrates their use with various examples. A brief overview of the solvers and preconditioners available through these interfaces is also given.</abstract>
	<keywords>Conceptual interfaces; High performance preconditioners; Scalable algorithms</keywords>
	<publication_month_year>2006-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 1-2</volumes_issues>
</paper>
<paper no=996>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The impact of data replication on job scheduling performance in the Data Grid</paper_heading>
	<authors>Ming Tang, Bu-Sung Lee, Xueyan Tang, Chai-Kiat Yeo</authors>
	<abstract>In the Data Grid environment, the primary goal of data replication is to shorten the data access time experienced by the job and consequently reduce the job turnaround time. After introducing a Data Grid architecture that supports efficient data access for the Grid job, the dynamic data replication algorithms are put forward. Combined with different Grid scheduling heuristics, the performances of the data replication algorithms are evaluated with various simulations. The simulation results demonstrate that the dynamic replication algorithms can reduce the job turnaround time remarkably. In particular, the combination of shortest turnaround time scheduling heuristic (STT) and centralized dynamic replication with response-time oriented replica placement (CDR_RTPlace) exhibits remarkable performance in diverse system environments and job workloads.</abstract>
	<keywords>Data replication; Grid scheduling; Data Grid; Simulation</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=997>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A detailed MPI communication model for distributed systems</paper_heading>
	<authors>Thuy T. Le, Jalel Rejeb</authors>
	<abstract>Message Passing Interface (MPI) is the most popular communication interface used in today PC clusters and other cluster-type parallel/distributed computers. Up-to-date the most popular analytical MPI communication performance model for parallel/distributed machines is the LogGP model, which is mostly based on system hardware parameters. Due to the popularity of MPI, the improvements of connection network in the past few years, and the development of MPI for computational Grids, LogGP model needs to be re-evaluated for the detailed hardware performance and for the inclusion of middleware overheads for different data structures. In this article, we use our experiment results to show that the current LogGP communication model is too limited for today parallel/distributed systems. We propose a modification by including into the model important factors that have been left out. We itemize the terms of the model to show the consistency and the meaning of these communication costs, which we believe to be the starting point for modeling MPI communication cost on the Grids. In this work, we start with point-to-point communication and plan to extend to other communication patterns, such as the broadcast communication.</abstract>
	<keywords>MPI communication; Parallel system Distributed system; Performance modeling</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=998>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An asynchronous algorithm on the NetSolve global computing system</paper_heading>
	<authors>Nahid Emad, S.-A. Shahzadeh-Fazeli, Jack Dongarra</authors>
	<abstract>The explicitly restarted Arnoldi method (ERAM) allows one to find a few eigenpairs of a large sparse matrix. The multiple explicitly restarted Arnoldi method (MERAM) is a technique based upon a multiple projection of ERAM and accelerates its convergence [N. Emamad, S. Petiton, G. Edjlali, Multiple explicitly restarted Arnoldi method for solving large eigenproblems, SIAM J. Sci. Comput. SJSC 27 (1) (2005) 253-277]. MERAM allows one to update the restarting vector of an ERAM by taking into account the interesting eigen-information obtained by its other ERAM processes. This method is particularly well suited to the GRID-type environments. We present an adaptation of the asynchronous version of MERAM for the NetSolve global computing system. We point out some advantages and limitations of this kind of system to implement the asynchronous hybrid algorithms. We give some results of our experiments and show that we can obtain a good acceleration of the convergence compared to ERAM. These results also show the potential of the MERAM-like hybrid methods for the GRID computing environments.</abstract>
	<keywords>Large eigenproblem; Arnoldi method; Explicit restarting; Global computing; NetSolve</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=999>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Reliability challenges in large systems</paper_heading>
	<authors>Daniel A. Reed, Charng-da Lu, Celso L. Mendes</authors>
	<abstract>Clusters built from commodity PCs dominate high-performance computing today, with systems containing thousands of processors now being deployed. As node counts for multi-teraflop systems grow to tens of thousands, with proposed petaflop system likely to contain hundreds of thousands of nodes, the assumption of fully reliable hardware and software becomes much less credible. In this paper, after presenting examples and experimental data that quantify the reliability of current systems, we describe possible approaches for effective system use. In particular, we present techniques for detecting imminent failures in the environment and that allow an application to run successfully despite such failures. We also show how intelligent and adaptive software can lead to failure resilience and efficient system usage.</abstract>
	<keywords>System reliability; Fault-tolerance; Adaptive software</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1000>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A higher order estimate of the optimum checkpoint interval for restart dumps</paper_heading>
	<authors>J.T. Daly</authors>
	<abstract>This paper examines methods of approximating the optimum checkpoint restart strategy for minimizing application run time on a system exhibiting Poisson single component failures. Two different models will be developed and compared. We will begin with a simplified cost function that yields a first-order model. Then we will derive a more complete cost function and demonstrate a perturbation solution that provides accurate high order approximations to the optimum checkpoint interval.</abstract>
	<keywords>Optimal checkpointing; Poisson failures; Perturbation series; Lambert function</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1001>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Logging kernel events on clusters</paper_heading>
	<authors>Jürgen Reuter, Walter F. Tichy</authors>
	<abstract>We present tools for recording and analysing kernel events on Linux clusters. The tools provide cluster-wide event logging at system clock accuracy. We demonstrate the usefulness of our tools by verifying an implementation of a simple remote scheduling feature with interesting results regarding schedule responsiveness, by analysing local clock drift in a cluster, and by observing the effect of a kernel read-ahead policy upon sequential file read. With our GUI-based Java application, data recorded on multiple hosts is integrated for visualization. These tools can be used for analysis of cluster schedulers such as gang schedulers, SMP scheduling affinity, or the interaction between user level applications and kernel policies such as file read-ahead. Our work can be easily extended for integration with application level logging.</abstract>
	<keywords>Event logging; Kernel; Operating systems; Cluster computing; xntp</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1002>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A performance model of non-deterministic particle transport on large-scale systems</paper_heading>
	<authors>Mark M. Mathis, Darren J. Kerbyson, Adolfy Hoisie</authors>
	<abstract>In this work we present a predictive analytical model that encompasses the performance and scaling characteristics of a non-deterministic particle transport application, MCNP (Monte Carlo N-Particle), that represents part of the Advanced Simulation and Computing (ASC) workload. MCNP can be used for the simulation of neutron, photon, electron, or coupled transport, and has found uses in many problem areas including nuclear reactors, radiation shielding, and medical physics. Monte Carlo methods in general and MCNP specifically do not solve an explicit equation, but rather obtain answers by simulating the interactions between individual particles and a predefined geometry. This is in contrast to deterministic transport methods, the most common of which is the discrete ordinates method, that solve the transport equation directly for the average particle behavior. Previous studies on the scalability of parallel Monte Carlo calculations have been rather general in nature. The performance model developed here is both detailed and parametric with both application characteristics (e.g. problem size), and system characteristics (e.g. communication latency, bandwidth, achieved processing rate) serving as input. The model is validated against measurements on an AlphaServer ES40 system showing high accuracy across many processor/problem combinations. The model is then used to provide insight into the achievable performance that should be possible on systems containing thousands of processors and to quantify the impact that possible improvements in sub-system performance may have. In addition, the impact on performance of modifying the communication structure of the code is also quantified.</abstract>
	<keywords>Performance modeling; Performance analysis; Large-scale systems</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1003>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A performance prediction framework for scientific applications</paper_heading>
	<authors>Laura Carrington, Allan Snavely, Nicole Wolter</authors>
	<abstract>This work presents the results of ongoing investigations in the development of a performance modeling framework, developed by the Performance Modeling and Characterization (PMaC) Lab at the San Diego Supercomputer Center. The framework is faster than traditional cycle-accurate simulation, more sophisticated than performance estimation based on system peak-performance metrics, and is shown to be effective on benchmarks and scientific applications. This paper focuses on one such functionality by investigating sensitivity studies to further understand observed and anticipated effect of both the architecture and the application in predicted runtime.</abstract>
	<keywords>Performance modeling; Performance prediction; HPC</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1004>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scaling applications to massively parallel machines using Projections performance analysis tool</paper_heading>
	<authors>Laxmikant V. Kalé, Gengbin Zheng, Chee Wai Lee, Sameer Kumar</authors>
	<abstract>Some of the most challenging applications to parallelize scalably are the ones that present a relatively small amount of computation per iteration. Multiple interacting performance challenges must be identified and solved to attain high parallel efficiency in such cases. We present case studies involving NAMD, a parallel classic molecular dynamics application for large biomolecular systems, and CPAIMD, Car-Parrinello ab initio molecular dynamics application, and efforts to scale them to large number of processors. Both applications are implemented in Charm++, and the performance analysis was carried out using Projections, the performance visualization/analysis tool associated with Charm++. We showcase a series of optimizations facilitated by Projections. The resultant performance of NAMD led to a Gordon Bell award at SC 2002 with unprecedented speedup on 3000 processors with teraflops level peak performance. We also explore the techniques for applying the performance visualization/analysis tool on future generation extreme-scale parallel machines and discuss the scalability issues with Projections.</abstract>
	<keywords>Performance analysis; Performance visualization; Molecular dynamics; NAMD</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1005>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Compressible memory data structures for event-based trace analysis</paper_heading>
	<authors>Andreas Knüpfer, Wolfgang E. Nagel</authors>
	<abstract>The paper presents a new compressible memory data structure for trace events. Its primary intention is to aid the analysis of huge traces by reducing the memory requirements significantly. Furthermore, customized evaluation algorithms reduce the computational effort. The data structure as well as algorithms for construction and evaluation are discussed in detail. Experiments with real-life traces demonstrate the theoretically derived capabilities of the new approach.</abstract>
	<keywords>Performance analysis; Tracing; Data structures; Compression</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1006>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance feature identification by comparative trace analysis</paper_heading>
	<authors>Daniel P. Spooner, Darren J. Kerbyson</authors>
	<abstract>This work introduces a method for instrumenting applications, producing execution traces, and visualizing multiple trace instances to identify performance features. The approach provides information on the execution behavior of each process within a parallel application and allows differences across processes to be readily identified. Traces events are directly related to the source code and call-chain that produced them. This allows the identification of the causes of events to be easily obtained. The approach is particularly suited to aid in the understanding of the achieved performance from an application centric viewpoint. In particular, it can be used to assist in the formation of analytical performance models which can be a time-consuming task for large complex applications. The approach is one of human-effort reduction: focus the interest of the performance specialist on performance critical code regions rather than automating the performance model formulation process completely. A supporting implementation analyses trace files from different runs of an application to determine the relative performance characteristics for critical regions of code and communication functions.</abstract>
	<keywords>Performance modeling; High performance systems; Dynamic trace analysis; Performance visualization</keywords>
	<publication_month_year>2006-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 3</volumes_issues>
</paper>
<paper no=1007>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Comments on a theorem on grid access control</paper_heading>
	<authors>Libin Wang, Kefei Chen</authors>
	<abstract>We analyze the access control model that was proposed by Xu and Bu. We demonstrate the weaknesses of the model, and show that the model may be vulnerable to information disclosure attacks.</abstract>
	<keywords>Access control; Grid; Security model; Security policy</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1008>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Web-based grid-enabled interaction with 3D medical data</paper_heading>
	<authors>Branko Marović, Zoran Jovanović</authors>
	<abstract>The grid-enabled Volumetric Image Visualization Environment (VIVE) is an interactive analysis tool for 3D medical images, facilitating diagnosis, surgical planning, therapy evaluation, and remote 3D examination. It improves understanding of complex anatomies by providing an interactive three-dimensional environment with simple Web-based user interface. The lightweight Java/VRML client enables user interaction and 3D rendering within a Web browser. The user interacts with the visualization server running on the grid through the specialized portal responsible for job submission and communication with server jobs. The grid provides computing and storage resources, as well as access methods required for volumetric data processing.</abstract>
	<keywords>Interactive grids; Grid portals; Medical imaging; 3D visualization</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1009>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Stability analysis of frequency and step length dependent Runge–Kutta–Nyström methods</paper_heading>
	<authors>M. Carpentieri, B. Paternoster</authors>
	<abstract>We present a symbolic–numerical package to analyze the linear stability properties of Runge–Kutta–Nyström methods for y′′=f(x,y), which are suitable for ordinary differential equations having periodic or oscillatory solutions, having frequency and step length dependent parameters. As an example, we apply this analysis to a special class of numerical methods appeared in the literature, which are based on trigonometric polynomials.</abstract>
	<keywords>Numerical methods for ordinary differential equations; Runge–Kutta–Nyström methods; Stability analysis</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1010>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On the computation of few eigenvalues of positive definite Hamiltonian matrices</paper_heading>
	<authors>Pierluigi Amodio</authors>
	<abstract>Given a Hamiltonian matrix H=JS with S symmetric and positive definite, we analyze a symplectic Lanczos algorithm to transform −H2 in a symmetric and positive definite tridiagonal matrix of half size. By means of two effective restarted procedures, this algorithm is then used to compute few extreme eigenvalues of H. Numerical examples are also reported to compare the presented techniques.</abstract>
	<keywords>Hamiltonian matrices; Eigenvalues; Lanczos algorithm</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1011>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Conservation properties of multisymplectic integrators</paper_heading>
	<authors>A.L. Islas, C.M. Schober</authors>
	<abstract>Recent results on the local and global properties of multisymplectic discretizations of Hamiltonian PDEs are discussed. We consider multisymplectic (MS) schemes based on Fourier spectral approximations and show that, in addition to a MS conservation law, conservation laws related to linear symmetries of the PDE are preserved exactly. We compare spectral integrators (MS versus non-symplectic) for the nonlinear Schrödinger (NLS) equation, focusing on their ability to preserve local conservation laws and global invariants, over long times. Using Lax-type nonlinear spectral diagnostics we find that the MS spectral method provides an improved resolution of complicated phase space structures.</abstract>
	<keywords>Multisymplectic integrators; Hamiltonian PDEs; Conservation laws; Long-time dynamics</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1012>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Numerical methods for computing SVD in the D-orthogonal group</paper_heading>
	<authors>T. Politi, A. Pugliese</authors>
	<abstract>In this paper, we consider the problem to compute a special kind of singular value decomposition of a square matrix A=UΣV, where U and V belong to the same D-orthogonal group, i.e. they are orthogonal with respect to a real diagonal orthogonal matrix D, while Σ is a real diagonal positive definite matrix. In this work, we propose an algebraic method and derive a continuous approach using the projected gradient technique. The differential systems given by the continuous approach are solved using a standard integration solver together with a projection technique obtained computing the D-orthogonal factor of the hyperbolic QR decomposition.</abstract>
	<keywords>Singular value decomposition; Quadratic groups; Hypernormal matrices</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1013>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fixed-point neural independent component analysis algorithms on the orthogonal group</paper_heading>
	<authors>Simone Fiori</authors>
	<abstract>Fixed-point algorithms for neural-network learning have gained increasing interest over recent years. In blind signal processing by neural systems, they normally appear in the literature as one-unit learning rules. The aim of the present short contribution is to introduce some fully-parallel (multi-unit) learning algorithms on the group of orthogonal matrices to be applied to independent component analysis.</abstract>
	<keywords>Fixed-point iteration; Independent component analysis; Orthogonal group</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1014>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A differential approach to solve the inverse eigenvalue problem derived from a neural network</paper_heading>
	<authors>N.Del Buono</authors>
	<abstract>We consider the problem of designing a general additive neural network which possesses prescribed equilibria. The relation between this design problem and a problem of generating a matrix with specified eiegenvalues, which maps a given set of vectors of another given set, is investigated. The obtained inverse eigenvalue problem is then solved using a gradient flow approach. Working with discretisation of systems of differential equations allows to preserve the original dimension of the problem and could give the possibility of constructing adaptive schemes faster than algebraic one.</abstract>
	<keywords>Inverse eigenvalue problem; Gradient flow approach; Neural network</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1015>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Large scale parallel FEM computations of far/near stress field changes in rocks</paper_heading>
	<authors>R. Blaheta, P. Byczanski, O. Jakl, R. Kohut, ... J. Starý</authors>
	<abstract>The paper describes an application of large scale finite element analysis for the assessment of stress changes in rocks induced by mining. This application allows us to illustrate efficiency of an approach based mainly on problem decomposition and parallel computing. For the main part of the computations, which is the solution of large linear systems, we use two decomposition techniques (displacement and domain decomposition). Another decomposition technique is exploited for local FE analysis of the near stress field on so called composite grids. The described methods are tested on the solution of two benchmark problems, which originate from the modelling of the effects of mining. The parallel computations are performed on a small Linux cluster.</abstract>
	<keywords>large scale finite element analysis; parallel computing; large linear systems; displacement and domain decomposition</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1016>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Use of parallel computing and visualisation techniques in the simulation of large scale geoenvironmental engineering problems</paper_heading>
	<authors>P.J. Cleall, H.R. Thomas, T.A. Melhuish, D.H. Owen</authors>
	<abstract>This paper discusses some of the computational and visualisation challenges encountered in the field of geoenvironmental engineering. The use of iterative solvers and parallel computation methods are identified as being of particular relevance to addressing the computation challenges. 3D stereoscopic visualisation is discussed in relation to data analysis and communication. An example analysis is presented related to the issue of nuclear waste disposal where the use of iterative solvers and parallel algorithms was found to yield significant results.</abstract>
	<keywords>Parallel computing; 3D visualisation; Geoenvironmental; Geoscience</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1017>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Worst scenario and domain decomposition methods in geomechanics</paper_heading>
	<authors>I. Hlaváček, J. Nedoma, J. Daněk</authors>
	<abstract>In geomechanics, there are problems whose investigations lead to solving model problems based on variational formulations. Such problems are frequently formulated by variational inequalities as they physically describe the principle of virtual work in its inequality form. In the first part of the contribution, the algorithm for the numerical solution of the discussed variational inequality problem will be investigated. The used parallel algorithm is based on a nonoverlapping domain decomposition method for unilateral contact problem with the given friction and the finite element approach. The conditions of solvability will be presented. In the second part of the contribution, a unilateral contact problem with friction and with uncertain input data in quasi-coupled thermo-elasticity is analysed. Method of worst scenario will be applied to find the most “dangerous” admissible input data. The solvability of the corresponding worst scenario (antioptimization) problem will be shortly discussed. Numerical experiments, e.g. a tunnel crossing by an active fault will be presented.</abstract>
	<keywords>numerical solution; parallel algorithm; nonoverlapping domain; decomposition method;  quasi-coupled thermo-elasticity </keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1018>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On a solvability of contact problems with visco-plastic friction in the thermo-visco-plastic Bingham rheology</paper_heading>
	<authors>Jiří Nedoma</authors>
	<abstract>This paper deals with the solvability of contact problems with a local visco-plastic friction in the thermo-visco-plastic Bingham rheology. The generalized case of bodies of arbitrary shapes being in mutual contacts is investigated. The model problem represents mathematical models of the Earth’s mantle movements, of the volcanic zones, etc. Numerical approaches in the dynamic case, based on the semi-implicit scheme in time and a finite element approximation in the space, and in the stationary flow case, based on the penalization, regularization and finite element techniques and semi-implicit scheme in thermal part of the problem, are shortly developed and discussed.</abstract>
	<keywords> solvability of contact problems; local visco-plastic friction; thermo-visco-plastic Bingham rheology; mathematical model; Numerical approaches</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1019>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Long-range static directional stress transfer in a cracked, nonlinear elastic crust</paper_heading>
	<authors>G. Ouillon, D. Sornette</authors>
	<abstract>Seeing the Earth crust as criss-crossed by faults filled with fluid at close to lithostatic pressures, we develop a model in which its elastic modulii are different in net tension versus compression. In constrast with standard nonlinear effects, this “threshold nonlinearity” is non-perturbative and occurs for infinitesimal perturbations around the lithostatic pressure taken as the reference. For a given earthquake source, such nonlinear elasticity is shown to (i) rotate, widen or narrow the different lobes of stress transfer, (ii) to modify the  2D-decay of elastic stress Green functions into the generalized power law , where  depends on the azimuth and on the amplitude of the modulii asymmetry. Using reasonable estimates, this implies an enhancement of the range of interaction between earthquakes by a factor up to 5–10, that is, stress perturbation of 0.1 bar or more are found up to distances of several tens of the rupture length. This may explain certain long-range earthquake triggering and hydrological anomalies in wells and suggest to revisit the standard stress transfer calculations which use linear elasticity. We also show that the standard double-couple of forces representing an earthquake source leads to an opening of the corresponding fault plane, which suggests a mechanism for the non-zero isotropic component of the seismic moment tensor observed for some events.</abstract>
	<keywords>criss-crossed; lithostatic pressures; elastic moduli; threshold nonlinearity; non-perturbative</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1020>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Finite element approximation to infinite Prandtl number Boussinesq equations with temperature-dependent coefficients—Thermal convection problems in a spherical shell</paper_heading>
	<authors>Masahisa Tabata</authors>
	<abstract>A stabilized finite element scheme for infinite Prandtl number Boussinesq equations with temperature-dependent coefficients is analyzed. The domain is a spherical shell and the P1-element is employed for every unknown function. The finite element solution is proved to converge to the exact one in the first order of the time increment and the mesh size. The scheme is applied to Earth’s mantle convection problems with viscosities strongly dependent on the temperature and some numerical results are shown.</abstract>
	<keywords>Stabilized finite element scheme; Infinite Prandtl number Boussinesq equations; Temperature-dependent coefficients; Thermal convection problems</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1021>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed Point Objects: A new concept for parallel finite elements applied to a geomechanical problem</paper_heading>
	<authors>Christian Wieners, Martin Ammann, Wolfgang Ehlers</authors>
	<abstract>We present a new concept for the realization of finite element computations on parallel machines which is based on a dynamic data structure address by points. All geometric objects (cells, faces, edges) are referenced by its midpoint, and all algebraic data structures (vectors and matrices) are tied to the nodal points of the finite elements. Together, they build Distributed Point Objects (DPO), where the parallel distribution is made transparent by processor lists assigned to the points. All objects are stored in hash tables (where the keys are points) so that pointers can be completely avoided. Then, we consider the application of the parallel programming model to a geomechanical porous media problem. This work complements our previous work [C. Wieners, M. Ammann, S. Diebels, W. Ehlers, Parallel 3-D simulations for porous media models in soil mechanics, Comput. Mech. 29 (2002) 75–87], where the geomechanical model, the interface of the finite element code and the parallel solver is described in detail. Here, we discuss the parallel data structure and the parallel performance for a characteristic application. Together, this demonstrates that demanding 3-D non-linear and time-dependent engineering applications on unstructured meshes can be parallelized very efficiently within a very small overhead for the parallel implementation.</abstract>
	<keywords>parallel machines; dynamic data structure; cells, faces, edges; Distributed Point Objects (DPO)</keywords>
	<publication_month_year>2006-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 4</volumes_issues>
</paper>
<paper no=1022>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Securing web services for deployment in health grids</paper_heading>
	<authors>D.J. Power, E.A. Politou, M.A. Slaymaker, A.C. Simpson</authors>
	<abstract>In this paper we describe an approach to the facilitation of system-wide security that enables fine-grained access control within systems in which third party web services are deployed. The primary motivation for the work comes from the authors’ research into the development of grid-enabled healthcare systems (or health grids). Indeed, we would argue that if the e-Health dream is to become reality, then there is a clear need for web services that enable remote access to medical data to be secured in an appropriate fashion. We compare our approach of wrapping web services with alternative approaches based on generic SOAP proxies. As an illustrative example we describe how the OGSA-DAI grid services have been secured via XACML-based access control policies.</abstract>
	<keywords>Security; Healthcare; Web services; Access control; Medical informatics</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1023>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Examination of load-balancing methods to improve efficiency of a composite materials manufacturing process simulation under uncertainty using distributed computing</paper_heading>
	<authors>Feng Zhang, Andryas Mawardi, Eugene Santos, Ranga Pitchumani, Luke E.K. Achenie</authors>
	<abstract>Process simulations play an important role in guiding process understanding and development, without requiring costly manufacturing trials. For process design under uncertainty, a large number of simulations is needed for an accurate convergence of the moments of the output distributions, which renders such stochastic analysis computationally intensive. This paper discusses the application of a basic distributed computing approach to reduce the computation time of a composite materials manufacturing process simulation under uncertainty. Specifically, several load-balancing methods are explored and analyzed to determine the best strategies given heterogeneous tasks and heterogeneous networks, especially when the individual task times cannot be predicted.</abstract>
	<keywords>Load balancing; Distributed computing; Design under uncertainty; Pultrusion process simulation; Processing variability</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1024>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>PGGA: A predictable and grouped genetic algorithm for job scheduling</paper_heading>
	<authors>Maozhen Li, Bin Yu, Man Qi</authors>
	<abstract>This paper presents a predictable and grouped genetic algorithm (PGGA) for job scheduling. The novelty of the PGGA is two-fold: (1) a job workload estimation algorithm is designed to estimate a job workload based on its historical execution records and (2) the divisible load theory (DLT) is employed to predict an optimal fitness value by which the PGGA speeds up the convergence process in searching a large scheduling space. Comparison with traditional scheduling methods, such as first-come-first-serve (FCFS) and random scheduling, heuristics, such as a typical genetic algorithm, Min–Min and Max–Min indicates that the PGGA is more effective and efficient in finding optimal scheduling solutions.</abstract>
	<keywords>Job scheduling; Job workload estimation; Divisible load theory; Predictable genetic algorithm; Load balancing</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1025>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and evaluation of an efficient proportional-share disk scheduling algorithm</paper_heading>
	<authors>Young Jin Nam, Chanik Park</authors>
	<abstract>Proportional-share algorithms are designed to allocate an available resource, such as a network, processor, or disk, for a set of competing applications in proportion to the resource weight allotted to each. While a myriad of proportional-share algorithms were made for network and processor resources, little research work has been conducted on disk resources, which exhibit non-linear performance characteristics attributed to disk head movements. This paper proposes a new proportional-share disk-scheduling algorithm, which accounts for overhead caused by disk head movements and QoS guarantees in an integrated manner. Performance evaluations via simulations reveal that the proposed algorithm improves I/O throughput by 11–19% with only 1–2% QoS deterioration.</abstract>
	<keywords>Proportional shareStorage QoS; Disk resource; Disk head movement overhead</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1026>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Adaptive parallel I/O scheduling algorithm for multiprogrammed systems</paper_heading>
	<authors>J.H. Abawajy</authors>
	<abstract>As the rate at which disk drives read and write data is improving at a much slower pace than the speed of processors, I/O has risen to become the bottleneck in high-performance computing for many applications. A possible approach to address this problem is to schedule parallel I/O operations explicitly. To this end, we propose two new I/O scheduling algorithms and evaluate the relative performance of the proposed policies against two baseline policies. Simulation results show that the proposed policies outperform the baseline policies.</abstract>
	<keywords>I/O scheduling algorithm; Cluster computing; Parallel I/O; Performance analysis</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1027>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>MAPFS: A flexible multiagent parallel file system for clusters</paper_heading>
	<authors>María S. Pérez, Jesús Carretero, Félix García, José M. Peña, Víctor Robles</authors>
	<abstract>The emergence of applications with greater processing and speedup requirements, such as Grand Challenge Applications (GCA), involves new computing and I/O needs. Many of these applications require access to huge data repositories and other I/O sources, making the I/O phase a bottleneck in the computing systems, due to its poor performance. In this sense, parallel I/O is becoming one of the major topics in the area of high-performance systems. Existing data-intensive GCA have been used in several domains, such as high energy physics, climate modeling, biology or visualization. Since the I/O problem has not been solved in this kind of applications, new approaches are required in this case. This paper presents MAPFS, a multiagent architecture, whose goal is to allow applications to access data in a cluster of workstations in an efficient and flexible fashion, providing formalisms for modifying the topology of the storage system, specifying different data access patterns and selecting additional functionalities.</abstract>
	<keywords>Parallel I/O; Cluster computing; Multiagent architectures; Access patterns</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1028>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Home-based Cooperative Cache for parallel I/O applications</paper_heading>
	<authors>In-Chul Hwang, Seung-Ryoul Maeng, Jung-Wan Cho</authors>
	<abstract>Recently, there has been much research about cluster computing to get high performance using low-cost nodes connected with high-speed networks. In many research fields, many I/O subsystems which access disks slower than any other component in cluster computing have been proposed such as parallel distributed file systems and Cooperative Cache. In many I/O subsystems, Cooperative Cache was proposed to reduce servers’ load and to achieve high performance by sharing nodes’ caches. Previous researches about Cooperative Cache focus on how to efficiently manage read caches and do not care about the write policy. In parallel I/O applications, which use cluster computing, there are many write accesses as well as read accesses. To improve the overall I/O performance, it is necessary to improve write performance as well as read performance. In this paper, we propose the home-based Cooperative Cache. In the home-based Cooperative Cache, every block has its static home. The home node of a block does the read caching and the write buffering of the block. We apply the home-based Cooperative Cache to Parallel Virtual File System (PVFS) and Cooperative Cluster File System (CCFS). Also, we evaluate and analyze the performance of the home-based Cooperative Cache in PVFS and in CCFS.</abstract>
	<keywords>Cooperative Cache; PVFS; CCFS File system; Cluster</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1029>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed smart disks for I/O-intensive workloads on switched interconnects</paper_heading>
	<authors>Steve C. Chiu, Wei-keng Liao, Alok N. Choudhary</authors>
	<abstract>Disk-based smart storage represents processor-embedded active I/O devices that are equipped with a dedicated network interface controller and on-disk storage (memory and disk). Today’s I/O-intensive workloads require architectures whose performance scales with increasing storage capacity. The advent of switch-based, low-latency and high-bandwidth network protocols, such as Virtual Interface Architecture and InfiniBand, promises to improve both performance and scalability beyond current state-of-the-art I/O interconnect topologies. This paper evaluates a disk-based distributed smart storage architecture composed of fully distributed processor-embedded disks that are interconnected by a high-performance switched network, and investigates the performance of the proposed processing model on different storage interconnects with representative workloads, including TPC-H queries, association rules mining, high-dimensional data clustering, and two-dimensional fast Fourier transform (FFT).</abstract>
	<keywords>Parallel I/O; Infini;Band; Storage interconnect; Cluster computing</keywords>
	<publication_month_year>2006-04-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 5</volumes_issues>
</paper>
<paper no=1030>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>BioSimGrid: Grid-enabled biomolecular simulation data storage and analysis</paper_heading>
	<authors>Muan Hong Ng, Steven Johnston, Bing Wu, Stuart E. Murdock, ... Paul Jeffreys</authors>
	<abstract>In computational biomolecular research, large amounts of simulation data are generated to capture the motion of proteins. These massive simulation data can be analysed in a number of ways to reveal the biochemical properties of the proteins. However, the legacy way of storing these data (usually in the laboratory where the simulations have been run) often hinders a wider sharing and easier cross-comparison of simulation results. The data is commonly encoded in a way specific to the simulation package that produced the data and can only be analysed with tools developed specifically for that simulation package. The BioSimGrid platform seeks to provide a solution to these challenges by exploiting the potential of the Grid in facilitating data sharing. By using BioSimGrid either in a scripting or web environment, users can deposit their data and reuse it for analysis. BioSimGrid tools manage the multiple storage locations transparently to the users and provide a set of retrieval and analysis tools for processing the data in a convenient and efficient manner. This paper details the usage and implementation of BioSimGrid using a combination of commercial databases, the Storage Resource Broker and Python scripts, gluing the building blocks together. It introduces a case study of how BioSimGrid can be used for better storage, retrieval and analysis of biomolecular simulation data.</abstract>
	<keywords>Biomolecular simulation; Database; Grid computing; Storage resource broker; Python</keywords>
	<publication_month_year>2006-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 6</volumes_issues>
</paper>
<paper no=1031>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Scheduling workflow applications on processors with different capabilities</paper_heading>
	<authors>Zhiao Shi, Jack J. Dongarra</authors>
	<abstract>Efficient scheduling of workflow applications represented by weighted directed acyclic graphs (DAG) on a set of heterogeneous processors is essential for achieving high performance. The optimization problem is NP-complete in general. A few heuristics for scheduling on heterogeneous systems have been proposed recently. However, few of them consider the case where processors have different capabilities. In this paper, we present a novel list scheduling based algorithm to deal with this situation. The algorithm (SDC) has two distinctive features. First, the algorithm takes into account the effect of Percentage of Capable Processors (PCP) when assigning the task node weights. For two task nodes with same average computation cost, our weight assignment policy tends to give higher weight to the task with small PCP. Secondly, during the processor selection phase, the algorithm adjusts the effective Earliest Finish Time strategy by incorporating the average communication cost between the current scheduling node and its children. Comparison study shows that our algorithm performs better than related work overall.</abstract>
	<keywords>Workflow; DAG; Task scheduling; List scheduling; Heterogeneous systems</keywords>
	<publication_month_year>2006-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 6</volumes_issues>
</paper>
<paper no=1032>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Task distribution with a random overlay network</paper_heading>
	<authors>Ladislau Bölöni, Damla Turgut, Dan C. Marinescu</authors>
	<abstract>We consider a model where commodity service providers are offering commodity computational services to a set of customers. We provide a solution for the efficient distribution of tasks by forwarding the service requests on an overlay network comprised on random cycles. We introduce algorithms for the creation, maintenance and repair of the overlay network. We discuss two algorithms, random wandering and weighted stochastic forwarding, for the allocation of the tasks to providers. Both approaches are highly scalable because the algorithms use only limited local information. As we are designing our approach for use in a commercial setting, there is a requirement that the tasks, being a source of profits, be allocated fairly to the providers. We investigate the fairness of the algorithms and show that adding a random pre-walk can improve the fairness. Through a simulation study we show that the approach provides efficient task allocation on networks loaded up to 95% of their capacity.</abstract>
	<keywords>Grid computing; Commodity computers; Routing</keywords>
	<publication_month_year>2006-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 6</volumes_issues>
</paper>
<paper no=1033>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GMPLS-based service differentiation for scalable QoS support in all-optical Grid applications</paper_heading>
	<authors>Francesco Palmieri</authors>
	<abstract>In the forthcoming new era of truly distributed computing, industry, businesses, and home users are placing complex and challenging demands on the transport network, now powered by the emerging photonic technologies, about Quality-of-Service (QoS) assurances that are required for new real-time computing and storage service applications geographically distributed worldwide according to the Grid model. There is the need to devise mechanisms for QoS provisioning in IP over WDM networks that must consider the physical characteristics and limitations of the optical domain to ensure the proper treatment of service classes when passing from the electrical switching to the optical domain and back. In addition, these mechanisms should be directly accessible to Grid applications to make them able to request and release network resources as they need. A (G)MPLS-based control plane combined with a wavelength-routed optical network is seen as a very promising approach for the realization of transport infrastructures for the future “photonic empowered” Grid computing paradigm, since it allows native user-controlled bandwidth resources and class-of-service provisioning, that is one of the strongest requirements for truly distributed computing. Considering this, we propose a general framework for providing differentiated services QoS to Grid applications in wavelength-routed photonic networks, built on the strengths of GMPLS for dynamic path selection and wavelength assignment. This framework makes it technically and economically viable to think of a set of computing, storage or combined computing storage nodes coupled through a high-performance optical network as one large computational and storage device.</abstract>
	<keywords>Grid computing; GMPLS; QoS; DiffServ; Optical transport networks; WDM</keywords>
	<publication_month_year>2006-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 6</volumes_issues>
</paper>
<paper no=1034>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The QuarkNet/Grid Collaborative Learning e-Lab</paper_heading>
	<authors>Marjorie Bardeen, Eric Gilbert, Thomas Jordan, Paul Nepywoda, ... Yong Zhao</authors>
	<abstract>This paper describes a case study that uses grid computing techniques to support the collaborative learning of high school students investigating cosmic rays. Students gather and upload science data to an e-Lab website. They explore those data using techniques from the GriPhyN collaboration. These techniques include virtual data transformations, workflows, metadata cataloging and indexing, data product provenance and persistence, as well as job planners for execution locally and on the grid. Students use web browsers and a custom interface that extends the GriPhyN Chiron portal to perform all of these tasks. They share results in the form of online posters and ask each other questions in this asynchronous environment. Students can discover and extend the research of other students, modeling the processes of modern large-scale scientific collaborations. Also, the e-Lab provides tools for teachers to guide student work throughout an investigation.http://quarknet.uchicago.edu/elab/cosmic</abstract>
	<keywords>Collaborative learning; Collaborative computing; Distance learning; Education; Human factors; Hypertext/hypermedia; Information systems education; Distributed systems; Online computation; Physics; Web-based interaction</keywords>
	<publication_month_year>2006-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 6</volumes_issues>
</paper>
<paper no=1035>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A semantic approach to discovering learning services in grid-based collaborative systems</paper_heading>
	<authors>Guillermo Vega-Gorgojo, Miguel L. Bote-Lorenzo, Eduardo Gómez-Sánchez, Yannis A. Dimitriadis, Juan I. Asensio-Pérez</authors>
	<abstract>CSCL systems can benefit from using grids since they offer a common infrastructure enabling access to an extended pool of resources that can provide supercomputing capabilities as well as specific hardware resources. Adopting a service oriented architecture such as OGSA can further benefit CSCL systems, enabling increased flexibility to adapt and reuse learning software offered by third party providers. However, service discovery is a challenge for educators, since they cannot use their own domain abstractions to search for learning services that may support their educational settings. Common service discovery mechanisms, such as the Index Service or UDDI, provide limited discovery capabilities since they rely on keyword matching and cannot deal with the description of service properties. In order to address these drawbacks, formal semantics of ontologies can be employed to represent semantic descriptions of services that can be exploited for service discovery. This paper proposes an ontology of CSCL tools that uses meaningful learning abstractions to describe them. That ontology is the basis of a service discovery facility that is developed for allowing educators to search service-based CSCL tools using learning concepts.</abstract>
	<keywords>Semantic web services; Service discovery; Learning services; CSCL systems</keywords>
	<publication_month_year>2006-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 6</volumes_issues>
</paper>
<paper no=1036>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Evaluating architectures for independently auditing service level agreements</paper_heading>
	<authors>Ana Carolina Barbosa, Jacques Sauvé, Walfredo Cirne, Mirna Carelli</authors>
	<abstract>Web and grid services are quickly maturing as a technology that allows for the integration of applications belonging to different administrative domains, enabling much faster and more efficient business-to-business arrangements. For such an integration to be effective, the provider and the consumer of a service must negotiate a service level agreement (SLA), i.e. a contract that specifies what one party can expect from the other. But, since SLAs are just contracts, auditing is key to assure that they hold. However, auditing can be very challenging when the parties do not blindly trust each other, which is expected to be the common case for large grid deployments. We here evaluate six architectures that perform SLA auditing both quantitatively and qualitatively. The quantitative evaluation focuses on the performance penalty that auditing introduces. The qualitative evaluation compares the architectures based on aspects such as intrusiveness, trust, use of extra requests, possibility of preferential treatment, possibility of auditing consumer load, and possibility of auditing encrypted messages. We conclude that no single architecture seems to be the best solution for all cases and indicate where each one is best suited.</abstract>
	<keywords>Service auditingIndependent auditing; Grid; Web services; Service level agreements</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1037>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On neighbor-selection strategy in hybrid peer-to-peer networks</paper_heading>
	<authors>Simon G.M. Koo, Karthik Kannan, C.S. George Lee</authors>
	<abstract>Recently, computer users have increased their interests in downloading large-volume contents like movies and software from the Internet, and a new generation of peer-to-peer (P2P) file-swapping applications that aims at improving the efficiency of transfer are gaining popularity. In these P2P applications, a central “tracker” decides about which peer becomes a neighbor to which other peers. This decision critically dictates the distribution efficiency. In this paper, we investigate the neighbor-selection process in this new category of P2P networks, and we propose an efficient neighbor-selection strategy that performs better than the existing ones. We show that by increasing the content availability of peers from their immediate neighbors, it can significantly improve system performance without trading off users’ satisfaction. We also show that the proposed strategy is incentive-compatible and is suitable for real-time deployment.</abstract>
	<keywords>Peer-to-peer networks; Neighbor selection; Genetic algorithms</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1038>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance prediction and its use in parallel and distributed computing systems</paper_heading>
	<authors>Stephen A. Jarvis, Daniel P. Spooner, Helene N. Lim Choi Keung, Junwei Cao, ... Graham R. Nudd</authors>
	<abstract>Performance prediction is set to play a significant role in supportive middleware that is designed to manage workload on parallel and distributed computing systems. This middleware underpins the discovery of available resources, the identification of a task’s requirements and the match-making, scheduling and staging that follow. This paper documents two prediction-based middleware services that address the implications of executing a particular workload on a given set of resources. These services are based on an established performance prediction system that is employed at both the local (intra-domain) and global (multi-domain) levels to provide dynamic workload steering. These additional facilities bring about significant performance improvements, the details of which are presented with regard to system- and user-level qualities of service. The middleware has been designed for the management of resources and distributed workload across multiple administrative boundaries, a requirement that is of central importance to grid computing.</abstract>
	<keywords>Performance prediction; Resource management; Grid computing</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1039>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Alternative approaches to multicast group management in large-scale distributed interactive simulation systems</paper_heading>
	<authors>Azzedine Boukerche, Caron Dzermajko, Kaiyuan Lu</authors>
	<abstract>A primary concern in implementing and executing large-scale distributed simulations is limiting and controlling the volume of data, regarding simulated entities, exchanged between entities participating in the simulation. Computer scientists in the academic, military and corporate world have spent much time studying and applying various methods of Data Distribution Management (DDM) to learn the strengths and weaknesses of each approach, improve upon existing DDM methods, and discover the most efficient method to use for a particular application. The key to efficient DDM is to successfully limit the data sent to only the data that is needed, and to direct that data to only those entities requiring the data. In this paper, we explain the benefits and goals of DDM, define and describe the various methods of DDM, compare similarities and differences in DDM methods, and discuss several existing DDM implementations. In our discussion of DDM methods, we include Region-Based, Fixed and Dynamic Grid-Based, Hybrid, Agent-Based and Class-Based DDM and compare and contrast these methods and their applications. We also discuss existing High-Level Architecture (HLA)-compliant and non-compliant Run-Time Infrastructures (RTI) and their DDM implementations. Our goal is to promote an understanding of the benefits of DDM and to offer a detailed explanation of the available DDM methods and the RTIs that employ those methods.</abstract>
	<keywords>Data Distribution Management (DDM); large-scale distributed; regarding simulated entitie; DDM method; Dynamic Grid-Based, Hybrid, Agent-Based; Class-Based DDM</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1040>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Benchmarking parallel compilers: A UPC case study</paper_heading>
	<authors>Tarek A. El-Ghazawi, François Cantonnet, Yiyi Yao, Smita Annareddy, Ahmed S. Mohamed</authors>
	<abstract>Unified Parallel C (UPC) is an explicit parallel extension to ISO C which follows the Partitioned Global Address Space (PGAS) programming model. UPC, therefore, combines the ability to express parallelism while exploiting locality. To do so, compilers must embody effective UPC-specific optimizations. In this paper we present a strategy for evaluating the performance of PGAS compilers. It is based on emulating possible optimizations and comparing the performance to the raw compiler performance. It will be shown that this technique uncovers missed optimization opportunities. The results also demonstrate that, with such automatic optimizations, the UPC performance will be compared favorably with other paradigms.</abstract>
	<keywords>Distributed shared memory; Unified Parallel C; Compilers; Non-Uniform Memory Accesses Architecture; Benchmark; NAS parallel benchmark</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1041>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Sequential in-core sorting performance for a SQL data service and for parallel sorting on heterogeneous clusters</paper_heading>
	<authors>Christophe Cérin, Michel Koskas, Hazem Fkaier, Mohamed Jemni</authors>
	<abstract>The aim of the paper is to introduce techniques in order to tune sequential in-core sorting algorithms in the frameworks of two applications. The first application is parallel sorting when the processor speeds are not identical in the parallel system. The second application is the Zeta-Data Project [M. Koskas, A hierarchical database management algorithm, in: Annales 67 du Lamsade, vol. 2, 2004, pp. 277–317. [9]] whose aim is to develop novel algorithms for databases issues. About 50% of the work done in building indexes is devoted to sorting sets of integers. We develop and compare algorithms built to sort with equal keys. Algorithms are variations of the 3Way-Quicksort of Sedgewick. In order to observe performances and to fully exploit functional units in processors, and also in order to optimize the use of the memory system and the different functional units, we use hardware performance counters that are available on most modern microprocessors. We also develop analytical results for one of our algorithms and compare expected results with the measures. For the two applications, we show, through fine experiments on an Athlon processor (a three-way superscalar x86 processor), that L1 data cache misses are not the central problem, but a subtle proportion of independent retired instructions should be advised to get performance for in-core sorting.</abstract>
	<keywords>Hardware performance counters; In-core sorting algorithms with equal keys; Two-level memory hierarchy; Optimizing memory accesses; Parallelism at the chip level; Data structures for databases; Parallel sorting</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1042>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance prediction of paging workloads using lightweight tracing</paper_heading>
	<authors>Ariel N. Burton, Paul H.J. Kelly</authors>
	<abstract>A trace of a workload’s system calls can be obtained with minimal interference, and can be used as to drive repeatable experiments to evaluate system configuration alternatives. Replaying system call traces alone sometimes leads to inaccurate predictions because paging, and access to memory-mapped files, are not modelled. This paper extends tracing to handle such workloads. At trace capture time, the application’s page-level virtual memory access is monitored. The size of the page access trace, and capture overheads, are reduced by excluding recently-accessed pages. This leads to a slight loss of accuracy. Using a suite of memory-intensive applications, we evaluate the capture overhead and measure the predictive accuracy of the approach.</abstract>
	<keywords>Software; Performance evaluation; Workload characterization</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1043>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A universal performance factor for multi-criteria evaluation of multistage interconnection networks</paper_heading>
	<authors>A. Chadi Aljundi, Jean-Luc Dekeyser, M-Tahar Kechadi, Isaac D. Scherson</authors>
	<abstract>The choice of an interconnection network for a parallel computer depends on a large number of performance factors which are very often application dependent. We propose a performance evaluation and a comparison methodology. This methodology is applied to a recently introduced class of interconnection networks (Multistage Chordal Ring Based multistage interconnection network). These networks are compared to the well known Omega network of comparable architectural characteristics. The methodology is expected to serve in the evaluation of the use of multistage interconnection networks as an intercommunication medium in today’s multi-processor systems.</abstract>
	<keywords>Interconnection networks; Parallel architectures; Delta networks; Banyan networks; MCRB networks</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1044>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>On balancing network traffic in path-based multicast communication</paper_heading>
	<authors>Ahmed Al-Dubai, Mohamed Ould-Khaoua, Lewis Mackenzie</authors>
	<abstract>This paper presents a new multicast path-based algorithm, referred to here as the Qualified Groups (QG for short), which can achieve a high degree of parallelism and low communication latency over a wide range of traffic loads in the mesh. The QG algorithm relies on a new approach that divides the destinations in a way that balances the traffic load on network channels during the propagation of the multicast message. Results from extensive simulations under a variety of working conditions confirm that the QG algorithm exhibits superior performance characteristics over those of some well-known existing algorithms, such as dual-path, multiple-path, and column-path algorithm.</abstract>
	<keywords>Qualified Groups (QG for short); low communication latency; QG algorithm; network channel</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1045>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fault-tolerant routing in hypercubes using partial path set-up</paper_heading>
	<authors>Dong Xiang</authors>
	<abstract>A new technique is proposed for fault-tolerant routing in hypercubes, which needs to set up a partial path based on local safety information. Local safety information is utilized to guide fault-tolerant routing. Local safety is a centralized metric. A feasible path from the source to the destination may not be guaranteed at the source based on local safety information when the system contains a large number of faults, although a feasible path is available. A partial path is set up for fault-tolerant routing, where the header flit is forwarded until a maximal safe subcube is found to contain the current node and the destination. Backtracking is adopted only for the header along the minimum paths or non-minimum feasible paths, if necessary, in order to set up a partial feasible path. Extensive simulation results have shown that the partial path set-up scheme is quite useful for fault-tolerant routing, while the extra cost caused by path set-up is trivial.</abstract>
	<keywords>Fault-tolerant routing; Hypercubes; Local safety; Maximal safe subcubes; Pipelined-circuit-switching; Partial path set-up</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1046>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Approximation in non-product form finite capacity queue systems</paper_heading>
	<authors>Nigel Thomas</authors>
	<abstract>In this paper a class of finite length Markovian queueing models is studied that, in general, does not exhibit a product form solution. Good approximations can be derived for the marginal queue size distributions in this case, and hence measures such as the average response time can be calculated. However, because no product form exists, expressions for the joint queue size distribution are much more costly to derive, hence many performance measures of interest cannot be easily computed. An approximation for the joint queue size distributions is explored here, which improves on a naive product form assumption by considering various boundary cases. The approximations are explored numerically by example.</abstract>
	<keywords>Finite capacity queue; Variance approximation; Stochastic process algebra</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1047>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed computation of transient state distributions and passage time quantiles in large semi-Markov models</paper_heading>
	<authors>Jeremy T. Bradley, Nicholas J. Dingle, Peter G. Harrison, William J. Knottenbelt</authors>
	<abstract>Semi-Markov processes (SMPs) are expressive tools for modelling parallel and distributed systems; they are a generalisation of Markov processes that allow for arbitrarily distributed sojourn times. This paper presents an iterative technique for transient and passage time analysis of large structurally unrestricted semi-Markov processes. Our method is based on the calculation and subsequent numerical inversion of Laplace transforms and is amenable to a highly scalable distributed implementation. Results for a distributed voting system model with up to 1.1 million states are presented and validated against simulation.</abstract>
	<keywords>Performance measures; Stochastic processes; Stochastic analysis; Algorithms</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1048>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Memory-efficient Kronecker algorithms with applications to the modelling of parallel systems</paper_heading>
	<authors>Anne Benoit, Brigitte Plateau, William J. Stewart</authors>
	<abstract>We present a new algorithm for computing the solution of large Markov chain models whose generators can be represented in the form of a generalized tensor algebra, such as networks of stochastic automata. The tensor structure inherently involves a product state space but, inside this product state space, the actual reachable state space can be much smaller. For such cases, we propose an improvement of the standard numerical algorithm, the so-called “shuffle algorithm”, which necessitates only vectors of the size of the actual state space. With this contribution, numerical algorithms based on tensor products can now handle larger models.</abstract>
	<keywords>Large and sparse Markov chains; Stochastic automata networks; Generalized tensor algebra; Vector-descriptor multiplication; Shuffle algorithm</keywords>
	<publication_month_year>2006-08-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 7</volumes_issues>
</paper>
<paper no=1049>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Transfer, processing and distribution of cosmic ray data from Tibet</paper_heading>
	<authors>Kai Nan, Yongzheng Ma, Hongmei Zhang, Gang Chen</authors>
	<abstract>The YBJ-ARGO experiment uses cosmic ray data collected at the International Cosmic Ray Observatory in Tibet. Because of the observatory’s location, transferring those data at a speed of 0.5 TB per day is challenging. Furthermore, the stream of raw data needs to be processed in a timely fashion before the resulting, reconstructed data are made available to the physicists–users. These processes demand large capacity, computing and storage resources. In this paper, we introduce the processes of transfer, processing and distribution of cosmic ray data from Tibet. A working demonstration was presented at the iGrid 2005 workshop.</abstract>
	<keywords>Data grid; Data processing; YBJ-ARGO; iGrid2005</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1050>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High-definition multimedia for multiparty low-latency interactive communication</paper_heading>
	<authors>Petr Holub, Luděk Matyska, Miloš Liška, Lukáš Hejtmánek, ... Eva Hladká</authors>
	<abstract>We describe a high-quality collaborative environment that uses High-Definition (HD) video to achieve near realistic perception of a remote site. The capture part, consisting of a HD camera, Centaurus HD-SDI capture card, and UltraGrid software, produces a 1.5 Gbps UDP data stream of uncompressed HD video that is transferred over a 10GE network interface to the high-speed IP network. The HD video stream displaying uses either a software-based solution with color depth down-sampling and field de-interlacing, or another Centaurus card. Data distribution to individual participants of the videoconference is achieved using a user-controlled UDP packet reflector based on the Active Element idea. The viability of this system has been demonstrated at the iGrid 2005 conference for a three-way high quality videoconference among sites in the Czech Republic, Louisiana, and California.</abstract>
	<keywords>High-Definition Video; Video Conference; Uncompressed HD Video; High-Speed Multipoint Data Distribution</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1051>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using RDF to describe networks</paper_heading>
	<authors>Jeroen J. van der Ham, Freek Dijkstra, Franco Travostino, Hubertus M.A. Andree, Cees T.A.M. de Laat</authors>
	<abstract>Conventions such as iGrid 2005 and SuperComputing show that there is increasing demand for more service options on networks. For such networks, large teams of experts are needed to configure and manage them. In order to make the full potential of hybrid networks available to the ordinary user, the complexity must be reduced. This paper presents the idea of the Network Description Language (NDL), which builds on Semantic Web techniques to create a distributed Topology Knowledge Base (TKB). The TKB can provide a collection of reachability graphs, showing connectivity rules among physical and/or virtual entities. Latching onto the Semantic Web provides network management with a new breed of tools—bots, compilers, browsers, both commercial off-the-shelf (COTS) and open source. The approach appears to be applicable to the Global Lambda Integrated Facility (GLIF) as well as other experimental communities.</abstract>
	<keywords>Network descriptions; Semantic Web; Resource Description Framework; Hybrid networks</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1052>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>G-lambda: Coordination of a Grid scheduler and lambda path service over GMPLS</paper_heading>
	<authors>Atsuko Takefusa, Michiaki Hayashi, Naohide Nagatsu, Hidemoto Nakada, ... Satoshi Sekiguchi</authors>
	<abstract>At iGrid2005, we conducted a live demonstration where our Grid scheduling system co-allocated computing and network resources with advance reservation through Web services interfaces using the Grid Resource Scheduler (GRS), the Network Resource Management System (NRM), which is capable of GMPLS network resource management, and a GMPLS-based network test-bed, for the first time. The goal of the G-lambda project is to define a standard Web services interface (GNS-WSI) between GRS and NRM that is acceptable for both application service providers and commercial network operators, and which can be used as a tool for realizing new and emerging commercial services.</abstract>
	<keywords>Grid scheduling; Optical network; GMPLS; Web services; Advance reservation</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1053>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The first functional demonstration of optical virtual concatenation as a technique for achieving Terabit networking</paper_heading>
	<authors>Akira Hirano, Luc Renambot, Byungil Jeong, Jason Leigh, ... Osamu Ishida</authors>
	<abstract>The optical virtual concatenation (OVC) function of The Terabit LAN was demonstrated for the first time at the iGrid 2005 workshop in San Diego, California. The TERAbit-LAN establishes a lambda group path (LGP) for an application where the number of lambdas/L2 connections in a LGP can be specified by the application. Each LGP is logically treated as one end-to-end optical path, so during parallel transport, the LGP channels have no relative latency deviation. However, optical path diversity (e.g. restoration) can cause LGP relative latency deviations and negatively affect quality of service. OVC hardware developed by NTT compensates for relative latency deviations to achieve a virtual bulk transport for the Electronic Visualization Laboratory’s (EVL) Scalable Adaptive Graphics Environment application.</abstract>
	<keywords> Optical virtual concatenation (OVC); Terabit LAN; lambda group path (LGP); NTT</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1054>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Interactive 3D HD video transport for e-science collaboration over UCLP-enabled GLORIAD lightpath</paper_heading>
	<authors>Jinyong Jo, Wontaek Hong, SeungJoo Lee, Dongkyun Kim, ... OkHwan Byeon</authors>
	<abstract>Emerging large-scale applications for e-science and Grid require end-to-end lightpaths that need to be dynamically provisioned with QoS (quality of service) guarantees. In this paper, an e-science collaboration is demonstrated as a part of iGRID2005, where an interactive 3D HD (high definition) video transport is realized over an inter-continental lambda integrated facility on 10 Gbps GLORIAD links. To realize real-time interactive (2-way) collaboration among scientific researchers, the 10 Gbps end-to-end user-controlled lightpath between Korea and USA is dynamically provided with very high network performance. Both uncompressed and stereoscopic HD videos are then exchanged between participating sites. Thus, we verify the potential of proposed e-science collaborations by discussing underlying lambda networking infrastructure, UCLP-enabled dynamic lightpath provisioning, HD video system setup and integration, and demonstration results.</abstract>
	<keywords>User-controlled lightpath; Uncompressed and stereoscopic high definition videos; e-Science collaboration</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1055>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Opening a University Fiber Highway between Mexico and the US</paper_heading>
	<authors>Raúl G. Hazas-Izquierdo, Salvador Castañeda-Ávila, Luis M. Farfán-Molina, Julián Delgado-Jiménez, ... José L. Rodríguez-Navarro</authors>
	<abstract>Binational business and cultural exchange between Mexico and the United States can be complemented adequately by an enhanced scientific collaboration. A high-bandwidth optical fiber link between La Jolla, California, USA and Tijuana, Baja California, Mexico is currently shared between the Corporación Universitaria para el Desarrollo de Internet (CUDI), Mexico’s Internet 2 initiative, and the Corporation for Education Network Initiatives in California (CENIC). A meteorological computing-intensive parallel application requiring high-bandwidth connectivity would launch collaborations between Mexico’s leading science, technology and Internet 2 organizations and associated Mexican universities, and their Californian counterparts. A demonstration grid was set up between the San Diego Supercomputer Center and CICESE, a scientific research facility in Ensenada, Mexico, and weather systems that occur over the western United States and the Baja California peninsula were simulated. Significant savings in computing time have been achieved. A successful implementation of the MM5 meteorological model on this demonstration grid may present an incentive to bring dark fiber to CICESE. With an enhanced connectivity, we expect to provide the means to realize collaborations between groups working on oceanographic, seismic and ecological undertakings in both countries.</abstract>
	<keywords>Clustering; Grid computing; Collaborative computing</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1056>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Highly interactive distributed visualization</paper_heading>
	<authors>M. Scarpa, R.G. Belleman, P.M.A. Sloot, C.T.A.M. de Laat</authors>
	<abstract>We report on our iGrid2005 demonstration, called the “Dead Cat Demo”; an example of a highly interactive augmented reality application consisting of software services distributed over a wide-area, high-speed network. We describe our design decisions, analyse the implications of the design on application performance and show performance measurements.</abstract>
	<keywords>Highly interactive visualization; Distributed visualization; Augmented Reality</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1057>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Seamless live migration of virtual machines over the MAN/WAN</paper_heading>
	<authors>Franco Travostino, Paul Daspit, Leon Gommans, Chetan Jog, ... Phil Yonghui Wang</authors>
	<abstract>The “VM Turntable” demonstrator at iGRID 2005 pioneered the integration of Virtual Machines (VMs) with deterministic “lightpath” network services across a MAN/WAN. The results provide for a new stage of virtualization—one for which computation is no longer localized within a data center but rather can be migrated across geographical distances, with negligible downtime, transparently to running applications and external clients. A noteworthy data point indicates that a live VM was migrated between Amsterdam, NL and San Diego, USA with just 1–2 s of application downtime. When compared to intra-LAN local migrations, downtime is only about 5–10 times greater despite 1000 times higher round-trip times.</abstract>
	<keywords>VirtualizationVirtual machine; Lightpaths; Control planes; Optical networks</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1058>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using zero configuration technology for IP addressing in optical networks</paper_heading>
	<authors>Freek Dijkstra, Jeroen J. van der Ham, Cees T.A.M. de Laat</authors>
	<abstract>Host configuration in optical networks is usually done by hand. In this paper we propose to use zero configuration techniques, including self-assigned IP addresses and multicast DNS to do this automatically. The proposed technology is designed for small networks without central control, and can be applied to optical private networks as long as there is no router in between the end-hosts.</abstract>
	<keywords>Zero configuration; Ad hoc networks; Optical networks; Hybrid networks</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1059>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The network infrastructure at iGrid2005: Lambda networking in action</paper_heading>
	<authors>Paola Grosso, Pieter de Boer, Linda Winkler</authors>
	<abstract>iGrid2005’s goal was to demonstrate the potential of hybrid networks showing applications that require the use of these high-performance multi-gigabit networks. In this article we describe the network infrastructure in place during the workshop, we focus on the innovative and original aspects of the set-up, and we show that the infrastructure was a clear example of lambda networking.</abstract>
	<keywords>Lambda networking; Hybrid networks; Light paths; iGrid2005</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1060>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Video transcoding in a Grid network with User Controlled LightPaths</paper_heading>
	<authors>Eduard Grasa, Sergi Figuerola, Joaquim Recio, Albert López, ... Michel Savoie</authors>
	<abstract>GridON is an application that converts high-resolution broadcast video into MPEG-2 format, thereby reducing the file size and resolution. The application uses the user controlled lightpaths (UCLP) software to create on-demand, end-to-end, high-bandwidth dedicated connections to access remote computers. The converted MPEG-2 files can be distributed much faster and further than the source files to these dispersed computers, for reassembly into the higher resolution format. This paper describes the demonstration that took place last September at the iGrid 2005 conference held in San Diego. As a proof of concept, we successfully demonstrated that a video transfer in a Grid network environment can be integrated with a user-controlled lightpath provisioning system.</abstract>
	<keywords>Distributed applications; Network management; User-centered design</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1061>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>International real-time streaming of 4K digital cinema</paper_heading>
	<authors>Takashi Shimizu, Daisuke Shirai, Hirokazu Takahashi, Takahiro Murooka, ... Larry Smarr</authors>
	<abstract>This paper describes the world’s first real-time, international transmission of 4K digital cinema and 4K Super High Definition (SHD) digital video at iGrid 2005, hosted at the California Institute of Telecommunications and Information Technology (Calit2) at the University of California, San Diego. Nearly six hours of live and pre-recorded 4K motion picture and audio content was streamed to iGrid in San Diego from the Research Institute for Digital Media and Content (DMC) at Keio University in Tokyo. To implement this demonstration, several new technologies were introduced, including a prototype high-performance 4K compressed multicasting system called “JPEG 2000 Flexcast”, and “Soundscape”, a practical scheme for synchronizing audio and video transmitted from different locations over IP networks. These iGrid 2005 demonstrations proved that it is now feasible to implement networked professional audio/video applications–production, post-production and distribution–even at 4K quality over IP networks up to 15,000 km long. The demonstrations also showed the new 4K motion picture technology being introduced for digital cinema can be usefully applied to other network applications such as remote telepresence, distance learning and scientific visualization.</abstract>
	<keywords>4K SHD; JPEG 2000; Flexcast; Soundscape</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1062>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Data mining middleware for wide-area high-performance networks</paper_heading>
	<authors>Robert L. Grossman, Yunhong Gu, David Hanley, Michal Sabala, ... Woojin Seok</authors>
	<abstract>In this paper, we describe two distributed, data intensive applications that were demonstrated at iGrid 2005 (iGrid Demonstration US109 and iGrid Demonstration US121). One involves transporting astronomical data from the Sloan Digital Sky Survey (SDSS) and the other involves computing histograms from multiple high-volume data streams. Both rely on newly developed data transport and data mining middleware. Specifically, we describe a new version of the UDT network protocol called Composible-UDT, a file transfer utility based upon UDT called UDT-Gateway, and an application for building histograms on high-volume data flows called BESH (for Best Effort Streaming Histogram). For both demonstrations, we include a summary of the experimental studies performed at iGrid 2005.</abstract>
	<keywords>High-performance network protocols; High-performance networks; High performance data mining; Data mining middleware</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1063>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>AMROEBA: Computational astrophysics modeling enabled by dynamic lambda switching</paper_heading>
	<authors>Joe Mambretti, Rachel Gold, Fei Yeh, Jim Chen</authors>
	<abstract>Many data and compute intensive Grid applications, such as computational astrophysics, may be able to benefit from networking supported by dynamically provisioned lightpaths. To date, the majority of high performance distributed environments have been based on traditional routed packet networks, provisioned as external services rather than as integrated components within those environments. Because this approach often cannot provide high performance capabilities required by these applications, an alternative distributed infrastructure architecture is being designed based on dynamic lightpaths, supported by optical networks. These designs implement communication services and infrastructure as integral components of distributed infrastructure. The resultant environments resemble large scale specialized instruments. Presented here is one such architecture, implemented on a wide-area, optical Grid test bed, featuring a closely integrated dedicated lightpath mesh. The test bed was used to conduct a series of experiments to explore its potential for supporting adaptive mesh refinement (AMR) astrophysics simulations. While preliminary, the results of these experiments indicate that this architecture may provide the deterministic capabilities required by a wide range of high performance distributed services and applications, especially for computational science.</abstract>
	<keywords>Grid; Grid networking; Optical networking; Lambda Grid; Lightpath; Distributed infrastructure; Computational science; Wavelength switching</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1064>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Collaborative data visualization for Earth Sciences with the OptIPuter</paper_heading>
	<authors>Nut Taesombut, Xinran (Ryan) Wu, Andrew A. Chien, Atul Nayak, ... John Orcutt</authors>
	<abstract>Collaborative visualization of large-scale datasets across geographically distributed sites is becoming increasingly important for Earth Sciences. Not only does it enhance our understanding of the geological systems, but also enables near-real-time scientific data acquisition and exploration across distant locations. While such a collaborative environment is feasible with advanced optical networks and resource sharing in the form of Grid, many technical challenges remain: (1) on-demand discovery, selection and configuration of supporting end and network resources; (2) construction of applications on heterogeneous, distributed environments; and (3) use of novel exotic transport protocols to achieve high performance. To address these issues, we describe the multi-layered OptIPuter middleware technologies, including simple resource abstractions, dynamic network provisioning, and novel data transport services. In this paper, we present an evaluation of the first integrated prototype of the OptIPuter system software recently demonstrated at iGrid 2005, which successfully supports real-time collaborative visualizations of 3D multi-gigabyte earth science datasets.</abstract>
	<keywords>Grid middleware; Configurable optical networks; Resource abstractions; Novel transport protocol; Earth Sciences; Collaborative data visualization</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1065>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The global lambda visualization facility: An international ultra-high-definition wide-area visualization collaboratory</paper_heading>
	<authors>Jason Leigh, Luc Renambot, Andrew Johnson, Byungil Jeong, ... Manuel Garcia</authors>
	<abstract>The research outlined in this paper marks an initial global cooperative effort between visualization and collaboration researchers to build a persistent virtual visualization facility linked by ultra-high-speed optical networks. The goal is to enable the comprehensive and synergistic research and development of the necessary hardware, software and interaction techniques to realize the next generation of end-user tools for scientists to collaborate on the global Lambda Grid. This paper outlines some of the visualization research projects that were demonstrated at the iGrid 2005 workshop in San Diego, California.</abstract>
	<keywords>GraphicsVisualization; Collaboration; Remote networking; High-definition video</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1066>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Building a 100 Mpixel graphics device for the OptIPuter</paper_heading>
	<authors>Bram Stolk, Paul Wielinga</authors>
	<abstract>Wide area optical lambda networks create new possibilities for data-intensive scientific research and collaboration. One initiative to create novel infrastructure is the OptIPuter project which aims to build a distributed virtual computer using optical networks. At the iGrid2005 venue, an experiment was conducted on how a 100 Mpixel display device in San Diego could be driven by a remote 100 Mpixel graphics device in Amsterdam. The experiment showed the viability of such a setup, thereby validating one specific part of the OptIPuter concept: geographic separation of 100 Mpixel graphics- and display device. A key element in our setup is the use of unreliable communication.</abstract>
	<keywords>OptIPuterS; treaming video; UDP; Remote visualization; Ultra-high resolution visualization; Optical networks; Lambda switching</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1067>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Personal Varrier: Autostereoscopic virtual reality display for distributed scientific visualization</paper_heading>
	<authors>Tom Peterka, Daniel J. Sandin, Jinghua Ge, Javier Girado, ... Thomas A. DeFanti</authors>
	<abstract>As scientific data sets increase in size, dimensionality, and complexity, new high resolution, interactive, collaborative networked display systems are required to view them in real-time. Increasingly, the principles of virtual reality (VR) are being applied to modern scientific visualization. One of the tenets of VR is stereoscopic (stereo or 3d) display; however the need to wear stereo glasses or other gear to experience the virtual world is encumbering and hinders other positive aspects of VR such as collaboration. Autostereoscopic (autostereo) displays presented imagery in 3d without the need to wear glasses or other gear, but few qualify as VR displays. The Electronic Visualization Laboratory (EVL) at the University of Illinois at Chicago (UIC) has designed and built a single-screen version of its 35-panel tiled Varrier display, called Personal Varrier. Based on a static parallax barrier and the Varrier computational method, Personal Varrier provides a quality 3d autostereo experience in an economical, compact form factor. The system debuted at iGrid 2005 in San Diego, CA, accompanied by a suite of distributed and local scientific visualization and 3d teleconferencing applications. The CAVEwave National LambdaRail (NLR) network was vital to the success of the stereo teleconferencing.</abstract>
	<keywords>Varrier; Personal Varrier; Virtual reality; Autostereoscopy; Autostereo; 3D display; Camera-based tracking; Parallax barrier</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1068>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Dynamic provisioning of LightPath services for radio astronomy applications</paper_heading>
	<authors>Jerry Sobieski, Tom Lehman, Bijan Jabbari, Chester Ruszczyk, ... Alan Whitney</authors>
	<abstract>A demonstration at iGRID 2005 used dynamic, deterministic, and dedicated LightPath network services to link radio telescopes from around the world with computational facilities at the MIT Haystack Observatory to create a single coherent instrument for real-time astronomical and geodetic research. The “electronic Very Long Baseline Interferometry” (e-VLBI) application provides ultra-high resolution images of very faint and very distant objects in the universe. The application-specific network topology carried 2 Gbps of VLBI data from radio telescopes in Europe, North America, and Japan to Haystack for real-time correlation processing. This paper describes the application, the network technologies employed for the demonstration, the results, challenges and future work.</abstract>
	<keywords>Electronic very long baseline interferometry; e-VLBI; VLBI; Generalized multi-protocol label switching; GMPLSRS; VPOSPF-TE; Light paths; Dedicated network resources; Deterministic network performancei; GRID; HOPI; DRAGON; UKLight; NetherLight; NorthernLight; JGN2</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1069>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Multi-domain Lambda Grid data portal for collaborative Grid applications</paper_heading>
	<authors>Oliver Yu, Anfei Li, Yuan Cao, Leping Yin, ... Huan Xu</authors>
	<abstract>This paper presents the iGrid2005 demonstration of the multi-domain Lambda Grid data portal to enable collaborative Grid applications to retrieve gigabyte-size scientific datasets on demand over a wide-area Lambda Grid test-bed spanning across the United States and Europe. The data portal allows collaborative users to register and discover distributed datasets across a multi-domain Lambda Grid through a web-service based dataset registry and search engine, and to deliver the discovered datasets through dynamic established lightpaths to specified computing cluster locations for dataset manipulation. The data portal employs the novel Secure Photonic Inter-domain Negotiator (SPIN) as a secure multi-domain control plane to set up dynamic lightpaths between the source and destination clusters for respective storage and manipulation of the discovered datasets over a multi-domain Lambda Grid. The data portal employs lambda-aware middleware and transport protocol to optimize transfer of the discovered datasets over the dynamic established lightpaths. Implementation details and observed performance results from the demonstrations of the SPIN-based Lambda Grid data portal are reported.</abstract>
	<keywords>Lambda Grid data portal; Collaborative Grid application; Inter-domain control plane; Optical wide-area network</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1070>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed and collaborative visualization of large data sets using high-speed networks</paper_heading>
	<authors>Andrei Hutanu, Gabrielle Allen, Stephen D. Beck, Petr Holub, ... Shalini Venkataraman</authors>
	<abstract>We describe an architecture for distributed collaborative visualization that integrates video conferencing, distributed data management and grid technologies as well as tangible interaction devices for visualization. High-speed, low-latency optical networks support high-quality collaborative interaction and remote visualization of large data.</abstract>
	<keywords>Collaborative visualization; Distributed applications; Co-scheduling; Interaction devices; Video conferencing</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1071>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Simulating and visualizing the human arterial system on the TeraGrid</paper_heading>
	<authors>Suchuan Dong, Joseph Insley, Nicholas T. Karonis, Michael E. Papka, ... George Karniadakis</authors>
	<abstract>We present a Grid solution to a grand challenge problem, the simulation and visualization of the human arterial system. We implemented our simulation and visualization system on the National Science Foundation’s TeraGrid and demonstrated it at the iGrid 2005 conference in San Diego, California. We discuss our experience in running on a computational Grid and present observations and suggestions for improving similar experiences.</abstract>
	<keywords>Grid</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1072>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Large-scale multimedia content delivery over optical networks for interactive TV services</paper_heading>
	<authors>Miroslaw Czyrnek, Ewa Kusmierek, Cezary Mazurek, Maciej Stroinski</authors>
	<abstract>iTVP is a system built for delivery of live TV programming, video-on-demand and audio-on-demand with interactive access over IP networks. It has a nationwide range and is designed to provide service to a high number of concurrent users. Hence, scalability is one of the most important issues. In this paper we described how scalability is achieved in a two-level hierarchical architecture of the delivery system. We present the principles of content caching mechanisms and the management of the resulting distributed content repository. The system is implemented and currently operates in a test installation. Although the range of the test system operations is limited, as far as the number of users and the size of the content repository is concerned, the experience gained with iTVP so far provides an insight into the expected prototype performance. We present a system performance evaluation and describe the experience gained during the iGrid demonstration.</abstract>
	<keywords>Content distribution network; Distributed caching; Multimedia delivery</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1073>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Token based networking: Experiment NL-101</paper_heading>
	<authors>Leon Gommans, Bas van Oudenaarde, Alfred Wan, Cees de Laat, ... Inder Monga</authors>
	<abstract>This short communication outlines an experiment where tokens, associated with application oriented IP streams, authorize access to an optical lightpath during iGrid 2005. The experiment showed the practical viability of this mechanism to perform access control and resource management within optical networks. The experiment was conducted in collaboration with the Virtual Machine Turntable (NL-103) experiment, which used the mechanism to obtain access to continental and transatlantic optical network segments that connected Virtual Machine sites.</abstract>
	<keywords> IP stream; lightpath;  iGrid 2005</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1074>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Real-time multi-scale brain data acquisition, assembly, and analysis using an end-to-end OptIPuter</paper_heading>
	<authors>Rajvikram Singh, Nicholas Schwarz, Nut Taesombut, David Lee, ... Mark H. Ellisman</authors>
	<abstract>At iGrid 2005 we demonstrated the transparent operation of a biology experiment on a test-bed of globally distributed visualization, storage, computational, and network resources. These resources were bundled into a unified platform by utilizing dynamic lambda allocation, high bandwidth protocols for optical networks, a Distributed Virtual Computer (DVC) [N. Taesombut, A. Chien, Distributed Virtual Computer (DVC): Simplifying the development of high performance grid applications, in: Proceedings of the Workshop on Grids and Advanced Networks, GAN 04, Chicago, IL, April 2004 (held in conjunction with the IEEE Cluster Computing and the Grid (CCGrid2004) Conference)], and applications running over the Scalable Adaptive Graphics Environment (SAGE) [L. Renambot, A. Rao, R. Singh, B. Jeong, N. Krishnaprasad, V. Vishwanath, V. Chandrasekhar, N. Schwarz, A. Spale, C. Zhang, G. Goldman, J. Leigh, A. Johnson, SAGE: The Scalable Adaptive Graphics Environment, in: Proceedings of WACE 2004, 23–24 September 2004, Nice, France, 2004]. Using these layered technologies we ran a multi-scale correlated microscopy experiment [M.E. Maryann, T.J. Deerinck, N. Yamada, E. Bushong, H. Ellisman Mark, Correlated 3D light and electron microscopy: Use of high voltage electron microscopy and electron tomography for imaging large biological structures, Journal of Histotechnology 23 (3) (2000) 261–270], where biologists imaged samples with scales ranging from 20X to 5000X in progressively increasing magnification. This allows the scientists to zoom in from entire complex systems such as a rat cerebellum to individual spiny dendrites. The images used spanned multiple modalities of imaging and specimen preparation, thus providing context at every level and allowing the scientists to better understand the biological structures. This demonstration attempts to define an infrastructure based on OptIPuter components which would aid the development and design of collaborative scientific experiments, applications and test-beds and allow the biologists to effectively use the high resolution real estate of tiled displays.</abstract>
	<keywords>Graphics clusters; Multi-scale correlated microscopy; Montage images; HDTV streaming; Telescience; Tile displays; Tiled displays; Optical networks; Scientific visualization; Remote instrumentation</keywords>
	<publication_month_year>2006-10-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 22, Issue 8</volumes_issues>
</paper>
<paper no=1075>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Elastic reservations for efficient bandwidth utilization in LambdaGrids</paper_heading>
	<authors>Sumit Naiksatam, Silvia Figueira</authors>
	<abstract>We introduce the concept of elastic reservation of bandwidth capacity to mitigate the problem of bandwidth fragmentation in LambdaGrids and present a network model which can support elastic reservations. We also define the Elastic Scheduling Problem (ESP), which succinctly captures the optimal utilization objective of elastic reservations. Analysis of ESP reveals that it is an NP-complete problem. Hence we present a heuristic algorithm, Squeeze In Stretch Out (SISO), for tackling ESP in polynomial time. SISO achieves good bandwidth utilization in simulation and efficiently handles the dynamic sharing of bandwidth between advance and immediate reservation requests. We also explore the impact of cost incentives for adopting elastic reservations on both the service provider and the user. In general, the approach for elastic reservation and scheduling presented in this paper is applicable to any concurrently accessible resource where the usage characteristics are quasi-flexible.</abstract>
	<keywords>Elastic traffic; Advance reservations; Immediate reservations; Grids; Bandwidth scheduling; Traffic engineering</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1076>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Programming the Grid with POP-C ++</paper_heading>
	<authors>Tuan-Anh Nguyen, Pierre Kuonen</authors>
	<abstract>Despite the fact that Grid computing is the main theme of distributed computing research during the last few years, programming on the Grid is still a huge difficulty to normal users. The POP-C ++ programming system has been built to provide Grid programming facilities which greatly ease the development and the deployment of parallel applications on the Grid. The original parallel object model used in POP-C ++ is a combination of powerful features of object-oriented programming and of high-level distributed programming capabilities. The model is based on the simple idea that objects are suitable structures to encapsulate and to distribute heterogeneous data and computing elements over the Grid. Programmers can guide the resource allocation for each object through the high-level resource descriptions. The object creation process, supported by the POP-C ++ runtime system, is transparent to programmers. Both inter-object and intra-object parallelism are supported through various method invocation semantics. The POP-C ++ programming language extends C ++ to support the parallel object model with just a few new keywords. In this paper, we present the Grid programming aspects of POP-C ++. With POP-C ++, writing a Grid-enabled application becomes as simple as writing a sequential C ++ application.</abstract>
	<keywords>Parallel object; Distributed object; Parallelism; Programming model; Grid programming</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1077>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed data mining services leveraging WSRF</paper_heading>
	<authors>Antonio Congiusta, Domenico Talia, Paolo Trunfio</authors>
	<abstract>The continuous increase of data volumes available from many sources raises new challenges for their effective understanding. Knowledge discovery in large data repositories involves processes and activities that are computationally intensive, collaborative, and distributed in nature. The Grid is a profitable infrastructure that can be effectively exploited for handling distributed data mining and knowledge discovery. To achieve this goal, advanced software tools and services are needed to support the development of KDD applications. The Knowledge Grid is a high-level framework providing Grid-based knowledge discovery tools and services. Such services allow users to create and manage complex knowledge discovery applications that integrate data sources and data mining tools provided as distributed services on the Grid. All of these services are currently being re-designed and re-implemented as WSRF-compliant Grid Services. This paper highlights design aspects and implementation choices involved in such a process.</abstract>
	<keywords>Distributed data mining; Grid computing; OGSA; WSRF</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1078>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and implementation of a data mining grid-aware architecture</paper_heading>
	<authors>María S. Pérez, Alberto Sánchez, Víctor Robles, Pilar Herrero, José M. Peña</authors>
	<abstract>Current business processes often use data from several sources. Data is characterized to be heterogeneous, incomplete and usually involves a huge amount of records. This implies that data must be transformed in a set of patterns, rules or some kind of formalism, which helps to understand the underlying information. The participation of several organizations in this process makes the assimilation of data more difficult. Data mining is a widely used approach for the transformation of data to useful patterns, aiding the comprehensive knowledge of the concrete domain information. Nevertheless, traditional data mining techniques find difficulties in their application on current scenarios, due to the complexity previously mentioned. Data Mining Grid tries to fix these problems, allowing data mining process to be deployed in a grid environment, in which data and services resources are geographically distributed, belong to several virtual organizations and the security can be flexibly solved. We propose both a novel architecture for Data Mining Grid, named DMGA, and the implementation of this architecture, named WekaG.</abstract>
	<keywords>Data mining; Data mining grid architecture (DMGA; )WekaG</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1079>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A grid-based approach for enterprise-scale data mining</paper_heading>
	<authors>Ramesh Natarajan, Radu Sion, Thomas Phan</authors>
	<abstract>We describe a grid-based approach for enterprise-scale data mining, which is based on leveraging parallel database technology for data storage, and on-demand compute servers for parallelism in the statistical computations. This approach is targeted towards the use of data mining in highly-automated vertical business applications, where the data is stored on one or more relational database systems, and an independent set of high-performance compute servers or a network of low-cost, commodity processors is used to improve the application performance and overall workload management. The goal of this paper is to describe an algorithmic decomposition of data mining kernels between the data storage and compute grids, which makes it possible to exploit the parallelism on the respective grids in a simple way, while minimizing the data transfer between these grids. This approach is compatible with existing standards for data mining task specification and results reporting, so that larger applications using these data mining algorithms do not have to be modified to benefit from this grid-based approach.</abstract>
	<keywords>Data mining; Grid computing; Predictive modeling; Parallel databases</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1080>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Using ontologies for preprocessing and mining spectra data on the Grid</paper_heading>
	<authors>M. Cannataro, P.H. Guzzi, T. Mazza, G. Tradigo, P. Veltri</authors>
	<abstract>The analysis of mass spectrometry proteomics data requires the composition of different software tools devoted to the loading, management, preprocessing, mining, and visualization of spectra data. This paper proposes the use of ontologies to guide the composition of preprocessing and data mining tools and describes the approach through MS-Analyzer, a software tool for the integrated management, preprocessing and mining of spectra data on the Grid.</abstract>
	<keywords>preprocessing, mining; data require; spectra data; MS-Analyzer; Grid</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1081>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed data mining on Agent Grid: Issues, platform and development toolkit</paper_heading>
	<authors>Jiewen Luo, Maoguang Wang, Jun Hu, Zhongzhi Shi</authors>
	<abstract>Centralized data mining techniques are widely used today for the analysis of large corporate and scientific data stored in databases. However, industry, science, and commerce fields often need to analyze very large datasets maintained over geographically distributed sites by using the computational power of distributed systems. The Grid can play a significant role in providing an effective computational infrastructure support for this kind of data mining. Similarly, the advent of multi-agent systems has brought us a new paradigm for the development of complex distributed applications. During the past decades, there have been several models and systems proposed to apply agent technology building distributed data mining (DDM). Through a combination of these two techniques, we investigated the critical issues to build DDM on Grid infrastructure and design an Agent Grid Intelligent Platform as a testbed. We also implement an integrated toolkit VAStudio for quickly developing agent-based DDM applications and compare its function with other systems.</abstract>
	<keywords>Data mining; databases; geographically distributed; Grid; distributed data mining (DDM)</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1082>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed feature extraction in a p2p setting — a case study</paper_heading>
	<authors>Michael Wurst, Katharina Morik</authors>
	<abstract>Finding the right data representation is essential for virtually every data mining application. In this work we describe an approach to collaborative feature extraction, selection and aggregation in distributed, loosely coupled domains. In contrast to other work in the field of distributed data mining, we focus on scenarios in which a large number of loosely coupled nodes apply data mining to different, usually very small and overlapping, subsets of the entire data space. The aim is not to find a global concept to cover all data, but to learn a set of local concepts. Our prototypical application is a distributed media organization platform, called Nemoz, that assists users in maintaining their media collections. We propose two models for collaborative feature extraction, selection and aggregation for supervised data mining. One is based on a centralized p2p architecture, and the other on a fully distributed p2p architecture. We compare both models on a real world data set and discuss their advantages and problems.</abstract>
	<keywords>Data mining; Distributed architectures; Collaborative computing</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1083>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Mining of the chemical information in GRID environment</paper_heading>
	<authors>Uko Maran, Sulev Sild, Iiris Kahn, Kalev Takkis</authors>
	<abstract>Data mining and knowledge exploration of chemical information is the key step in life science fields, such as drug discovery, property/activity prediction and many others, where the meaningful linking of experimental knowledge and information about chemical structure is necessary. In these fields the applications are often based on quantitative structure activity/property relationship (QSAR/QSPR) models, where relevant information for models is extracted from the large data sets of molecular descriptors. This requires multiple software packages to be used and linked via usually complicated workflow. It also requires extensive computational resources to be accessed via Grid middleware when applied to the complex datasets and/or when the time factor in decision support is critical. The OpenMolGRID system provides a grid-enabled infrastructure for molecular design and engineering, including tools for QSAR/QSPR modelling and building automated scientific workflows on top of the UNICORE Grid middleware. In the present article, the OpenMolGRID system is used for the modelling of the inhibition of aspartyl protease enzyme. Efficient inhibition of this enzyme can combat HIV-1.</abstract>
	<keywords>Data mining; Distributed systems; Modelling and prediction; Chemistry</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1084>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed data mining in grid computing environments</paper_heading>
	<authors>Ping Luo, Kevin Lü, Zhongzhi Shi, Qing He</authors>
	<abstract>The computing-intensive data mining for inherently Internet-wide distributed data, referred to as Distributed Data Mining (DDM), calls for the support of a powerful Grid with an effective scheduling framework. DDM often shares the computing paradigm of local processing and global synthesizing. It involves every phase of Data Mining (DM) processes, which makes the workflow of DDM very complex and can be modelled only by a Directed Acyclic Graph (DAG) with multiple data entries. Motivated by the need for a practical solution of the Grid scheduling problem for the DDM workflow, this paper proposes a novel two-phase scheduling framework, including External Scheduling and Internal Scheduling, on a two-level Grid architecture (InterGrid, IntraGrid). Currently a DM IntraGrid, named DMGCE (Data Mining Grid Computing Environment), has been developed with a dynamic scheduling framework for competitive DAGs in a heterogeneous computing environment. This system is implemented in an established Multi-Agent System (MAS) environment, in which the reuse of existing DM algorithms is achieved by encapsulating them into agents. Practical classification problems from oil well logging analysis are used to measure the system performance. The detailed experiment procedure and result analysis are also discussed in this paper.</abstract>
	<keywords>Distributed data mining; Directed acyclic graph; InterGrid; IntraGrid; Multi-agent system environment</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1085>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Mining performance data for metascheduling decision support in the Grid</paper_heading>
	<authors>Hui Li, David Groep, Lex Wolters</authors>
	<abstract>Metaschedulers in the Grid need dynamic information to support their scheduling decisions. Job response time on computing resources, for instance, is such a performance metric. In this paper, we propose an Instance Based Learning technique to predict response times by mining historical performance data. The novelty of our approach is to introduce policy attributes in representing and comparing resource states, which are defined as the pools of running and queued jobs on the resources at the time of making predictions. The policy attributes reflect the local scheduling policies and they can be automatically discovered using genetic search. An extensive empirical evaluation is conducted to validate our technique using real workload traces, which are collected from the NIKHEF production cluster on the LHC Computing Grid and Blue Horizon in the San Diego Supercomputer Center (SDSC). The experimental results show that acceptable prediction accuracy can be achieved, where the normalized average prediction errors for response times are ranging from 0.57 to 0.79.</abstract>
	<keywords>Response time predictions; Instance based learning; Metascheduling; Grid</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1086>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Promoting performance and separation of concerns for data mining applications on the grid</paper_heading>
	<authors>Vasco Furtado, Francisco Flávio de Souza, Walfredo Cirne</authors>
	<abstract>Grid Computing brought the promise of making high-performance computing cheaper and more easily available than traditional supercomputing platforms. Such a promise was very well received by the data mining (DM) community, as DM applications typically process very large datasets and are thus very resource intensive. However, since the Grid is very dynamic and parallel data mining is prone to load unbalancing, obtaining good data mining performance on the Grid is hard. It typically requires the scheduler to understand the inner workings of the application, bringing two related problems. First, good Grid schedulers tend to be very specialized in the application they target. Second, changing the application may require changing the scheduler, which may be especially challenging when there is no clear separation between the application and the scheduler code. We here propose and evaluate a knowledge-based approach that provides abstractions to the DM developer and optimizes at runtime the DM application on the Grid.</abstract>
	<keywords>Data mining application; Grid; Problem solving method; Knowledge-based system; Ontology</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1087>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Decentralized data management framework for Data Grids</paper_heading>
	<authors>Houda Lamehamedi, Boleslaw K. Szymanski</authors>
	<abstract>A new class of data intensive applications has led to increased demand for cost-efficient resource sharing approaches. Yet, providing efficient access to widely distributed data for large numbers of users poses considerable challenges. Most existing Grid systems are centrally managed, thus hindering their scalable expansion. We introduce a new distributed, adaptive, and scalable middleware that provides transparent access to data in Data Grids. Our approach relies on dynamic techniques that adapt replica creation to continuously changing network connectivity and users’ behavior. Results from simulations and deployment of our middleware show that our solution provides better data access performance than static approaches.</abstract>
	<keywords>Data Grids; Replication; Middleware; Simulation</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1088>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A global and parallel file system for grids</paper_heading>
	<authors>Félix García-Carballeira, Jesús Carretero, Alejandro Calderón, J. Daniel García, Luis. M. Sanchez</authors>
	<abstract>Data management is one of the most important problems in grid environments. Most of the efforts in data management in grids have been focused on data replication. Data replication is a practical and effective method to achieve efficient data access in grids. However all data replication schemes lack in providing a grid file system. One important challenge facing grid computing is the design of a grid file system. The Global Grid Forum defines a Grid File System as a human-readable resource namespace for management of heterogeneous distributed data resources, that can span across multiple autonomous administrative domains. This paper describes a new Grid File System according to the Global Grid Forum recommendations that integrates heterogeneous data storage resources in grids using standard grid technologies: GridFTP and the Resource Namespace Services, both defined by the Global Grid Forum. To obtain high performance, we apply the parallel I /O techniques used in traditional parallel file systems.</abstract>
	<keywords>Data grids; Parallel I /O; Data declustering; High performance I /O; GridFTP; RNS</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1089>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards a complete grid filesystem functionality</paper_heading>
	<authors>Soha Maad, Brian Coghlan, Geoff Quigley, John Ryan, ... David O’Callaghan</authors>
	<abstract>To be successful the grid must serve a variety of application domains. The data management solutions offered by existing grid middleware have significant shortcomings with respect to the needs of some of these application domains. This paper attributes these shortages to a lack of an underlying grid storage infrastructure, similar to that within an operating system. In an effort to develop this missing infrastructure we have created a relatively complete grid filesystem comprising four abstract engines for directory management, discovery, data movement and consistency. This will facilitate simple data management both within and between grids.</abstract>
	<keywords>Grid; Filesystem; Data management; Interoperability</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1090>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Design and analysis of a load balancing strategy in Data Grids</paper_heading>
	<authors>Xiao Qin</authors>
	<abstract>Developing Data Grids has increasingly become a major concern to make Grids attractive for a wide range of data-intensive applications. Storage subsystems are most likely to be a performance bottleneck in Data Grids and, therefore, the focus of this paper is to design and evaluate a data-aware load-balancing strategy to improve the global usage of storage resources in Data Grids. We build a model to estimate the response time of job running at a local site or remote site. In light of this model, we can calculate slowdowns imposed on jobs in a Data Grid environment. Next, we propose a load-balancing strategy that aims to balance load of a Data Grid in such a judicious way that computation and storage resources in each site are simultaneously well utilized. We conduct experiments using a simulated Data Grid to analyze the performance of the proposed strategy. Experimental results confirm that our load-balancing strategy can achieve high performance for data-intensive jobs in Data Grid environments.</abstract>
	<keywords>Load balancing; Data-intensive; Datagrids; Performance evaluation</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1091>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>MAPFS-DAI, an extension of OGSA-DAI based on a parallel file system</paper_heading>
	<authors>Alberto Sánchez, María S. Pérez, Konstantinos Karasavvas, Pilar Herrero, Antonio Pérez</authors>
	<abstract>Since current applications demand access to a huge volume of data, new and sophisticated I/O systems are required. Several mass storage systems have been developed by different institutions access their own data repositories. These systems expose native interfaces with no interoperability among them. In order to deal with this requirement, OGSA-DAI has emerged to provide uniform access to data sources. However, this initiative is not focused on the performance of the I/O operations. This paper describes MAPFS-DAI, an approach which tries to combine both ideas: performance and interoperability.</abstract>
	<keywords>Data access on grids; OGSA-DAI; MAPFS-Grid; Interoperability; Performance</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1092>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A catallactic market for data mining services</paper_heading>
	<authors>L. Joita, Omer F. Rana, Felix Freitag, Isaac Chao, ... Oscar Ardaiz</authors>
	<abstract>We describe a Grid market for exchanging data mining services based on the catallactic market mechanism proposed by von Hayek. This market mechanism allows selection between multiple instances of services based on operations required in a data mining task (such as data migration, data pre-processing and subsequently data analysis). Catallaxy is a decentralized approach, based on a “free market” mechanism, and is particularly useful when the number of market participants is large or when conditions within the market often change. It is therefore particularly suitable in Grid and peer-2-peer systems. The approach assumes that the service provider and user are not co-located, and require multiple message exchanges to carry out a data mining task. A market of J48-based decision tree algorithm instances, each implemented as a Web service, is used to demonstrate our approach. We have validated the feasibility of building catallactic data mining grid applications, and implemented a proof-of-concept application (Cat-COVITE) mapped to a catallactic Grid middleware.</abstract>
	<keywords>Data mining; Grid market; Application</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1093>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Concurrency control issues in Grid databases</paper_heading>
	<authors>David Taniar, Sushant Goel</authors>
	<abstract>Grid architecture is a fast evolving distributed computing architecture. The working of databases in the Grid architecture is not well understood. In view of changing distributed architecture we strongly feel that concurrency control issues should be revisited and reassessed for this new and evolving architecture. Implementing global lock table and global log records may not be practically possible in the Grid architecture due to the scalability issues. In this paper, we propose a correctness criterion and the Grid concurrency control protocol, which has the capability to deal with heterogeneity, autonomy, distribution and high volume of data in Grids. We then prove the correctness of the protocol followed by performance evaluation of the protocol.</abstract>
	<keywords>Grid database; Data Grid; Concurrency control protocol; Distributed data management; Transaction management</keywords>
	<publication_month_year>2007-01-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 1</volumes_issues>
</paper>
<paper no=1094>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient task replication and management for adaptive fault tolerance in Mobile Grid environments</paper_heading>
	<authors>Antonios Litke, Dimitrios Skoutas, Konstantinos Tserpes, Theodora Varvarigou</authors>
	<abstract>Fault tolerant Grid computing is of vital importance as the Grid and Mobile computing worlds converge to the Mobile Grid computing paradigm. We present an efficient scheme based on task replication, which utilizes the Weibull reliability function for the Grid resources so as to estimate the number of replicas that are going to be scheduled in order to guarantee a specific fault tolerance level for the Grid environment. The additional workload that is produced by the replication is handled by a resource management scheme which is based on the knapsack formulation and which aims to maximize the utilization and profit of the Grid infrastructure. The proposed model has been evaluated through simulation and has shown its efficiency for being used in a middleware approach in future mobile Grid environments.</abstract>
	<keywords>Fault tolerance; Scheduling; Grid computing</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1095>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Communication-efficient parallel generic pairwise elimination</paper_heading>
	<authors>Alexander Tiskin</authors>
	<abstract>The model of bulk-synchronous parallel (BSP) computation is an emerging paradigm of general-purpose parallel computing. In this paper, we consider the parallel complexity of generic pairwise elimination, special cases of which include Gaussian elimination with pairwise pivoting, Gaussian elimination over a finite field, generic Neville elimination and Givens reduction. We develop a new block-recursive, communication-efficient BSP algorithm for generic pairwise elimination.</abstract>
	<keywords>Parallel algorithms; Algebraic algorithms; Linear systems</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1096>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Enabling Grid technologies for Planck space mission</paper_heading>
	<authors>Giuliano Taffoni, Davide Maino, Claudio Vuerli, Giuliano Castelli, ... Fabio Pasian</authors>
	<abstract>Planck, the ESA satellite aimed at mapping the microwave sky through two complete sky surveys, will fly in 2007. It is an extremely demanding space mission in terms of computing power and data storage. Planck simulations mimic the whole mission starting from a virtual sky (ideal or contaminated by introducing several noise sources). Their main goal is the validation of the acquisition and reduction procedure that will be used to build the final scientific data during the operative phase of the mission. The Grid technology seems to be a promising answer to data storage and processing needs of the satellite. In the framework of the EGEE grid infrastructure, we managed to run a number of experiments aimed at designing and defining an application specific environment for the simulation software and data sharing. Our successful experiments demonstrate that the “gridification” of Planck pipelines is not only possible but even extremely convenient in terms of data processing speed and data sharing.</abstract>
	<keywords>Planck satellite mission; CMB; Grid computing; Globus; EGEE; HPC</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1097>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A structured hierarchical P2P model based on a rigorous binary tree code algorithm</paper_heading>
	<authors>Hongjun Liu, Ping Luo, Zhifeng Zeng</authors>
	<abstract>One of the main problems in peer-to-peer systems is how to balance the lookup latency, the number of transmitted messages and the network expansibility. In this paper, we present a P2P model based on a rigorous binary tree code algorithm. Our model is robust and efficient with constant lookup latency and constant message transmission. More importantly, the model has good expansibility and is suitable for super-scale networks.</abstract>
	<keywords>P2P; Super-peer; Rigorous binary tree code algorithm; DHT</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1098>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Optimal task partition and distribution in grid service system with common cause failures</paper_heading>
	<authors>Yuan-Shun Dai, Gregory Levitin, Xiaolong Wang</authors>
	<abstract>The grid service system is a type of large-scale distributed system, which is being widely used in many fields now. The partition of a grid service task into subtasks and the distribution of them on available resources have great influence on the extent of the service reliability and profits. This paper presents a novel optimization model for maximizing the expected grid service profit and develops a genetic algorithm to solve this optimization problem. As the basis of the objective function, the grid service reliability needs to be modeled and quantified first. However, due to the largeness and complexity of the grid service system, the existing models for small-scale distributed system reliability cannot be directly implemented for the grid. Therefore, this paper presents a virtual model with two-root tree structure that is more general than the star topology. It is because the tree structure not only can represent the virtual architecture of a grid service but also can take into account the common cause failures on the channels that are shared by multiple resources. Finally, a case of a BioGrid application is illustrated for example.</abstract>
	<keywords>DMS; Data Management Server; MRST; Minimal Resource Spanning Tree; RMS; Resource Management System; RST; Resource spanning tree; VL; Virtual link; VN; Virtual node; VO; Virtual organization.</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1099>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Advanced Resource Connector middleware for lightweight computational Grids</paper_heading>
	<authors>M. Ellert, M. Grønager, A. Konstantinov, B. Kónya, ... A. Wäänänen</authors>
	<abstract>As computational Grids move away from the prototyping state, reliability, performance and ease of use and maintenance become focus areas of their adoption. In this paper, we describe ARC (Advanced Resource Connector) Grid middleware, where these issues have been given special consideration. We present an in-depth view of the existing components of ARC, and discuss some of the new components, functionalities and enhancements currently under development. This paper also describes architectural and technical choices that have been made to ensure scalability, stability and high performance. The core components of ARC have already been thoroughly tested in demanding production environments, where it has been in use since 2002. The main goal of this paper is to provide a first comprehensive description of ARC.</abstract>
	<keywords>Grid; Globus; Middleware; Distributed computing; Cluster; Linux; Scheduling</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1100>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>An optimized two-tier P2P architecture for contextualized keyword searches</paper_heading>
	<authors>James Salter, Nick Antonopoulos</authors>
	<abstract>We describe a two-tiered P2P network for efficient lookup of contextualized data, utilizing an alternative strategy for building the network overlay designed to reduce the hops required to route lookups and improve fault tolerance by allowing the selection of nodes to build subrings. We show how the method is used to support context–keyword queries. Our calculations indicate that the technique yields improvements in the average query hop count while reducing the amount of state stored on each node. The use of Preference Lists can further reduce the average hop count through bypassing previously traversed segments of the structure.</abstract>
	<keywords>Peer-to-peer; Distributed hash tables; Context; Chord</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1101>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A modular meta-scheduling architecture for interfacing with pre-WS and WS Grid resource management services</paper_heading>
	<authors>Eduardo Huedo, Rubén S. Montero, Ignacio M. Llorente</authors>
	<abstract>The last version of the Globus Toolkit includes both pre-WS and WS GRAM services to submit, monitor, and control jobs on remote Grid resources. In the medium term and until a full transition is accomplished, both pre-WS and WS GRAM services will coexist in Grid infrastructures. In this paper, we describe the modular architecture of the GridWay meta-scheduler, which allows the simultaneous and coordinated use of pre-WS and WS GRAM services and, therefore, makes easy the transition to a Web Service implementation of the Globus components. Such functionality is demonstrated on a infrastructure that comprises resources from a research testbed, based on the Globus Toolkit 4.0, and the EGEE production infrastructure, based on the LCG middleware. The Web Service implementation of Globus components has been optimized for flexibility, stability and scalability. However, part of the Grid community is still reluctant to transition to the Web Service model due mainly to its supposed lower performance. We demonstrate that WS GRAM achieves a performance comparable to that of pre-WS GRAM.</abstract>
	<keywords>Grid computing; GridWay Meta-scheduler; Globus toolkit; Grid resource management; WSRF</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1102>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Web framework with Java and XML in multi-tiers for productivity</paper_heading>
	<authors>Gun Ho Lee, Junsu Jung</authors>
	<abstract>As web-based business systems get larger and more complex, an efficient development framework is an important issue for developers. An efficient web framework improves the quality of multi-tier systems and productivity in the development in distributed computing environments. We provide a framework approach with Java for 3-tiers (Client/Web, Business/EIS), which allows transporting, transforming, or integrating data through the systems using structured-information based on textual format of XML. In the framework for 3-tiers system development with Java, XML is used as a mediator for application data communication as well as a descriptor for configuration. The framework in this paper simplifies workflow and reduces time and effort to develop the systems. We analyze and compare workflows of the framework and productivities in terms of lines of code and effort (man-months) needed to develop a system with identical functions.</abstract>
	<keywords>Web framework; Multi-tiers; Java; XML; Productivity</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1103>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The data building blocks of the enterprise architect</paper_heading>
	<authors>Frank G. Goethals, Wilfried Lemahieu, Monique Snoeck, Jacques A. Vandenbulcke</authors>
	<abstract>When setting up an Enterprise Architecture a number of decisions have to be made with respect to data storage and data transmissions. In this paper we bring a structured overview of those data aspects on which decisions need to be made, and we show that different nodes in a data exchange are dependent upon decisions made by another node with respect to these data aspects. Communicating the decisions on data aspects can happen via architectural descriptions.</abstract>
	<keywords>Data architecture; Data decisions; Data dependencies; Data sharing</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1104>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>High performance AAA architecture for massive IPv4 networks</paper_heading>
	<authors>Minhyung Kim, Sangkyun Kim, Arcy J. Kong</authors>
	<abstract>Even though the total volume of paid data through networks has increased rapidly, most telecommunication and network service providers are still struggling with the continuously decreasing voice revenue and the overheating competition as regards broadband services. Moreover, new technologies, such as VoIP, HomeNet and Bluetooth telephones, are still having trouble generating profit. Obviously these obstacles have dogged the transition to the next generation of business. To maintain the existing customer base and to survive under tight competition, network service providers are being forced to improve their service quality and to create new value added services without additional mass investment or replacing existing infrastructure. In the light of these factors, it is necessary to have a set of different prices for differentiated service levels, instead of the current flat pricing and network access-only service. The preliminary factors for transforming the current system to an upgraded and flexible one are identifying and personalizing subscribers. As a result, the current Authentication, Authorization, Accounting (AAA) infrastructure must be changed to meet those requirements. However there are two main technical limits. First, the limit of large volume user information processing capability. Second, the low rate of progress in the field of applications especially in countries such as the United States and Korea. To deal with these limits, we will first present neo-functional AAA architecture for massive and distributed network environments and describe successful cases of nationwide deployment in Korea.</abstract>
	<keywords>AAA architecture; Auto-provisioning; High capacity edge router; Usage based pricing</keywords>
	<publication_month_year>2007-02-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 2</volumes_issues>
</paper>
<paper no=1105>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Analysis of business process integration in Web service context</paper_heading>
	<authors>Jun Shen, Georg Grossmann, Yun Yang, Markus Stumptner, ... Thomas Reiter</authors>
	<abstract>The integration of Web services is a recent outgrowth of the Business Process integration field that will require powerful meta-schema matching mechanisms supported by higher level abstractions, such as UML meta-models. Currently, there are many XML-based workflow process specification languages (e.g. XPDL, BPEL) which can be used to define business processes in the Web services and Grid Computing world. However, with limited capability to describe the relationships (schemas or ontologies) between process objects, the dominant use of XML as a meta-data markup language makes the semantics of the processes ambiguous. OWL-S (Ontology Web Language for Services) exploits the semantic description power of OWL to build an ontology language for services. It therefore becomes a candidate for an inter lingua. In this paper, we propose an integration framework for business processes, which is applied to Web services defined in OWL-S.</abstract>
	<keywords>Business process management; Web services; Semantic web; Specification integration; UML meta-models</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1106>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Knowledge technology through functional layered intelligence</paper_heading>
	<authors>Martin Dimkovski, Kevin Deeb</authors>
	<abstract>The Internet and other various networks are growing into an interconnection environment of information pools that is often overwhelming. Knowledge technology has been developing intensively, however with limited success: the concepts such as knowledge, information, and data are still amorphous; limitations are re-confirmed in the predominant formalist–reductionist model and lack of organic attributes; there is a lack of coordination and interoperability. These issues are reviewed here over many prominent knowledge technology efforts. An architecture reference model called Functional Layered Intelligence (FLI) is also introduced as a solutions framework. Knowledge technology is then presented as a subset of the FLI model.</abstract>
	<keywords>predominant formalist–reductionist model; interoperability; Functional Layered Intelligence (FLI); Knowledge technology </keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1107>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A layered workflow knowledge Grid/P2P architecture and its models for future generation workflow systems</paper_heading>
	<authors>Kwang-Hoon Kim</authors>
	<abstract>This paper addresses the scalability issue on workflow systems using the knowledge Grid/P2P computing environment. As a feasible solution for the scalability issue, we design a Layered Workflow Knowledge Grid/P2P Architecture. The architecture is based upon a conceptual and generic framework that considers various perspectives for creating workflow knowledge Grid/P2P architectures. In order to show how the architecture is applicable in making up a future generation workflow system, we propose three types of workflow knowledge models and a physical architecture that is possibly configurable from the layered workflow knowledge Grid/P2P architecture. That is, the architecture has a 3-layered hierarchical configuration–procedure-layer, role-layer, and actor-layer–and the three workflow knowledge models–ICN-based model, role-based model, and actor-based model–are the workflow knowledge representations on Grid/P2P nodes in the corresponding layers, respectively. In particular, the most critical rationale of this paper is based on the fact that the nature of the knowledge Grid/P2P computing environment ought to be fitted very well into building a platform for future generation workflow systems characterized by maximally parallel and very large scale workflow procedures that are frequently observed in very large scale enterprizes. And we strongly believe that the rationale must be acceptable. Conclusively, the architecture and the models proposed in this paper shall be useful for not only maximizing the usability of computing facilities in the enterprize but also appropriately handling the scalability issue of its underlined future generation workflow management system in order to cope with massively parallel and very large scale workflow applications.</abstract>
	<keywords>Layered workflow knowledge Grid/P2P architectures; Workflow knowledge models; Knowledge Grid/P2P; Future generation workflow systems; Massively parallel and very large-scale workflows</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1108>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Fault tolerant high performance Information Services for dynamic collections of Grid and Web services</paper_heading>
	<authors>Mehmet S. Aktas, Geoffrey C. Fox, Marlon Pierce</authors>
	<abstract>E-Science Semantic Grids can often be thought of as a dynamic collection of semantic subgrids where each subgrid is a collection of a modest number of services that are assembled for specific tasks. We define a Gaggle as a modest number of managed and actively interacting Grid/Web Services, where services are put together for particular functionality. The information management requirements in Gaggles include both the management of large amounts of relatively static services and associated semantic information as well as the management of multiple dynamic regions (sessions or subgrids) where the semantic information is changing frequently. We design a hybrid, fault tolerant, and high performance Information Service supporting both the scalability of large amounts of relatively slowly varying data and a high performance rapidly updated Information Service for dynamic regions. We use the two Web Service standards: Universal Description, Discovery, and Integration (UDDI) and Web Services Context (WS-Context). We evaluate our approach by applying various tests to investigate the performance and sustainability of the centralized version of our implementation that is applied to sensor and collaboration grids. The experimental study on system responsiveness of the proposed approach shows promising results. This study indicates that communication among services can be achieved with efficient centralized metadata strategies, with metadata coming from more than two services. In contrast point-to-point methodologies provide service conversation with metadata only from the two services that exchange information. In addition, our performance indicates that efficient mediator services also allow us to perform collective operations such as queries on subsets of all available metadata in service conversation.</abstract>
	<keywords>Information Services; Grid information systems; Web Service Architectures; Grid and Web services</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1109>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Dynamic batch processing in workflows: Model and implementation</paper_heading>
	<authors>Jianxun Liu, Jinmin Hu</authors>
	<abstract>Dynamic batch processing commonly exists in production or business processes. Traditional workflow management systems (WfMSs) do not support dynamic batch processing. This paper applies dynamic batch processing to WfMSs. The proposed mechanism consists of a dynamic batch processing scheduling model as well as the design and implementation. Our experiment shows that the proposed dynamic batch processing mechanism is useful for and convenient to business processes.</abstract>
	<keywords>Workflow; Workflow management system; Batch service; Scheduling; ECA rule</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1110>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Grid organizational memory—provision of a high-level Grid abstraction layer supported by ontology alignment</paper_heading>
	<authors>Bartosz Kryza, Renata Słota, Marta Majewska, Jan Pieczykolan, Jacek Kitowski</authors>
	<abstract>In this paper the problem of managing ontological descriptions for a dynamically-changing Grid environment is addressed. The focus of the research is on unification of semantic descriptions for Grid services and resources through ontologies. The Grid Organizational Memory (GOM) has been designed and implemented to enable storing and accessing Grid metadata, kept in the form of ontologies. In GOM, ontology storage and management are designed to support the natural evolution process both in the knowledge structure and knowledge management services. An important element of the GOM framework is the ontology separation schema, specifying the internal vertical and horizontal dependencies according to the Virtual Organization thematic domains, efficiency of knowledge retrieval and ontology development support. The separation schema is applied in the knowledge base distribution model on the Grid. An ontology alignment-based approach is proposed to minimize user commitment on the Grid ontological metadata, to support ontology usage and development. In particular the ontology similarity based approach is presented as support for ontology updates, e.g. extension with new facts from external ontologies and environments, as well as a more efficient and complete, less implementation-bounded querying process. This paper presents research on semantic description of the Grid environment within the scope of the K-Wf Grid project that addresses knowledge-based semiautomatic workflow construction for applications on the Grid.</abstract>
	<keywords>Semantic grid; Metadata management; Ontology similarity; Ontology store</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1111>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A decentralized resource allocation policy in minigrid</paper_heading>
	<authors>Juan Yang, Yun Bai, Yuhui Qiu</authors>
	<abstract>Resource allocating is the key to enhance the concurrency ability of the heterogeneous distributed systems. The traditional resource allocating policies statically assign resources to the jobs according to the distribution schema computed by the job scheduling policy. Those policies cannot handle the DAGs with large jobs since the computing cost is intolerably huge. Decentralized resource allocating policy is inevitably generated to solve this problem by sharing the computing burden on several processors. However the existing decentralized policies cannot dynamically compute the dependent relationships of a given DAG or even schedule the data driven jobs. This paper proposes a cluster-based resource allocating policy—CDRAP, by which the large-scale distributed systems can precisely schedule the data driven jobs and the computing cost of jobs’ scheduling can be controlled in an acceptable range as well. The Broker + describe/issue message mechanism is used as the underlying structure in CDRAP.</abstract>
	<keywords>Broker; Describe/issue message mechanism; Distributed resource allocation policy; Cluster; DAG</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1112>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A high performance grid-web service framework for the identification of ‘conserved sequence tags’</paper_heading>
	<authors>Paolo D’Onorio De Meo, Danilo Carrabino, Nico Sanna, Tiziana Castrignano, ... Graziano Pesole</authors>
	<abstract>The continuous increasing of computing power in biological research places a threshold to the single host use and suggests an approach based on distributed computing. An emerging solution is grid technology, which allows organization to make better use of existing computing resources by providing them with a single, transparent, aggregated source of computing power. Equally, bioinformatics analysis often involves many web services, allowing shared access to information and helping the biologist to design, describe, record complex experiments. A new generation of grid infrastructure, where web services are building blocks, allow managent of a web services workflow. This work shows a tool for the identification and functional annotation of ‘Conserved Sequence Tags’ (CSTs) through cross-species genome comparisons, deployed on a Grid System Architecture, based on Web Services concepts and technologies.</abstract>
	<keywords>Distributed computing; grid technology; single, transparent, aggregated source; Conserved Sequence Tags’ (CSTs).</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1113>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>GEMMA — A Grid environment for microarray management and analysis in bone marrow stem cells experiments</paper_heading>
	<authors>Francesco Beltrame, Adam Papadimitropoulos, Ivan Porro, Silvia Scaglione, ... Federica Viti</authors>
	<abstract>Microarray techniques are successfully used to investigate thousands gene expression profiling in a variety of genomic analyses such as gene identification, drug discovery and clinical diagnosis, providing a large amount of genomic data for the overall research community. A Grid based Environment for distributed Microarray data Management and Analysis (GEMMA) is being built. This platform is planned to provide shared, standardized and reliable tools for managing and analyzing biological data related to bone marrow stem cell cultures, in order to maximize the results of distributed experiments. Different microarray analysis algorithms may be offered to the end-user, through a web interface. A set of modular and independent applications may be published on the portal, and either single algorithms or a combination of them might be invoked by the user, through a workflow strategy. Services may be implemented within an existing Grid computing infrastructure to solve problems concerning both large datasets storage (data intensive problem) and large computational times (computing intensive problem). Moreover, experimental data annotation may be collected according to the same rules and stored through the Grid portal, by using a metadata schema, which allows a comprehensive and replicable sharing of microarray experiments among different researchers. The environment has been tested, so far, as regards performance results concerning Grid parallelization of a microarray based gene expression analysis. First results show a very promising speedup ratio.</abstract>
	<keywords>Grid; Bioinformatics; Mesenchymal stem cells; Microarray</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1114>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Algorithmic re-structuring and data replication for protein structure comparison on a GRID</paper_heading>
	<authors>G. Ciriello, M. Comin, C. Guerra</authors>
	<abstract>This paper describes a major restructuring of PROuST, a method for protein structure comparison, for an efficient porting to the Grid. PROuST consists of different components: an index-based search that produces a list of proteins that are good candidates for similarity, and a dynamic programming algorithm that aligns the target protein with each candidate protein. Both components use the same geometric properties of secondary structure elements of proteins. Thus, an important issue arises when porting the application to the Grid, i.e. the tradeoff between data transfer and data recomputation. Our restructured application avoids recomputation by re-using the data as much as possible, once they are accessed. The algorithmic changes to PROuST allow to reduce the number of data accesses to storage elements and consequently the execution time. This paper also discusses data replication policies on a Grid environment to optimize the data transfer time.</abstract>
	<keywords>Protein similarity; Dynamic programming; Data management; Distributed applications; Grid computing</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1115>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A parallel hybrid genetic algorithm for protein structure prediction on the computational grid</paper_heading>
	<authors>A.-A. Tantar, N. Melab, E.-G. Talbi, B. Parent, D. Horvath</authors>
	<abstract>Solving the structure prediction problem for complex proteins is difficult and computationally expensive. In this paper, we propose a bicriterion parallel hybrid genetic algorithm (GA) in order to efficiently deal with the problem using the computational grid. The use of a near-optimal metaheuristic, such as a GA, allows a significant reduction in the number of explored potential structures. However, the complexity of the problem remains prohibitive as far as large proteins are concerned, making the use of parallel computing on the computational grid essential for its efficient resolution. A conjugated gradient-based Hill Climbing local search is combined with the GA in order to intensify the search in the neighborhood of its provided configurations. In this paper we consider two molecular complexes: the tryptophan-cage protein (Brookhaven Protein Data Bank ID 1L2Y) and α-cyclodextrin. The experimentation results obtained on a computational grid show the effectiveness of the approach.</abstract>
	<keywords>Protein structure prediction; Genetic algorithm; Hill climbing; Parallel computing; Grid computing</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1116>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>The configuration space of homologous proteins: A theoretical and practical framework to reduce the diversity of the protein sequence space after massive all-by-all sequence comparisons</paper_heading>
	<authors>Olivier Bastien, Philippe Ortet, Sylvaine Roy, Eric Maréchal</authors>
	<abstract>Most of the millions of virtual protein sequences deduced from genomic DNA, and the millions to come, will not be experimentally confirmed, neither their function directly analyzed. The exploration of the majority of the protein space relies on our ability to extrapolate the portion of knowledge on characterized sequences to unknown sequences. In this paper we analyzed the large scale comparisons of hundreds of thousands of protein sequences that have been previously carried out using the power of supercomputers or grid frameworks. Following these comparisons, pragmatic rules were used to reduce protein diversity, but none was based on a rigorous and robust framework. We examined how projection of sequences in the configuration space of homologous proteins (CSHP) could help in providing a theoretically robust and long-term practical solution to help organize the protein space. The CSHP can be constructed from the output of any all-by-all pair-wise comparison in which Z-values were computed after Monte Carlo simulations. Reduction of protein diversity can be carried out according to an evolutionary model raising consistent phylogenetic clusters. Projection in the CSHP can be easily updated after sequence database updates, and the accuracy of the phylogenetic topology can be upgraded by improving sub-models. Clusters of homologous proteins can be represented as phylogenetic trees (TULIP trees). In this paper, we showed that the CSHP projection can be used to process the outputs of previous massive comparison projects based on Z-value statistics, given minor corrections for uncollected low values and we propose guidelines for future generations of massive protein sequence comparison projects.</abstract>
	<keywords>Protein space; Protein sequence comparison; TULIP; Configuration space of homologous protein; CluSTr; Z-value</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1117>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Systems Biology and grid technologies: Challenges for understanding complex cell signaling networks</paper_heading>
	<authors>Irina Strizh, Alexei Joutchkov, Nikolay Tverdokhlebov, Sergey Golitsyn</authors>
	<abstract>Modern high-throughput analytical methods and computational modeling lead to the production of huge amounts of data in biology. This provides an opportunity to investigate dynamical properties of a biological system in a more rational way than previously, but it also requires advancements in applied information technologies. Experts need tools and technologies that enable data analysis and knowledge capture within such distributed and heterogeneous sources of information. This article outlines the benefits of grid technologies in Systems Biology, as illustrated through the example of reactive oxygen species pathways that are involved in the pathogenesis of a variety of diseases and in signal transduction.</abstract>
	<keywords>Biology and genetics; Cell networks; Distributed data integration; Grid; Ontologies; Systems Biology</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1118>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Electron tomography of complex biological specimens on the Grid</paper_heading>
	<authors>José-Jesús Fernández, Inmaculada García, Jose-María Carazo, Roberto Marabini</authors>
	<abstract>Electron tomography allows elucidation of the three-dimensional structure of large complex biological specimens at molecular resolution. To achieve such resolution levels, sophisticated algorithms for tomographic reconstruction are needed. Iterative algebraic algorithms yield high quality reconstructions, but they are computationally expensive and high performance techniques are needed to exploit them in practice. We present here a grid computing approach for tomographic reconstruction of large biological specimens. The approach is based on the computational Single-Program-Multiple-Data model, which basically decomposes the global problem into a number of independent 3D reconstruction subproblems. New performance metrics and job submission policies are proposed here that could be of general interest in the field of Grid Computing. We have evaluated this approach on the grid hosted by the European EGEE (Enabling Grids for E-sciencE) project. The influence of the problem size and the parallelism grain has been thoroughly analyzed. Our results demonstrate that the grid is better suited for large reconstructions, as currently needed in electron tomography. To fully exploit the potential of computational grids, the global problem should be decomposed into an adequate number of subdomains.</abstract>
	<keywords>Grid computing; Electron tomography; Electron microscopy; Structural biology; Reconstruction algorithms</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1119>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>From access and integration to mining of secure genomic data sets across the Grid</paper_heading>
	<authors>Richard O. Sinnott</authors>
	<abstract>The UK Department of Trade and Industry (DTI) funded BRIDGES project (Biomedical Research Informatics Delivered by Grid Enabled Services) has developed a Grid infrastructure to support cardiovascular research. This includes the provision of a compute Grid and a data Grid infrastructure with security at its heart. In this paper we focus on the BRIDGES data Grid. A primary aim of the BRIDGES data Grid is to help control the complexity in access to and integration of a myriad of genomic data sets through simple Grid-based tools. We outline these tools, how they are delivered to the end user scientists. We also describe how these tools are to be extended in the BBSRC funded Grid Enabled Microarray Expression Profile Search (GEMEPS) to support a richer vocabulary of search capabilities to support mining of microarray data sets. As with BRIDGES, fine grain Grid security underpins GEMEPS.</abstract>
	<keywords>Grid; Security; Genomics; Data Grid; Microarray data</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1120>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Integrative Biology — the challenges of developing a collaborative research environment for heart and cancer modelling</paper_heading>
	<authors>Sharon Lloyd, Dave Gavaghan, Andrew Simpson, Matthew Mascord, ... Gianni de Fabritiis</authors>
	<abstract>The Integrative Biology project is building a Grid infrastructure which will enable biomedical scientists to develop accurate multi-scale computational models of the heart and of cancer tumours. This infrastructure will remove many of the constraints limiting research progress today by providing access to leading-edge supercomputing and data management resources and by improving facilities to support collaboration between scientists distributed around the world. This paper describes how the project is addressing some specific technical challenges in developing such a collaborative research environment.</abstract>
	<keywords>GridIn silico modelling; Data management; Visualization; Collaborative research environments; Workflow; Model development</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1121>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Towards applying content-based image retrieval in the clinical routine</paper_heading>
	<authors>Marcelo Costa Oliveira, Walfredo Cirne, Paulo M. de Azevedo Marques</authors>
	<abstract>Content-based image retrieval (CBIR) has been one the most vivid research areas in the field of computer vision, and substantial progress has been made over the last years. As such, many have argued for the use of CBIR to support medical imaging diagnosis. However, the sheer volume of data produced in radiology centers has precluded the use of CBIR in the daily routine of hospitals and clinics. This paper aims to change this status quo. We here present a solution that applies Computational Grids to significantly speed up the CBIR procedure, while preserving the security of data in the clinical routine. This solution combines texture attributes and registration algorithms that together are capable of retrieving images with greater-than-90% precision, yet running in a few minutes over the Grid, making it usable in the clinical routine.</abstract>
	<keywords>Content-based image retrieval; Texture attributes; Image registration; Grid Computing</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1122>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Distributed medical images analysis on a Grid infrastructure</paper_heading>
	<authors>R. Bellotti, P. Cerello, S. Tangaro, V. Bevilacqua, ... B. Canesi</authors>
	<abstract>In this paper medical applications on a Grid infrastructure, the MAGIC-5 Project, are presented and discussed. MAGIC-5 aims at developing Computer Aided Detection (CADe) software for the analysis of medical images on distributed databases by means of GRID Services. The use of automated systems for analyzing medical images improves radiologists’ performance; in addition, it could be of paramount importance in screening programs, due to the huge amount of data to check and the cost of related manpower. The need for acquiring and analyzing data stored in different locations requires the use of Grid Services for the management of distributed computing resources and data. Grid technologies allow remote image analysis and interactive online diagnosis, with a relevant reduction of the delays presently associated with the diagnosis in the screening programs. The MAGIC-5 project develops algorithms for the analysis of mammographies for breast cancer detection, Computed-Tomography (CT) images for lung cancer detection and Positron Emission Tomography (PET) images for the early diagnosis of Alzheimer Disease (AD). A Virtual Organization (VO) has been deployed, so that authorized users can share data and resources and implement the following use cases: screening, tele-training and tele-diagnosis for mammograms and lung CT scans, statistical diagnosis by comparison of candidates to a distributed data-set of negative PET scans for the diagnosis of the AD. A small-scale prototype of the required Grid functionality was already implemented for the analysis of digitized mammograms.</abstract>
	<keywords>GRID; Virtual organization; CAD; Mammography; Medical applications</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1123>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>From bioinformatic web portals to semantically integrated Data Grid networks</paper_heading>
	<authors>Adriana Budura, Philippe Cudré-Mauroux, Karl Aberer</authors>
	<abstract>We propose a semi-automated method for redeploying bioinformatic databases indexed in a Web portal as a decentralized, semantically integrated and service-oriented Data Grid. We generate peer-to-peer schema mappings leveraging on cross-referenced instances and instance-based schema matching algorithms. Analyzing real-world data extracted from an existing portal, we show how a rather trivial combination of lexicographical measures with set distance measures yields surprisingly good results in practice. Finally, we propose data models for redeploying all instances, schemas and schema mappings in the Data Grid, relying on standard Semantic Web technologies.</abstract>
	<keywords>Data sharing; Data mapping; Distributed databases; Semantics; Biology</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1124>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Bio-STEER: A Semantic Web workflow tool for Grid computing in the life sciences</paper_heading>
	<authors>Sung Lee, Taowei David Wang, Nada Hashmi, Michael P. Cummings</authors>
	<abstract>Life science research is becoming evermore computationally intensive. Hence, from a computational resource perspective, Grid computing provides a logical approach to meeting many of the computational needs of life science research. However, there are several barriers to the widespread use of Grid computing in life sciences. In this paper, we attempt to address one particular barrier: the difficulty of using Grid computing by life scientists. Life science research often involves connecting multiple applications together to form a workflow. This process of constructing a workflow is complex. When combined with the difficulty of using Grid services, composing a meaningful workflow using Grid services can present a challenge to life scientists. Our proposed solution is a Semantic Web-enabled computing environment, called Bio-STEER. In Bio-STEER, bioinformatics Grid services are mapped to Semantic Web services, described in OWL-S. We also defined an ontology in OWL to model bioinformatics applications. A graphical user interface helps to construct a scientific workflow by showing a list of services that are semantically sound; that is, the output of one service is semantically compatible with the input of the connecting service. Bio-STEER can help users take full advantage of Grid services through a user-friendly graphical user interface (GUI), which allows them to easily construct the workflows they need.</abstract>
	<keywords>Client/server; Distributed systems; Integrated environments; SemanticsWeb-base services; Workflow management; User interface</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1125>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A Resourceomic Grid for bioinformatics</paper_heading>
	<authors>Nicola Cannata, Flavio Corradini, Emanuela Merelli</authors>
	<abstract>In this work we revise the layered software architecture for the Knowledge Grid by explicitly introducing the concept of Resourceome, i.e. an “alive” ontology of resources. This approach is necessary to tackle the challenges posed by the “ome” status, reached by the world of bioinformatics resources. The resulting software architecture, a Resourceomic Grid, integrates components to support issues like awareness, discovery, integration and abstraction of resources.</abstract>
	<keywords>OmicsResourceome; Bioinformatics; Knowledge grid; Agents</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1126>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Grid Services Base Library: A high-level, procedural application programming interface for writing Globus-based Grid services</paper_heading>
	<authors>Adam L. Bazinet, Daniel S. Myers, John Fuetsch, Michael P. Cummings</authors>
	<abstract>The Grid Services Base Library (GSBL) is a procedural application programming interface (API) that abstracts many of the high-level functions performed by Globus Grid services, thus dramatically lowering the barriers to writing Grid services. The library has been extensively tested and used for computational biology research in a Globus Toolkit-based Grid system, in which no fewer than twenty Grid services written with this API are deployed.</abstract>
	<keywords>Client/server; Parallel processing; Web-based services; Software libraries; Object-oriented programming; Distributed systems</keywords>
	<publication_month_year>2007-03-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 3</volumes_issues>
</paper>
<paper no=1127>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>‘Cheap grid’: Leveraging system failure using stochastic computation</paper_heading>
	<authors>Andrew Hamilton-Wright, Deborah Stacey</authors>
	<abstract>Traditionally, network and computation failure on a heterogeneous network are viewed as an unfortunate obstacle to reliable, efficient computation. We propose that such noise can be incorporated into the algorithm design as part of the necessary source of randomness used in stochastic computation. This paradigm incorporates network and computation failure at a high level in the solution-discovery algorithm, rather than attempting to hide and suppress all such noise at the lowest possible levels in the computation tool. This idea enables the creation of a network solution system with extremely small amounts of global state. This lack of required system state allows for heightened degrees of scalability in the computation engine, and fewer resources are consumed by system management. Algorithms with a stochastic component are easily adapted to this system; various types of evolutionary computation are particularly well adapted to this hybrid paradigm. A specific example using a modified steady-state genetic algorithm is provided to explore the functionality of the resulting composite system. The developed architecture is used to calculate the solution to a number of problems, in each case converging on a solution measurably faster than that of a fully “fault-tolerant” scheme, thereby resulting in lower overhead and faster execution time.</abstract>
	<keywords>Grid computing; Parallel computation; Fault-tolerance; Reliable computing; Stochastic systems; Evolutionary computing; Genetic algorithms</keywords>
	<publication_month_year>2007-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 4</volumes_issues>
</paper>
<paper no=1128>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Complete and fragmented replica selection and retrieval in Data Grids</paper_heading>
	<authors>Ruay-Shiung Chang, Po-Hung Chen</authors>
	<abstract>Data Grids support data-intensive applications in wide area Grid systems. They utilize local storage systems as distributed data stores by replicating datasets. Replication is a commonly used technique in a distributed environment. The motivation of replication is that replication can improve data availability, data access performance, and load balancing. Usually a complete file is copied to many Grid sites for local access. However, a site may only need parts of a replica. Therefore, to use the storage systems efficiently, it is necessary for a Grid site to store only parts of a replica. In this paper, we propose a concept called fragmented replicas. That is, when doing replication, a site can store only some partial contents needed locally. It can greatly save the storage space wasted in storing unused data. We also propose a block mapping procedure to determine the distribution of blocks in every available server for later replica retrieval. According to this procedure, a server can provide its available partial replica contents for other members in the Grid system to access. On the other hand, a client can retrieve a fragmented replica directly by using the block mapping procedure. After the block mapping procedure, some co-allocation schemes can be used to retrieve data sets from the available servers. The simulation shows that the co-allocation schemes also improve download performance in a fragmented replication system.</abstract>
	<keywords>Data Grid; Replication; Dynamic self-adaptive replica location; Fragmented replication</keywords>
	<publication_month_year>2007-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 4</volumes_issues>
</paper>
<paper no=1129>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>A grid-enabled software distributed shared memory system on a wide area network</paper_heading>
	<authors>Tyng-Yeu Liang, Chun-Yi Wu, Ce-Kuen Shieh, Jyh-Biau Chang</authors>
	<abstract>This study implements a grid-enabled software distributed shared memory (SDSM) system called Teamster-G on a wide area network (WAN). With the support of Teamster-G, users can develop applications on computational grids by means of shared variables. When they wish to execute their applications, Teamster-G provides a transparent resource allocation service for the execution of the programs. To minimize the turnaround time, Teamster-G employs a session-oriented protocol to reduce the cost of resource allocation, and a two-level consistency protocol to minimize the cost of maintaining data consistency over the WAN. This paper presents the framework of Teamster-G and discusses its performance.</abstract>
	<keywords>Distributed shared memory; Teamster-GGlobus toolkit; Session-oriented; Resource allocation; Two-level consistency protocol</keywords>
	<publication_month_year>2007-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 4</volumes_issues>
</paper>
<paper no=1130>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Efficient scheduling algorithm for component-based networks</paper_heading>
	<authors>Seokcheon Lee, Soundar Kumara, Natarajan Gautam</authors>
	<abstract>In a grid computing environment, it is important to be capable of agilely quantifying the quality of service achievable by each alternative composition of resources. This capability is an essential driver to not only efficiently utilizing the resources but also promoting the virtual economy. In this paper, we design an efficient scheduling algorithm of minimizing completion time for component-based networks. The performance of the network is a function of resource assignment and resource allocation. Resource assignment assigns components to available machines and resource allocation allocates the resources of each machine to the residing components. Though similar problems can be found in the multiprocessor scheduling literature, our problem is different especially because the components in our networks process multiple tasks in parallel with their successor or predecessor components. The designed algorithm is simple but effective since it incorporates the fact that the components in a network can be considered independent under a certain resource allocation policy.</abstract>
	<keywords>Multiprocessor scheduling; Grid computing; Component-based network; Completion time</keywords>
	<publication_month_year>2007-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 4</volumes_issues>
</paper>
<paper no=1131>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Performance effective pre-scheduling strategy for heterogeneous grid systems in the master slave paradigm</paper_heading>
	<authors>Ching-Hsien Hsu, Tai-Lung Chen, Kuan-Ching Li</authors>
	<abstract>It is well known that grid technology has the ability to coordinate shared resources and scheduled tasks. However, the problem of resource management and task scheduling has always been one of the main challenges. In this paper, we present a performance effective pre-scheduling strategy for dispatching tasks onto heterogeneous processors. The main extension of this study is the consideration of heterogeneous communication overheads in grid systems. One significant improvement of our approach is that average turnaround time could be minimized by selecting the processor that has the smallest communication ratio first. The other advantage of the proposed method is that system throughput can be increased by dispersing processor idle time. Our proposed technique can be applied on heterogeneous cluster systems as well as computational grid environments, in which the communication costs vary in different clusters. To evaluate performance of the proposed techniques, we have implemented the proposed algorithms along with previous methods. The experimental results show that our techniques outperform other algorithms in terms of lower average turnaround time, higher average throughput, less processor idle time and higher processors’ utilization.</abstract>
	<keywords>Master-slave paradigm; Heterogeneous processors; Task scheduling; Computational grid</keywords>
	<publication_month_year>2007-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 4</volumes_issues>
</paper>
<paper no=1132>
	<journal_theme>Future Generation Computer Systems</journal_theme>
	<paper_heading>Experiences with GeneRecon on MiG</paper_heading>
	<authors>Thomas Mailund, Christian N.S. Pedersen, Jonas Bardino, Brian Vinter, Henrik H. Karlsen</authors>
	<abstract>We report on our experiences so far with running a bioinformatics simulation study on a newly developed Grid architecture. We briefly describe the bioinformatics application–an association mapping algorithm for both locating disease loci and separating cases into those diseased due to genetic factors and those diseased solely due to environment factors–and describe the Grid architecture and how the application is set up to run on the Grid.</abstract>
	<keywords>Minimum intrusion Grid; Association mapping; Simulation study</keywords>
	<publication_month_year>2007-05-01 00:00:00</publication_month_year>
	<volumes_issues>Volume 23, Issue 4</volumes_issues>
</paper>